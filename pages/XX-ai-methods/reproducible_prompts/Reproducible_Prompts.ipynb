{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "051a17b6",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "We will be working with Ollama, which will allow us to install a number of large language models (LLMs) locally. It has a few installation options.\n",
    "\n",
    "1. Windows\n",
    "    * [Installer](https://ollama.com/download/OllamaSetup.exe)\n",
    "2. Mac\n",
    "    * [Installer](https://ollama.com/download/Ollama-darwin.zip)\n",
    "3. Linux\n",
    "    * `curl -fsSL https://ollama.com/install.sh | sh`\n",
    "4. Conda environment\n",
    "    * `conda install --yes conda-forge::ollama`\n",
    "    * ::: {.callout-caution}\n",
    "      The conda package is somewhat out of date\n",
    "\n",
    "      :::\n",
    "\n",
    "We will also want to install python packages to help interface with our LLM\n",
    "models. There are two options we will explore the Jupyter Notebooks AI extension\n",
    "and ollama langchain package. We will also be using the `pydantic` and `pandas`\n",
    "packages a little later.\n",
    "\n",
    "```{bash}\n",
    "pip install jupyter-ai langchain-ollama pydantic pandas\n",
    "```\n",
    "\n",
    "::: {.callout-tip}\n",
    "[Langchain](https://www.langchain.com/) is a software platform that offers a\n",
    "standardized way to interface with LLM models. It is particularly useful when\n",
    "you want to integrate them into your code or applications.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09fc4d-6112-4823-a784-bfade51f8197",
   "metadata": {},
   "source": [
    "## Loading the ai extension\n",
    "It is important to now restart the you jupyter kernel. Then run the following\n",
    "command to load the jupyter_ai_magicks extensions which are part of the \n",
    "jupyter-ai package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9120eed-94d7-4d3c-a3f9-caaac9038f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyter_ai_magics extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyter_ai_magics\n"
     ]
    }
   ],
   "source": [
    "%load_ext jupyter_ai_magics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbb3e2",
   "metadata": {},
   "source": [
    "We can see the LLM options that we could interface with by using the `%ai list`\n",
    "command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d4d077c-852a-4d44-947f-a8579fe2d063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`ai21:j1-large`</li><li>`ai21:j1-grande`</li><li>`ai21:j1-jumbo`</li><li>`ai21:j1-grande-instruct`</li><li>`ai21:j2-large`</li><li>`ai21:j2-grande`</li><li>`ai21:j2-jumbo`</li><li>`ai21:j2-grande-instruct`</li><li>`ai21:j2-jumbo-instruct`</li></ul> |\n",
       "| `gpt4all` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`gpt4all:ggml-gpt4all-j-v1.2-jazzy`</li><li>`gpt4all:ggml-gpt4all-j-v1.3-groovy`</li><li>`gpt4all:ggml-gpt4all-l13b-snoozy`</li><li>`gpt4all:mistral-7b-openorca.Q4_0`</li><li>`gpt4all:mistral-7b-instruct-v0.1.Q4_0`</li><li>`gpt4all:gpt4all-falcon-q4_0`</li><li>`gpt4all:wizardlm-13b-v1.2.Q4_0`</li><li>`gpt4all:nous-hermes-llama2-13b.Q4_0`</li><li>`gpt4all:gpt4all-13b-snoozy-q4_0`</li><li>`gpt4all:mpt-7b-chat-merges-q4_0`</li><li>`gpt4all:orca-mini-3b-gguf2-q4_0`</li><li>`gpt4all:starcoder-q4_0`</li><li>`gpt4all:rift-coder-v0-7b-q4_0`</li><li>`gpt4all:em_german_mistral_v01.Q4_0`</li></ul> |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `ollama` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | See [https://www.ollama.com/library](https://www.ollama.com/library) for a list of models. Pass a model's name; for example, `deepseek-coder-v2`. |\n",
       "| `qianfan` | `QIANFAN_AK`, `QIANFAN_SK` | <abbr title=\"You have not set all of these environment variables, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`qianfan:ERNIE-Bot`</li><li>`qianfan:ERNIE-Bot-4`</li></ul> |\n",
       "| `togetherai` | `TOGETHER_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`togetherai:Austism/chronos-hermes-13b`</li><li>`togetherai:DiscoResearch/DiscoLM-mixtral-8x7b-v2`</li><li>`togetherai:EleutherAI/llemma_7b`</li><li>`togetherai:Gryphe/MythoMax-L2-13b`</li><li>`togetherai:Meta-Llama/Llama-Guard-7b`</li><li>`togetherai:Nexusflow/NexusRaven-V2-13B`</li><li>`togetherai:NousResearch/Nous-Capybara-7B-V1p9`</li><li>`togetherai:NousResearch/Nous-Hermes-2-Yi-34B`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-13b`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-70b`</li></ul> |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:davinci-002` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n",
       "| `ernie-bot` | `qianfan:ERNIE-Bot` |\n",
       "| `ernie-bot-4` | `qianfan:ERNIE-Bot-4` |\n",
       "| `titan` | `bedrock:amazon.titan-tg1-large` |\n",
       "| `openrouter-claude` | `openrouter:anthropic/claude-3.5-sonnet:beta` |\n"
      ],
      "text/plain": [
       "ai21\n",
       "Requires environment variable: AI21_API_KEY (not set)\n",
       "* ai21:j1-large\n",
       "* ai21:j1-grande\n",
       "* ai21:j1-jumbo\n",
       "* ai21:j1-grande-instruct\n",
       "* ai21:j2-large\n",
       "* ai21:j2-grande\n",
       "* ai21:j2-jumbo\n",
       "* ai21:j2-grande-instruct\n",
       "* ai21:j2-jumbo-instruct\n",
       "\n",
       "gpt4all\n",
       "* gpt4all:ggml-gpt4all-j-v1.2-jazzy\n",
       "* gpt4all:ggml-gpt4all-j-v1.3-groovy\n",
       "* gpt4all:ggml-gpt4all-l13b-snoozy\n",
       "* gpt4all:mistral-7b-openorca.Q4_0\n",
       "* gpt4all:mistral-7b-instruct-v0.1.Q4_0\n",
       "* gpt4all:gpt4all-falcon-q4_0\n",
       "* gpt4all:wizardlm-13b-v1.2.Q4_0\n",
       "* gpt4all:nous-hermes-llama2-13b.Q4_0\n",
       "* gpt4all:gpt4all-13b-snoozy-q4_0\n",
       "* gpt4all:mpt-7b-chat-merges-q4_0\n",
       "* gpt4all:orca-mini-3b-gguf2-q4_0\n",
       "* gpt4all:starcoder-q4_0\n",
       "* gpt4all:rift-coder-v0-7b-q4_0\n",
       "* gpt4all:em_german_mistral_v01.Q4_0\n",
       "\n",
       "huggingface_hub\n",
       "Requires environment variable: HUGGINGFACEHUB_API_TOKEN (not set)\n",
       "* See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`.\n",
       "\n",
       "ollama\n",
       "* See [https://www.ollama.com/library](https://www.ollama.com/library) for a list of models. Pass a model's name; for example, `deepseek-coder-v2`.\n",
       "\n",
       "qianfan\n",
       "Requires environment variables: QIANFAN_AK (not set), QIANFAN_SK (not set)\n",
       "* qianfan:ERNIE-Bot\n",
       "* qianfan:ERNIE-Bot-4\n",
       "\n",
       "togetherai\n",
       "Requires environment variable: TOGETHER_API_KEY (not set)\n",
       "* togetherai:Austism/chronos-hermes-13b\n",
       "* togetherai:DiscoResearch/DiscoLM-mixtral-8x7b-v2\n",
       "* togetherai:EleutherAI/llemma_7b\n",
       "* togetherai:Gryphe/MythoMax-L2-13b\n",
       "* togetherai:Meta-Llama/Llama-Guard-7b\n",
       "* togetherai:Nexusflow/NexusRaven-V2-13B\n",
       "* togetherai:NousResearch/Nous-Capybara-7B-V1p9\n",
       "* togetherai:NousResearch/Nous-Hermes-2-Yi-34B\n",
       "* togetherai:NousResearch/Nous-Hermes-Llama2-13b\n",
       "* togetherai:NousResearch/Nous-Hermes-Llama2-70b\n",
       "\n",
       "\n",
       "Aliases and custom commands:\n",
       "gpt2 - huggingface_hub:gpt2\n",
       "gpt3 - openai:davinci-002\n",
       "chatgpt - openai-chat:gpt-3.5-turbo\n",
       "gpt4 - openai-chat:gpt-4\n",
       "ernie-bot - qianfan:ERNIE-Bot\n",
       "ernie-bot-4 - qianfan:ERNIE-Bot-4\n",
       "titan - bedrock:amazon.titan-tg1-large\n",
       "openrouter-claude - openrouter:anthropic/claude-3.5-sonnet:beta\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5c59f",
   "metadata": {},
   "source": [
    "## Installing our LLM models\n",
    "Ollama is a framework that allows for the installation of many LLM models\n",
    "locally. We need to choose a starting model and then install it. A popular\n",
    "option is the llama model. While not open source it has fairly permissive\n",
    "[terms](https://www.llama.com/llama3/license/) of use.\n",
    "\n",
    "::: {.callout-note}\n",
    "We will be running a console command here. We will be using the integrated\n",
    "jupyter magic command `!` which will send our code to the system shell. Make sure the ollama server is running, or this will give an error.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6e404dd-4613-4f3f-ab89-590880cae92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling dde5aa3fc5ff... 100% ▕████████████████▏ 2.0 GB                         \n",
      "pulling 966de95ca8a6... 100% ▕████████████████▏ 1.4 KB                         \n",
      "pulling fcc5a6bec9da... 100% ▕████████████████▏ 7.7 KB                         \n",
      "pulling a70ff7e570d9... 100% ▕████████████████▏ 6.0 KB                         \n",
      "pulling 56bb8bd477a5... 100% ▕████████████████▏   96 B                         \n",
      "pulling 34bb5ab01051... 100% ▕████████████████▏  561 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "# Meta's Llama 3.2 goes small with 1B and 3B models. 3B is default\n",
    "# Licensed under the llama Community Licence Agreement \n",
    "!ollama pull llama3.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6545d7ce",
   "metadata": {},
   "source": [
    "We can check that the model was successfully installed with the\n",
    "`ollama list` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2df2e05-f51a-4626-9e97-51b4daf7e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   \tID          \tSIZE  \tMODIFIED               \n",
      "llama3.2:latest        \ta80c4f17acd5\t2.0 GB\tLess than a second ago\t\n",
      "deepseek-r1:8b         \t28f8fd6cdc67\t4.9 GB\t47 hours ago          \t\n",
      "qwen2.5-coder:1.5b-base\t02e0f2817a89\t986 MB\t13 days ago           \t\n",
      "wizardlm2:latest       \tc9b1aff820f2\t4.1 GB\t2 weeks ago           \t\n",
      "llama3.1:8b            \t46e0c10c039e\t4.9 GB\t2 weeks ago           \t\n",
      "phi3:medium            \tcf611a26b048\t7.9 GB\t4 weeks ago           \t\n",
      "mathstral:latest       \t4ee7052be55a\t4.1 GB\t4 weeks ago           \t\n",
      "mistral:latest         \tf974a74358d6\t4.1 GB\t4 weeks ago           \t\n",
      "mistral-nemo:latest    \t994f3b8b7801\t7.1 GB\t4 weeks ago           \t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3cea6-e0f7-4236-8b08-4906736972ac",
   "metadata": {},
   "source": [
    "# Jupyter AI **🪄Magic🪄** \n",
    "## Now let's play\n",
    "- We already loaded the ai_magicks extension to use it in our notebook\n",
    "- We'll also just set the AiMagics max_history to 0, so it wont remember any past prompt information in the current prompt\n",
    "  - This is to make things a little more repeatable\n",
    "  - ::: {.callout-warning}\n",
    "    The docs suggests using `%ai reset` to clear the chat history, DON'T DO THIS. It also clears any parameters you have set, and seems to actually hurt reproducibility.\n",
    "    \n",
    "    :::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "174cb5aa-ba3a-4db9-bcb8-898df6806893",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config AiMagics.max_history = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ae393",
   "metadata": {},
   "source": [
    "Now we can ask llama3 to introduce itself. There are a few important parts of\n",
    "the code we must include:\n",
    "1.  `%%ai` Designates the code cell as an ai cell. We can then treat it as a \n",
    "    chat window.\n",
    "2.  `ollama:llama3.2` tells jupyter that the cell well use the ollama interface.\n",
    "    Additionally we have to specify the actual LLM because ollama can any model\n",
    "    you have installed.\n",
    "3.  `-m {}` allows you to provide additionall arguments to your LLM provider. In\n",
    "    our case we will be providing the default ollama port on local host\n",
    "    `\"base_url\":\"http://localhost:11434\"` so jupyter knows where to look for\n",
    "    ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2816b7dd-f84b-4c3e-9c61-d4869054bb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Hello World\n",
       "### My Introduction\n",
       "\n",
       "I'm an AI designed to provide information and answer questions on a wide range of topics, from science and history to entertainment and culture.\n",
       "\n",
       "### My Capabilities\n",
       "* Provide definitions and explanations for various terms and concepts\n",
       "* Answer general knowledge questions and trivia\n",
       "* Offer suggestions and recommendations for books, movies, music, and more\n",
       "* Generate text and responses based on user input\n",
       "\n",
       "### My Goals\n",
       "* Assist users in finding the information they need\n",
       "* Provide accurate and reliable answers to the best of my ability\n",
       "* Continuously learn and improve to become a better conversational AI\n",
       "\n",
       "### How Can I Help You?\n",
       "Feel free to ask me any question or start a conversation. I'm here to help!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai ollama:llama3.2 -m {\"base_url\":\"http://localhost:11434\"}\n",
    "Introduce yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eeb9aa-a9c8-4004-a3e2-93ac49815de3",
   "metadata": {},
   "source": [
    "## Now let's try incoporating a python object\n",
    "We can incoporate a python object using the `{}` syntax inside our prompt. First\n",
    "Lets make a list in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "233cc291-30a0-417c-bb75-c65320401ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'dog', 'bird', 'monkey', 'elephant', 'deer', 'wolf', 'bison', 'elk']\n"
     ]
    }
   ],
   "source": [
    "random_words = [\"cat\", \"dog\", \"bird\", \"monkey\", \"elephant\", \"deer\", \"wolf\", \"bison\", \"elk\"]  \n",
    "print(random_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217449a",
   "metadata": {},
   "source": [
    "Now we can embed this list in our prompt by calling the variable name inside of\n",
    "`{}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfe775f0-d28c-4b17-84ba-5e118f7cebdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "* Deer\n",
       "* Wolf\n",
       "* Elephant\n",
       "* Cat\n",
       "* Elk\n",
       "* Bird\n",
       "* Bison\n",
       "* Monkey\n",
       "* Dog"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai ollama:llama3.2 -m {\"base_url\":\"http://localhost:11434\"}\n",
    "Create a bullet list from the text of {random_words} in random order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdfb776-eb63-414e-8667-247ed0673f1f",
   "metadata": {},
   "source": [
    "We can set the output format using the `-f` flag. Lets specify we would like\n",
    "our output in json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4bee2bc-9153-4547-bea2-d58499ebc01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "value": [
        "deer",
        "bison",
        "elk",
        "bird",
        "cat",
        "dog",
        "monkeys",
        "elephant",
        "wolf"
       ]
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "application/json": {
       "expanded": false,
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       },
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai ollama:llama3.2 -f json -m {\"base_url\":\"http://localhost:11434\"}\n",
    "Create a list from the text of {random_words} in random order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac73d87-2ea2-400f-b7f8-59aa0a2c944e",
   "metadata": {},
   "source": [
    "## Lets keep making things more reproducible\n",
    "We can Set a seed to try and get more reproducible results. This is an ollama\n",
    "parameter, so we provide it inside the `-m {}` arguments. Just add `\"seed:42`\n",
    "after our `\"base_url\":\"http://localhost:11434\"` seperated by a comma. **Do not add any spaces**, as it can interfere with parsing the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c859b6b-5d84-4a8a-8bff-72263e9011ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "* Wolf\n",
       "* Deer\n",
       "* Cat\n",
       "* Elephant\n",
       "* Bird\n",
       "* Bison\n",
       "* Monkey\n",
       "* Dog\n",
       "* Elk"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai ollama:llama3.2 -m {\"base_url\":\"http://localhost:11434\",\"seed\":42}\n",
    "Create a bullet list from the text of {random_words} in random order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af188d-60ae-4926-9442-25ed29f3bb62",
   "metadata": {},
   "source": [
    "## Let's keep tweaking the randomness\n",
    "- You can adjust to \"Creativeness\" of the responses with the `\"temperature\"` \n",
    "- Default is 0.8, higher is more creative\n",
    "- You can set `\"temperature\":0` to give the least \"creative\" response which can help reproducibity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3258cfe-404b-4f15-a57b-1c064550c4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Random Words\n",
       "### List of 10 Words\n",
       "\n",
       "1. **Space**\n",
       "2. **Butterfly**\n",
       "3. **Fountain**\n",
       "4. **Kitten**\n",
       "5. **Harmony**\n",
       "6. **Piano**\n",
       "7. **Starfish**\n",
       "8. **Candy**\n",
       "9. **Llama**\n",
       "10. **Computer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai ollama:llama3.2 -m {\"base_url\":\"http://localhost:11434\",\"seed\":42,\"temperature\":1.5}\n",
    "Create a list of 10 random words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d52c2c7c-b7df-432d-807d-138bc426505b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "* Fjord\n",
       "* Caramel\n",
       "* Banjo\n",
       "* Space\n",
       "* Diamond\n",
       "* Perfume\n",
       "* Llama\n",
       "* Submarine\n",
       "* Harmonica\n",
       "* Snowflake"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai ollama:llama3.2 -m {\"base_url\":\"http://localhost:11434\",\"seed\":42,\"temperature\":0}\n",
    "Create a list of 10 random words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf750d6-e675-40ad-9304-b8fb58488050",
   "metadata": {},
   "source": [
    "## What about passing the output to your python code?\n",
    "You may want to pass the output of a Juypter AI cell to your python code.\n",
    "The best way to do this is to call the contents of your output cell. First let's\n",
    "run an AI cell that outputs json. Python can then input the json as a python\n",
    "dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f790fad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "word1": "cloud",
       "word10": "starfish",
       "word2": "butterfly",
       "word3": "space",
       "word4": "helmet",
       "word5": "piano",
       "word6": "snowflake",
       "word7": "computer",
       "word8": "guitar",
       "word9": "library"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "application/json": {
       "expanded": false,
       "jupyter_ai": {
        "model_id": "llama3.2",
        "provider_id": "ollama"
       },
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai ollama:llama3.2 -f json -m {\"base_url\":\"http://localhost:11434\",\"seed\":42,\"temperature\":0}\n",
    "Create a list of 10 random words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f0d67b",
   "metadata": {},
   "source": [
    "To capture the output of this cell You can call the `Out[##]` function, substituting `##` for the cell number. This can be tricky because the cell\n",
    "number changes based on the order of cells run or if you rerun a cell. The `_`\n",
    "operator will return the output of the last run cell. As long as you run two\n",
    "cells one after the other, it should always successfully retrieve the output.\n",
    "The output will likely be a `IPython.core.display.JSON` object which has the\n",
    "contents of the json saved as a python dictionary in the .data property.\n",
    "\n",
    "Lets call the retrieve the output.data `_.data` and save it to a object `rando`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ca8a175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rando = _.data\n",
    "type(rando)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70483a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IPython.core.display.JSON"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Out[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef61d71",
   "metadata": {},
   "source": [
    "We can select an element of the dictionary using a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f77ee283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'butterfly'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rando[\"word2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f673920",
   "metadata": {},
   "source": [
    "Using a key can be tricky, as we can't be %100 certain what keys our LLM will\n",
    "choose. It can be useful instead to index by position. To do this we need to\n",
    "get the list of keys using `.keys()`, coerce it into a `list()` and then select\n",
    "and element by index `[#]`. This will give use the key at position `#`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d6c6a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'helmet'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rando[list(rando.keys())[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763e3ab",
   "metadata": {},
   "source": [
    "# Langchain\n",
    "## What about embeding LLM prompts directly into Python Code?\n",
    "Thats what langchain is for! Lets load a few packages; `pandas`, and `ollama`,\n",
    "`pydantic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11698561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a651e53e",
   "metadata": {},
   "source": [
    "Using the pandas library let's read in the example dataset \n",
    "`Agaricus_descriptions_examples.csv`. This is table of species descriptions. We\n",
    "are going to use an LLM to parse the descriptions and extract some trait\n",
    "for information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b152840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    base__id                base_name  description__id  \\\n",
      "0     312386        Agaricus leaianus            36157   \n",
      "1     125415          Agaricus edulis            50364   \n",
      "2     125415          Agaricus edulis            50517   \n",
      "3     170147    Agaricus subfloccosus            30039   \n",
      "4     125513    Agaricus marasmioides            38151   \n",
      "5     124122      Agaricus geesterani            33499   \n",
      "6     124122      Agaricus geesterani            33500   \n",
      "7     125629     Agaricus gnapholopus            35879   \n",
      "8     152355         Agaricus petchii            46743   \n",
      "9     152355         Agaricus petchii            46744   \n",
      "10    125635      Agaricus zeylanicus            37426   \n",
      "11    312439    Agaricus clavuliferus            35776   \n",
      "12    312447     Agaricus microcosmus            35902   \n",
      "13     78188        Agaricus hirsutus              698   \n",
      "14     78188        Agaricus hirsutus            44112   \n",
      "15     78188        Agaricus hirsutus            50422   \n",
      "16     78188        Agaricus hirsutus            50527   \n",
      "17    218457   Agaricus angustifolius            50238   \n",
      "18    218477        Agaricus enudatus            50232   \n",
      "19    218481     Agaricus subimmundus            50236   \n",
      "20    218499      Agaricus cuneiforme            50240   \n",
      "21    218503      Agaricus gigantulus            50233   \n",
      "22    218518      Agaricus indetritus            50235   \n",
      "23    218522       Agaricus luridatus            50229   \n",
      "24    218537  Agaricus convexo-planus            50239   \n",
      "25    218542   Agaricus subsulphureus            50237   \n",
      "26    218550    Agaricus illecebrosus            50226   \n",
      "27    218564  Agaricus sanguineoalbus            50230   \n",
      "28    218677     Agaricus tenuisporus            50241   \n",
      "29    218707      Agaricus vepallidus            50231   \n",
      "30    134478      Agaricus subbalteus            39640   \n",
      "31    426878      Agaricus allantopus            36105   \n",
      "32     86276      Agaricus multicolor            50729   \n",
      "33    426885        Agaricus crinalis            36104   \n",
      "34     87331       Agaricus cryptarum              768   \n",
      "35     87331       Agaricus cryptarum             3710   \n",
      "36     87331       Agaricus cryptarum             7549   \n",
      "37     87331       Agaricus cryptarum            43383   \n",
      "38     87331       Agaricus cryptarum            43893   \n",
      "39    426902          Agaricus lepton            36108   \n",
      "40    426926      Agaricus mucidolens            36159   \n",
      "41    426928      Agaricus muculentus            36101   \n",
      "42    426929      Agaricus nidiformis            36017   \n",
      "43    314643    Agaricus mesodactylus            39097   \n",
      "44    314674        Agaricus viscidus            50520   \n",
      "45    141614  Agaricus porphyrophaeus            20986   \n",
      "46    141614  Agaricus porphyrophaeus            20987   \n",
      "47    141614  Agaricus porphyrophaeus            42105   \n",
      "48    141629       Agaricus popinalis            41559   \n",
      "49    314916    Agaricus porcellaneus            50396   \n",
      "\n",
      "                             description_description_  \n",
      "0   75. Agaricus (Mycena) Leaianus, n. sp.\\nPileo ...  \n",
      "1   XXVIII. AGARICUS CAMPESTRIS. Der gemeine Champ...  \n",
      "2   9. Agaricus campestris\\nAGARICUS stipitatus; p...  \n",
      "3   Agaricus subfloccosus (J.E. Lange) Hlaváek, My...  \n",
      "4   508. Agaricus (Naucoria) marasmioides, n. s.\\n...  \n",
      "5   Agaricus geesterani Bas & Heinem., spec. nov. ...  \n",
      "6   Agaricus geesterani Bas & Heinem., spec. nov. ...  \n",
      "7   227. Agaricus (Naucoria) gnapholopus, B. & Br....  \n",
      "8   Psalliota zeylanica.\\nPileo primo conico-cylin...  \n",
      "9   Psalliota zeylanica n. sp.\\nPileus at first co...  \n",
      "10  142. Agaricus (Lepiota) Zeylanicus, n. sp.\\nPi...  \n",
      "11  126. Agaricus (Mycena) clavulifer, B. & Br. \\n...  \n",
      "12  262. Agaricus (Psalliota) microcosmus, B. & Br...  \n",
      "13  Lenzites betulina L. ex Fries \\nCultural chara...  \n",
      "14  Lenzites betulinus (L.:Fr.) Fr. - Epicr. p. 40...  \n",
      "15  LXVII. AGARICUS HIRSUTUS. Der haarige Holzschw...  \n",
      "16  26. Agaricus betulinus\\nAGARICUS acaulis coria...  \n",
      "17  Agaricus augustifolius B. f. 665; Hut 30 breit...  \n",
      "18  Agaricus enudatus B. f. 576, 627; Hut 40 breit...  \n",
      "19  Agaricus subimmundus B. f. 483, 574; Hut 100 b...  \n",
      "20  Agaricus cuneiformis B. f. 491, 497; Hut 70 br...  \n",
      "21  Agaricus gigantulus B. f. 412; Hut 200 , breit...  \n",
      "22  Agaricus indetritus B. f. 273; Hut 140 breit, ...  \n",
      "23  Agaricus luridatus B. f. 490; Hut 90 breit. br...  \n",
      "24  Agaricus convexoplanus B. f. 666; Hut 40 breit...  \n",
      "25  Agaricus subsulphureus B. f. 156, 426; Hut 110...  \n",
      "26  Agaricus illecebrosus B. f. 571; Hut 60 breit,...  \n",
      "27  Agaricus sanguineoalbus B. f. 662; Hut 130 bre...  \n",
      "28  Agaricus tenuisporus B. f. 577; Hut 70 breit, ...  \n",
      "29  Agaricus vepallidus B. f. 419; Hut 90 breit, v...  \n",
      "30  923. Agaricus (Panaeolus) subbalteatus, n. sp....  \n",
      "31  27. Agaricus (Pholiota) allantopus, Berk.\\nPil...  \n",
      "32  CXCIII. BOLETUS MULTICOLOR. Der vielfärbige Ba...  \n",
      "33  24. Agaricus (Mycena) crinalis, n. sp.\\nTenerr...  \n",
      "34  Poria vaillantii (Fries) Cooke\\nCultural chara...  \n",
      "35  Poria vaillantii (DC. ex Fries) Cooke Fig. 103...  \n",
      "36  PORIA VAILLANTII (DC. ex Fr.) Cke.\\nKey Patter...  \n",
      "37  Poria vaillantii (DC. ex Fr.) Cke., Grevillea ...  \n",
      "38  Antrodia vaillantii (DC.:Fr.) Ryv. - Norw. J. ...  \n",
      "39  29. Agaricus (Crepidotus) lepton, n. sp.\\nE re...  \n",
      "40  76. Agaricus (Galera) mucidolens, n. sp.\\nOlid...  \n",
      "41  22. Agaricus (Tricholoma) muculentus, n. sp.\\n...  \n",
      "42  Agaricus (Pleuropus) nidiformis, n. s. praegra...  \n",
      "43  329. Ag. (Pholiota) mesodactylius, n. s. Pileo...  \n",
      "44  12. Agaricus viscidus\\nAGARICUS stipitatus, pi...  \n",
      "45  Locis graminosis pratorum juxta Upsaliam nobil...  \n",
      "46  Entoloma porphyrophaeum (Fr.) Karsten (tav. 44...  \n",
      "47  Entoloma porphyrophaeum (Fr.) P. Karst., Ryssl...  \n",
      "48  Rhodocybe popinalis (Fr.: Fr.) Sing. in Lilloa...  \n",
      "49  XL. AGARICUS PORCELLANEUS. Porcellartartiger M...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Agaricus_descriptions_examples.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2490dce",
   "metadata": {},
   "source": [
    "First we are going to use BaseModel from pydantic to create a json schema that\n",
    "we will use to constrain to form of our LLM output. Lets specify two possible\n",
    "variables `stripe_length` which will be a `float` and `stripe_unit` which will\n",
    "be `str`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e92f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stripe(BaseModel):\n",
    "  stripe_length: float\n",
    "  stripe_unit: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe5a1a",
   "metadata": {},
   "source": [
    "We can call a LLM using the `chat` function from `ollama`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3eba5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response: ChatResponse = chat(\n",
    "  messages=[{\n",
    "    'role': 'user',\n",
    "    'content': f'extract the length of the Stipe from {df.iloc[1, 3]} as stipe_length and the unit of measurement labed as stripe_unit',\n",
    "  }],\n",
    "  model='llama3.2', \n",
    "  format=Stripe.model_json_schema(),\n",
    "  options={\"temperature\":0, \"seed\":42, \"repeat_last_n\":0,\"repeat_penalty\":0}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c7e6ac",
   "metadata": {},
   "source": [
    "The output has been saved in a reponse object. It contains the output inside the message.content trait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39021f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"stripe_length\": 5,\\n  \"stripe_unit\": \"cm\"\\n}'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a9b4ed",
   "metadata": {},
   "source": [
    "This gets output as a string, but we can use the `.model_validate_json` function that was inherited from `BaseModel` to the\n",
    "Stripe object to get it into a `Stripe` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93327f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stripe(stripe_length=5.0, stripe_unit='cm')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripe_data = Stripe.model_validate_json(response.message.content)\n",
    "stripe_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efe5093",
   "metadata": {},
   "source": [
    "Finally, the Stripe object can be directly coerced into a dict if we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5e4715d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stripe_length': 5.0, 'stripe_unit': 'cm'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(stripe_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7f349c",
   "metadata": {},
   "source": [
    "One advantage to using the `langchain-ollama` package is that we can embed our\n",
    "LLM calls into more complex python code. A good example is embedding our LLM\n",
    "call into a for loop. In this way we can go row by row over our entire table,\n",
    "and generate two new columns `stripe_length` and `stripe_units`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7298b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 stripe_length=2.5 stripe_unit='inches'\n",
      "1 stripe_length=5.0 stripe_unit='cm'\n",
      "2 stripe_length=9.0 stripe_unit='cm'\n",
      "3 stripe_length=4.5 stripe_unit='cm'\n",
      "4 stripe_length=1.5 stripe_unit='inch'\n",
      "5 stripe_length=55.0 stripe_unit='mm'\n",
      "6 stripe_length=10.0 stripe_unit='mm'\n",
      "7 stripe_length=0.0003 stripe_unit='inches'\n",
      "8 stripe_length=4.5 stripe_unit='cm'\n",
      "9 stripe_length=4.5 stripe_unit='cm'\n",
      "10 stripe_length=3.0 stripe_unit='inches'\n",
      "11 stripe_length=0.25 stripe_unit='inch'\n",
      "12 stripe_length=0.75 stripe_unit='inch'\n",
      "13 stripe_length=1.5 stripe_unit='cm'\n",
      "14 stripe_length=1.5 stripe_unit='cm'\n",
      "15 stripe_length=5.5 stripe_unit='cm'\n",
      "16 stripe_length=1085.0 stripe_unit='SWE }'\n",
      "17 stripe_length=20.0 stripe_unit='cm'\n",
      "18 stripe_length=40.0 stripe_unit='hoch'\n",
      "19 stripe_length=60.0 stripe_unit='hoch'\n",
      "20 stripe_length=50.0 stripe_unit='cm'\n",
      "21 stripe_length=160.0 stripe_unit='hoch'\n",
      "22 stripe_length=100.0 stripe_unit='hoch'\n",
      "23 stripe_length=70.0 stripe_unit='hoch'\n",
      "24 stripe_length=50.0 stripe_unit='cm'\n",
      "25 stripe_length=15.0 stripe_unit='breit'\n",
      "26 stripe_length=60.0 stripe_unit='cm'\n",
      "27 stripe_length=60.0 stripe_unit='hoch'\n",
      "28 stripe_length=50.0 stripe_unit='cm'\n",
      "29 stripe_length=80.0 stripe_unit='cm'\n",
      "30 stripe_length=2.1 stripe_unit='inches'\n",
      "31 stripe_length=4.0 stripe_unit='inches'\n",
      "32 stripe_length=4.5 stripe_unit='cm'\n",
      "33 stripe_length=1.0 stripe_unit='inch'\n",
      "34 stripe_length=3.0 stripe_unit='cm'\n",
      "35 stripe_length=3.0 stripe_unit='mm'\n",
      "36 stripe_length=10.0 stripe_unit='mm'\n",
      "37 stripe_length=14.0 stripe_unit='cm'\n",
      "38 stripe_length=4.0 stripe_unit='mm'\n",
      "39 stripe_length=2.0 stripe_unit='lines'\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "\n",
    "for i,col in df.iterrows():\n",
    "  response: ChatResponse = chat(\n",
    "    messages=[{\n",
    "      'role': 'user',\n",
    "      'content': f'extract the length of the Stipe from {col.iloc[3]} as stipe_length and the unit of measurement labed as stripe_unit',\n",
    "    }],\n",
    "    model='llama3.2', \n",
    "    format=Stripe.model_json_schema(),\n",
    "    options={\"temperature\":0, \"seed\":42, \"repeat_last_n\":0,\"repeat_penalty\":0}\n",
    "  )\n",
    "  stripe = Stripe.model_validate_json(response.message.content)\n",
    "  print(i, stripe)\n",
    "  results = pd.concat([\n",
    "    results,\n",
    "    pd.concat([col, pd.Series(dict(stripe))]).to_frame().T\n",
    "  ])\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
