[
  {
    "objectID": "notebooks/03-species-distribution/species-distribution-98-download.html",
    "href": "notebooks/03-species-distribution/species-distribution-98-download.html",
    "title": "\n                Access locations and times of Veery encounters\n            ",
    "section": "",
    "text": "For this challenge, you will use a database called the Global Biodiversity Information Facility (GBIF). GBIF is compiled from species observation data all over the world, and includes everything from museum specimens to photos taken by citizen scientists in their backyards.\n\n\n\n\n\n\nTry It: Explore GBIF\n\n\n\nBefore your get started, go to the GBIF occurrences search page and explore the data.\n\n\n\n\n\n\n\n\nContribute to open data\n\n\n\nYou can get your own observations added to GBIF using iNaturalist!\n\n\n\nSet up your code to prepare for download\nWe will be getting data from a source called GBIF (Global Biodiversity Information Facility). We need a package called pygbif to access the data, which may not be included in your environment. Install it by running the cell below:\n\n%%bash\npip install pygbif\n\n\n\n\n\n\n\nTry It: Import packages\n\n\n\nIn the imports cell, we’ve included some packages that you will need. Add imports for packages that will help you:\n\nWork with reproducible file paths\nWork with tabular data\n\n\n\n\nimport time\nimport zipfile\nfrom getpass import getpass\nfrom glob import glob\n\nimport pygbif.occurrences as occ\nimport pygbif.species as species\n\n\n\nSee our solution!\nimport os\nimport pathlib\nimport time\nimport zipfile\nfrom getpass import getpass\nfrom glob import glob\n\nimport pandas as pd\nimport pygbif.occurrences as occ\nimport pygbif.species as species\n\n\n\n# Create data directory in the home folder\ndata_dir = os.path.join(\n    # Home directory\n    pathlib.Path.home(),\n    # Earth analytics data directory\n    'earth-analytics',\n    'data',\n    # Project directory\n    'your-project-directory-name-here',\n)\nos.makedirs(data_dir, exist_ok=True)\n\n# Define the directory name for GBIF data\ngbif_dir = os.path.join(data_dir, 'your-gbif-data-directory-name-here')\n\n:::\n\n\nSee our solution!\n# Create data directory in the home folder\ndata_dir = os.path.join(\n    pathlib.Path.home(),\n    'earth-analytics',\n    'data',\n    'species-distribution',\n)\nos.makedirs(data_dir, exist_ok=True)\n\n# Define the directory name for GBIF data\ngbif_dir = os.path.join(data_dir, 'veery_observations')\n\n\n\n\nRegister and log in to GBIF\nYou will need a GBIF account to complete this challenge. You can use your GitHub account to authenticate with GBIF. Then, run the following code to save your credentials on your computer.\n\n\n\n\n\n\nWarning\n\n\n\nYour email address must match the email you used to sign up for GBIF!\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you accidentally enter your credentials wrong, you can set reset_credentials=True instead of reset_credentials=False.\n\n\n\nreset_credentials = False\n# GBIF needs a username, password, and email\ncredentials = dict(\n    GBIF_USER=(input, 'GBIF username:'),\n    GBIF_PWD=(getpass, 'GBIF password'),\n    GBIF_EMAIL=(input, 'GBIF email'),\n)\nfor env_variable, (prompt_func, prompt_text) in credentials.items():\n    # Delete credential from environment if requested\n    if reset_credentials and (env_variable in os.environ):\n        os.environ.pop(env_variable)\n    # Ask for credential and save to environment\n    if not env_variable in os.environ:\n        os.environ[env_variable] = prompt_func(prompt_text)\n\n\n\nGet the species key\n\n\n\n\n\n\n{{&lt; fa keyboard large &gt;}} Your task\n\n\n\n\nReplace the species_name with the name of the species you want to look up\nRun the code to get the species key\n\n\n\n\n# Query species\nspecies_info = species.name_lookup(species_name, rank='SPECIES')\n\n# Get the first result\nfirst_result = species_info['results'][0]\n\n# Get the species key (nubKey)\nspecies_key = first_result['nubKey']\n\n# Check the result\nfirst_result['species'], species_key\n\n\n\nSee our solution!\n# Query species\nspecies_info = species.name_lookup('catharus fuscescens', rank='SPECIES')\n\n# Get the first result\nfirst_result = species_info['results'][0]\n\n# Get the species key (nubKey)\nspecies_key = first_result['nubKey']\n\n# Check the result\nfirst_result['species'], species_key\n\n\n\n\nDownload data from GBIF\n::: {.callout-task title=“Submit a request to GBIF”\n\nReplace csv_file_pattern with a string that will match any .csv file when used in the glob function. HINT: the character * represents any number of any values except the file separator (e.g. /)\nAdd parameters to the GBIF download function, occ.download() to limit your query to:\n\nobservations\nfrom 2023\nwith spatial coordinates.\n\nThen, run the download. This can take a few minutes. :::\n\n\n# Only download once\ngbif_pattern = os.path.join(gbif_dir, csv_file_pattern)\nif not glob(gbif_pattern):\n    # Submit query to GBIF\n    gbif_query = occ.download([\n        \"speciesKey = \",\n        \"year = \",\n        \"hasCoordinate = \",\n    ])\n    # Only download once\n    if not 'GBIF_DOWNLOAD_KEY' in os.environ:\n        os.environ['GBIF_DOWNLOAD_KEY'] = gbif_query[0]\n\n        # Wait for the download to build\n        wait = occ.download_meta(download_key)['status']\n        while not wait=='SUCCEEDED':\n            wait = occ.download_meta(download_key)['status']\n            time.sleep(5)\n\n    # Download GBIF data\n    download_info = occ.download_get(\n        os.environ['GBIF_DOWNLOAD_KEY'], \n        path=data_dir)\n\n    # Unzip GBIF data\n    with zipfile.ZipFile(download_info['path']) as download_zip:\n        download_zip.extractall(path=gbif_dir)\n\n# Find the extracted .csv file path\ngbif_path = glob(gbif_pattern)[0]\n\n\n\nSee our solution!\n# Only download once\ngbif_pattern = os.path.join(gbif_dir, '*.csv')\nif not glob(gbif_pattern):\n    # Submit query to GBIF\n    gbif_query = occ.download([\n        \"speciesKey = 2490804\",\n        \"hasCoordinate = TRUE\",\n        \"year = 2023\",\n    ])\n    download_key = gbif_query[0]\n\n    # Wait for the download to build\n    if not 'GBIF_DOWNLOAD_KEY' in os.environ:\n        os.environ['GBIF_DOWNLOAD_KEY'] = gbif_query[0]\n\n        # Wait for the download to build\n        wait = occ.download_meta(download_key)['status']\n        while not wait=='SUCCEEDED':\n            wait = occ.download_meta(download_key)['status']\n            time.sleep(5)\n\n    # Download GBIF data\n    download_info = occ.download_get(\n        os.environ['GBIF_DOWNLOAD_KEY'], \n        path=data_dir)\n\n    # Unzip GBIF data\n    with zipfile.ZipFile(download_info['path']) as download_zip:\n        download_zip.extractall(path=gbif_dir)\n\n# Find the extracted .csv file path (take the first result)\ngbif_path = glob(gbif_pattern)[0]\n\n\n\n\nLoad the GBIF data into Python\n\n\n\n\n\n\nTry It: Load GBIF data\n\n\n\n\nLook at the beginning of the file you downloaded using the code below. What do you think the delimiter is?\nRun the following code cell. What happens?\nUncomment and modify the parameters of pd.read_csv() below until your data loads successfully and you have only the columns you want.\n\n\n\nYou can use the following code to look at the beginning of your file:\n\n!head -n 2 $gbif_path \n\n\n# Load the GBIF data\ngbif_df = pd.read_csv(\n    gbif_path, \n    #delimiter='',\n    #index_col='',\n    #usecols=[]\n)\ngbif_df.head()\n\n\n\nSee our solution!\n# Load the GBIF data\ngbif_df = pd.read_csv(\n    gbif_path, \n    delimiter='\\t',\n    index_col='gbifID',\n    usecols=['gbifID', 'decimalLatitude', 'decimalLongitude', 'month'])\ngbif_df.head()"
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution-32-normalize.html",
    "href": "notebooks/03-species-distribution/species-distribution-32-normalize.html",
    "title": "\n                STEP 4: Count the number of observations in each ecosystem, during each month of 2023\n            ",
    "section": "",
    "text": "Much of the data in GBIF is crowd-sourced. As a result, we need not just the number of observations in each ecosystem each month – we need to normalize by some measure of sampling effort. After all, we wouldn’t expect the same number of observations in the Arctic as we would in a National Park, even if there were the same number of Veeries. In this case, we’re normalizing using the average number of observations for each ecosystem and each month. This should help control for the number of active observers in each location and time of year.\n\nSet up your analysis\nFirst things first – let’s load your stored variables.\n\n%store -r\n\n\n\nIdentify the ecoregion for each observation\nYou can combine the ecoregions and the observations spatially using a method called .sjoin(), which stands for spatial join.\n\n\n\n\n\n\nRead More\n\n\n\nCheck out the geopandas documentation on spatial joins to help you figure this one out. You can also ask your favorite LLM (Large-Language Model, like ChatGPT)\n\n\n\n\n\n\n\n\nTry It: Perform a spatial join\n\n\n\n\nIdentify the correct values for the how= and predicate= parameters of the spatial join.\nSelect only the columns you will need for your plot.\nRun the code.\n\n\n\n\ngbif_ecoregion_gdf = (\n    ecoregions_gdf\n    # Match the CRS of the GBIF data and the ecoregions\n    .to_crs(gbif_gdf.crs)\n    # Find ecoregion for each observation\n    .sjoin(\n        gbif_gdf,\n        how='', \n        predicate='')\n    # Select the required columns\n    \n)\ngbif_ecoregion_gdf\n\n\n\nSee our solution!\ngbif_ecoregion_gdf = (\n    ecoregions_gdf\n    # Match the CRS of the GBIF data and the ecoregions\n    .to_crs(gbif_gdf.crs)\n    # Find ecoregion for each observation\n    .sjoin(\n        gbif_gdf,\n        how='inner', \n        predicate='contains')\n    # Select the required columns\n    [['month', 'name']]\n)\ngbif_ecoregion_gdf\n\n\n\n\nCount the observations in each ecoregion each month\n\n\n\n\n\n\nTry It: Group observations by ecoregion\n\n\n\n\nReplace columns_to_group_by with a list of columns. Keep in mind that you will end up with one row for each group – you want to count the observations in each ecoregion by month.\nSelect only month/ecosystem combinations that have more than one occurrence recorded, since a single occurrence could be an error.\nUse the .groupby() and .mean() methods to compute the mean occurrences by ecoregion and by month.\nRun the code – it will normalize the number of occurrences by month and ecoretion.\n\n\n\n\noccurrence_df = (\n    gbif_ecoregion_gdf\n    # For each ecoregion, for each month...\n    .groupby(columns_to_group_by)\n    # ...count the number of occurrences\n    .agg(occurrences=('name', 'count'))\n)\n\n# Get rid of rare observations (possible misidentification?)\noccurrence_df = occurrence_df[...]\n\n# Take the mean by ecoregion\nmean_occurrences_by_ecoregion = (\n    occurrence_df\n    ...\n)\n# Take the mean by month\nmean_occurrences_by_month = (\n    occurrence_df\n    ...\n)\n\n\n\nSee our solution!\noccurrence_df = (\n    gbif_ecoregion_gdf\n    # For each ecoregion, for each month...\n    .groupby(['ecoregion', 'month'])\n    # ...count the number of occurrences\n    .agg(occurrences=('name', 'count'))\n)\n\n# Get rid of rare observation noise (possible misidentification?)\noccurrence_df = occurrence_df[occurrence_df.occurrences&gt;1]\n\n# Take the mean by ecoregion\nmean_occurrences_by_ecoregion = (\n    occurrence_df\n    .groupby(['ecoregion'])\n    .mean()\n)\n# Take the mean by month\nmean_occurrences_by_month = (\n    occurrence_df\n    .groupby(['month'])\n    .mean()\n)\n\n\n\n\nNormalize the observations\n\n\n\n\n\n\nTry It: Normalize\n\n\n\n\nDivide occurrences by the mean occurrences by month AND the mean occurrences by ecoregion\n\n\n\n\n# Normalize by space and time for sampling effort\noccurrence_df['norm_occurrences'] = (\n    occurrence_df\n    ...\n)\noccurrence_df\n\n\n\nSee our solution!\noccurrence_df['norm_occurrences'] = (\n    occurrence_df\n    / mean_occurrences_by_ecoregion\n    / mean_occurrences_by_month\n)\noccurrence_df"
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution-30-overview.html",
    "href": "notebooks/03-species-distribution/species-distribution-30-overview.html",
    "title": "",
    "section": "",
    "text": "In the cell below, reflect on what you know about migration. You could consider:\n\nWhat are some reasons that animals migrate?\nHow might climate change affect animal migrations?\nDo you notice any animal migrations in your area?\n\n\n\nYOUR ANSWER HERE"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the ESIIL Learning Portal!",
    "section": "",
    "text": "Welcome to the ESIIL Learning Portal!\nExplore textbooks:\n\nIntroduction to Earth Data Science\nESIIL Data Short Course\nESIIL STARS Textbook\n\nExplore collaborative workshops:\n\nMacrosystems Ecology for All (MEFA)\nChicago Symposium"
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution-31-wrangle.html",
    "href": "notebooks/03-species-distribution/species-distribution-31-wrangle.html",
    "title": "\n                STEP 1: Set up your reproducible workflow\n            ",
    "section": "",
    "text": "Try It: Import packages\n\n\n\nIn the imports cell, we’ve included a number of packages that you will need. Add imports for packages that will help you:\n\nWork with tabular data\nWork with geospatial vector data\n\n\n\n\nimport os\nimport pathlib\n\n\n\nSee our solution!\nimport os\nimport pathlib\n\nimport geopandas as gpd\nimport pandas as pd"
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution-31-wrangle.html#step-2-define-your-study-area-the-ecoregions-of-north-america",
    "href": "notebooks/03-species-distribution/species-distribution-31-wrangle.html#step-2-define-your-study-area-the-ecoregions-of-north-america",
    "title": "\n                STEP 1: Set up your reproducible workflow\n            ",
    "section": "STEP 2: Define your study area – the ecoregions of North America",
    "text": "STEP 2: Define your study area – the ecoregions of North America\nTrack observations of Taciyagnunpa across different ecoregions! You should be able to see changes in the number of observations in each ecoregion throughout the year.\n\n\n\n\n\n\nRead More\n\n\n\nThe ecoregion data will be available as a shapefile. Learn more about shapefiles and vector data in this Introduction to Spatial Vector Data File Formats in Open Source Python\n\n\n\nDownload and save ecoregion boundaries\nThe ecoregion boundaries take some time to download – they come in at about 150MB. To use your time most efficiently, we recommend caching the ecoregions data on the machine you’re working on so that you only have to download once. To do that, we’ll also introduce the concept of conditionals, or code that adjusts what it does based on the situation.\n\n\n\n\n\n\nRead More\n\n\n\nRead more about conditionals in this Intro Conditional Statements in Python\n\n\n\n\n\n\n\n\nTry It: Get ecoregions boundaries\n\n\n\n\nFind the URL for for the ecoregion boundary Shapefile. You can get ecoregion boundaries from Google..\nReplace your/url/here with the URL you found, making sure to format it so it is easily readable. Also, replace ecoregions_dirname and ecoregions_filename with descriptive and machine-readable names for your project’s file structure.\nChange all the variable names to descriptive variable names, making sure to correctly reference variables you created before.\nRun the cell to download and save the data.\n\n\n\n\n# Set up the ecoregion boundary URL\nurl = \"your/url/here\"\n\n# Set up a path to save the data on your machine\nthe_dir = os.path.join(project_data_dir, 'ecoregions_dirname')\n# Make the ecoregions directory\n\n# Join ecoregions shapefile path\na_path = os.path.join(the_dir, 'ecoregions_filename.shp')\n\n# Only download once\nif not os.path.exists(a_path):\n    my_gdf = gpd.read_file(your_url_here)\n    my_gdf.to_file(your_path_here)\n\n\n\nSee our solution!\n# Set up the ecoregion boundary URL\necoregions_url = (\n    \"https://storage.googleapis.com/teow2016/Ecoregions2017.zip\")\n\n# Set up a path to save the data on your machine\necoregions_dir = os.path.join(data_dir, 'wwf_ecoregions')\nos.makedirs(ecoregions_dir, exist_ok=True)\necoregions_path = os.path.join(ecoregions_dir, 'wwf_ecoregions.shp')\n\n# Only download once\nif not os.path.exists(ecoregions_path):\n    ecoregions_gdf = gpd.read_file(ecoregions_url)\n    ecoregions_gdf.to_file(ecoregions_path)\n\n\nLet’s check that that worked! To do so we’ll use a bash command called find to look for all the files in your project directory with the .shp extension:\n\n%%bash\nfind ~/earth-analytics/data/species-distribution -name '*.shp' \n\n\n\n\n\n\n\nTip\n\n\n\nYou can also run bash commands in the terminal!\n\n\n\n\n\n\n\n\nRead More\n\n\n\nLearn more about bash in this Introduction to Bash\n\n\n\n\nLoad the ecoregions into Python\n\n\n\n\n\n\nTry It: Load ecoregions into Python\n\n\n\nDownload and save ecoregion boundaries from the EPA:\n\nReplace a_path with the path your created for your ecoregions file.\n(optional) Consider renaming and selecting columns to make your GeoDataFrame easier to work with. Many of the same methods you learned for pandas DataFrames are the same for GeoDataFrames!\nMake a quick plot with .plot() to make sure the download worked.\nRun the cell to load the data into Python\n\n\n\n\n# Open up the ecoregions boundaries\ngdf = gpd.read_file(a_path)\n\n# Name the index so it will match the other data later on\ngdf.index.name = 'ecoregion'\n\n# Plot the ecoregions to check download\n\n\n\nSee our solution!\n# Open up the ecoregions boundaries\necoregions_gdf = (\n    gpd.read_file(ecoregions_path)\n    .rename(columns={\n        'ECO_NAME': 'name',\n        'SHAPE_AREA': 'area'})\n    [['name', 'area', 'geometry']]\n)\n\n# We'll name the index so it will match the other data\necoregions_gdf.index.name = 'ecoregion'\n\n# Plot the ecoregions to check download\necoregions_gdf.plot(edgecolor='black', color='skyblue')"
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution-31-wrangle.html#step-3-download-species-observation-data",
    "href": "notebooks/03-species-distribution/species-distribution-31-wrangle.html#step-3-download-species-observation-data",
    "title": "\n                STEP 1: Set up your reproducible workflow\n            ",
    "section": "STEP 3: Download species observation data",
    "text": "STEP 3: Download species observation data\nFor this challenge, you will use a database called the Global Biodiversity Information Facility (GBIF). GBIF is compiled from species observation data all over the world, and includes everything from museum specimens to photos taken by citizen scientists in their backyards. We’ve compiled some sample data in the same format that you will get from GBIF.\n\nDownload sample data\n\n\n\n\n\n\nTry It: Import GBIF Data\n\n\n\n\nDefine the gbif_url. You can get sample data from https://github.com/cu-esiil-edu/esiil-learning-portal/releases/download/data-release/species-distribution-foundations-data.zip\nUsing the ecoregions code, modify the code cell below so that the download only runs once, as with the ecoregion data.\nRun the cell\n\n\n\n\n# Load the GBIF data\ngbif_df = pd.read_csv(\n    gbif_url, \n    delimiter='\\t',\n    index_col='gbifID',\n    usecols=['gbifID', 'decimalLatitude', 'decimalLongitude', 'month'])\ngbif_df.head()\n\n\n\nSee our solution!\n# Define the download URL\ngbif_url = (\n    \"https://github.com/cu-esiil-edu/esiil-learning-portal/releases/download\"\n    \"/data-release/species-distribution-foundations-data.zip\")\n\n# Set up a path to save the data on your machine\ngbif_dir = os.path.join(data_dir, 'gbif_veery')\nos.makedirs(gbif_dir, exist_ok=True)\ngbif_path = os.path.join(gbif_dir, 'gbif_veery.zip')\n\n# Only download once\nif not os.path.exists(gbif_path):\n    # Load the GBIF data\n    gbif_df = pd.read_csv(\n        gbif_url, \n        delimiter='\\t',\n        index_col='gbifID',\n        usecols=['gbifID', 'decimalLatitude', 'decimalLongitude', 'month'])\n    # Save the GBIF data\n    gbif_df.to_csv(gbif_path, index=False)\n\ngbif_df = pd.read_csv(gbif_path)\ngbif_df.head()\n\n\n\n\nConvert the GBIF data to a GeoDataFrame\nTo plot the GBIF data, we need to convert it to a GeoDataFrame first. This will make some special geospatial operations from geopandas available, such as spatial joins and plotting.\n\n\n\n\n\n\nTry It: Convert `DataFrame` to `GeoDataFrame`\n\n\n\n\nReplace your_dataframe with the name of the DataFrame you just got from GBIF\nReplace longitude_column_name and latitude_column_name with column names from your `DataFrame\nRun the code to get a GeoDataFrame of the GBIF data.\n\n\n\n\ngbif_gdf = (\n    gpd.GeoDataFrame(\n        your_dataframe, \n        geometry=gpd.points_from_xy(\n            your_dataframe.longitude_column_name, \n            your_dataframe.latitude_column_name), \n        crs=\"EPSG:4326\")\n    # Select the desired columns\n    [[]]\n)\ngbif_gdf\n\n\n\nSee our solution!\ngbif_gdf = (\n    gpd.GeoDataFrame(\n        gbif_df, \n        geometry=gpd.points_from_xy(\n            gbif_df.decimalLongitude, \n            gbif_df.decimalLatitude), \n        crs=\"EPSG:4326\")\n    # Select the desired columns\n    [['month', 'geometry']]\n)\ngbif_gdf"
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution-33-plot.html",
    "href": "notebooks/03-species-distribution/species-distribution-33-plot.html",
    "title": "\n                STEP 5: Plot the Veery observations by month\n            ",
    "section": "",
    "text": "First thing first – let’s load your stored variables and import libraries.\n\n%store -r\n\n\n\n\n\n\n\nTry It: Import packages\n\n\n\nIn the imports cell, we’ve included some packages that you will need. Add imports for packages that will help you:\n\nMake interactive maps with vector data\n\n\n\n\n# Get month names\nimport calendar\n\n# Libraries for Dynamic mapping\nimport cartopy.crs as ccrs\nimport panel as pn\n\n\n\nSee our solution!\n# Get month names\nimport calendar\n\n# Libraries for Dynamic mapping\nimport cartopy.crs as ccrs\nimport hvplot.pandas\nimport panel as pn\n\n\n\nCreate a simplified GeoDataFrame for plotting\nPlotting larger files can be time consuming. The code below will streamline plotting with hvplot by simplifying the geometry, projecting it to a Mercator projection that is compatible with geoviews, and cropping off areas in the Arctic.\n\n\n\n\n\n\nTry It: Simplify ecoregion data\n\n\n\nDownload and save ecoregion boundaries from the EPA:\n\nSimplify the ecoregions with .simplify(.05), and save it back to the geometry column.\nChange the Coordinate Reference System (CRS) to Mercator with .to_crs(ccrs.Mercator())\nUse the plotting code that is already in the cell to check that the plotting runs quickly (less than a minute) and looks the way you want, making sure to change gdf to YOUR GeoDataFrame name.\n\n\n\n\n# Simplify the geometry to speed up processing\n\n# Change the CRS to Mercator for mapping\n\n# Check that the plot runs in a reasonable amount of time\ngdf.hvplot(geo=True, crs=ccrs.Mercator())\n\n\n\nSee our solution!\n# Simplify the geometry to speed up processing\necoregions_gdf.geometry = ecoregions_gdf.simplify(\n    .05, preserve_topology=False)\n\n# Change the CRS to Mercator for mapping\necoregions_gdf = ecoregions_gdf.to_crs(ccrs.Mercator())\n\n# Check that the plot runs\necoregions_gdf.hvplot(geo=True, crs=ccrs.Mercator())\n\n\n\n\n\n\n\n\nTry It: Map migration over time\n\n\n\n\nIf applicable, replace any variable names with the names you defined previously.\nReplace column_name_used_for_ecoregion_color and column_name_used_for_slider with the column names you wish to use.\nCustomize your plot with your choice of title, tile source, color map, and size.\n\n\n\n\n\n\n\nNote\n\n\n\nYour plot will probably still change months very slowly in your Jupyter notebook, because it calculates each month’s plot as needed. Open up the saved HTML file to see faster performance!\n\n\n\n\n\n# Join the occurrences with the plotting GeoDataFrame\noccurrence_gdf = ecoregions_gdf.join(occurrence_df)\n\n# Get the plot bounds so they don't change with the slider\nxmin, ymin, xmax, ymax = occurrence_gdf.total_bounds\n\n# Plot occurrence by ecoregion and month\nmigration_plot = (\n    occurrence_gdf\n    .hvplot(\n        c=column_name_used_for_shape_color,\n        groupby=column_name_used_for_slider,\n        # Use background tiles\n        geo=True, crs=ccrs.Mercator(), tiles='CartoLight',\n        title=\"Your Title Here\",\n        xlim=(xmin, xmax), ylim=(ymin, ymax),\n        frame_height=600,\n        widget_location='bottom'\n    )\n)\n\n# Save the plot\nmigration_plot.save('migration.html', embed=True)\n\n# Show the plot\nmigration_plot\n\n\n\nSee our solution!\n# Join the occurrences with the plotting GeoDataFrame\noccurrence_gdf = ecoregions_gdf.join(occurrence_df)\n\n# Get the plot bounds so they don't change with the slider\nxmin, ymin, xmax, ymax = occurrence_gdf.total_bounds\n\n# Define the slider widget\nslider = pn.widgets.DiscreteSlider(\n    name='month', \n    options={calendar.month_name[i]: i for i in range(1, 13)}\n)\n\n# Plot occurrence by ecoregion and month\nmigration_plot = (\n    occurrence_gdf\n    .hvplot(\n        c='norm_occurrences',\n        groupby='month',\n        # Use background tiles\n        geo=True, crs=ccrs.Mercator(), tiles='CartoLight',\n        title=\"Veery migration\",\n        xlim=(xmin, xmax), ylim=(ymin, ymax),\n        frame_height=600,\n        colorbar=False,\n        widgets={'month': slider},\n        widget_location='bottom'\n    )\n)\n\n# Save the plot\nmigration_plot.save('migration.html', embed=True)\n\n# Show the plot\nmigration_plot\n\n\n\n\n\n\n\n\n\nLooking for an Extra Challenge?: Fix the month labels\n\n\n\nNotice that the month slider displays numbers instead of the month name. Use pn.widgets.DiscreteSlider() with the options= parameter set to give the months names. You might want to try asking ChatGPT how to do this, or look at the documentation for pn.widgets.DiscreteSlider(). This is pretty tricky!"
  }
]