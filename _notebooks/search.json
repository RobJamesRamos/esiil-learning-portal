[
  {
    "objectID": "notebooks/10-redlining/redlining-42-tree-model.html",
    "href": "notebooks/10-redlining/redlining-42-tree-model.html",
    "title": "\n                STEP 7: Fit a model\n            ",
    "section": "",
    "text": "One way to determine if redlining is related to NDVI is to see if we can correctly predict the redlining grade from the mean NDVI value. With 4 categories, we’d expect to be right only about 25% of the time if we guessed the redlining grade at random. Any accuracy greater than 25% indicates that there is a relationship between vegetation health and redlining.\nTo start out, we’ll fit a Decision Tree Classifier to the data. A decision tree is good at splitting data up into squares by setting thresholds. That makes sense for us here, because we’re looking for thresholds in the mean NDVI that indicate a particular redlining grade.\n\n\n\n\n\n\nTry It: Import packages\n\n\n\nThe cell below imports some functions and classes from the scikit-learn package to help you fit and evaluate a decision tree model on your data. You may need some additional packages later one. Make sure to import them here.\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n\n\nSee our solution!\nimport hvplot.pandas\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n\nAs with all models, it is possible to overfit our Decision Tree Classifier by splitting the data into too many disconnected rectangles. We could theoretically get 100% accuracy this way, but drawing a rectangle for each individual data point. There are many ways to try to avoid overfitting. In this case, we can limit the depth of the decision tree to 2. This means we’ll be drawing 4 rectangles, the same as the number of categories we have.\nAlternative methods of limiting overfitting include:\n\nSplitting the data into test and train groups – the overfitted model is unlikely to fit data it hasn’t seen. In this case, we have relatively little data compared to the number of categories, and so it is hard to evaluate a test/train split.\nPruning the decision tree to maximize accuracy while minimizing complexity. scikit-learn will do this for you automatically. You can also fit the model at a variety of depths, and look for diminishing accuracy returns.\n\n\n\n\n\n\n\nTry It: Fit a tree model\n\n\n\nReplace predictor_variables and observed_values with the values you want to use in your model.\n\n\n\n# Convert categories to numbers\ndenver_ndvi_gdf['grade_codes'] = denver_ndvi_gdf.grade.cat.codes\n\n# Fit model\ntree_classifier = DecisionTreeClassifier(max_depth=2).fit(\n    predictor_variables,\n    observed_values,\n)\n\n# Visualize tree\nplot_tree(tree_classifier)\nplt.show()\n\n\n\nSee our solution!\ndenver_ndvi_gdf['grade_codes'] = denver_ndvi_gdf.grade.cat.codes\n\ntree_classifier = DecisionTreeClassifier(max_depth=2).fit(\n    denver_ndvi_gdf[['mean']],\n    denver_ndvi_gdf.grade_codes,\n)\n\nplot_tree(tree_classifier)\nplt.show()\n\n\n\n\n\n\n\n\nTry It: Plot model results\n\n\n\nCreate a plot of the results by:\n\nPredict grades for each region using the .predict() method of your DecisionTreeClassifier.\nSubtract the actual grades from the predicted grades\nPlot the calculated prediction errors as a chloropleth.\n\n\n\n\n\nSee our solution!\ndenver_ndvi_gdf['predict'] = tree_classifier.predict(\n    denver_ndvi_gdf[['mean']],\n)\n\ndenver_ndvi_gdf['error'] = (\n    denver_ndvi_gdf['predict']\n    - denver_ndvi_gdf['grade_codes']\n)\n\ndenver_ndvi_gdf.hvplot(\n    c='error', cmap='coolwarm',\n    geo=True, tiles='CartoLight',\n)\n\n\nOne method of evaluating your model’s accuracy is by cross-validation. This involves selecting some of your data at random, fitting the model, and then testing the model on a different group. Cross-validation gives you a range of potential accuracies using a subset of your data. It also has a couple of advantages, including:\n\nIt’s good at identifying overfitting, because it tests on a different set of data than it trains on.\nYou can use cross-validation on any model, unlike statistics like \\(p\\)-values and \\(R^2\\) that you may have used in the past.\n\nA disadvantage of cross-validation is that with smaller datasets like this one, it is easy to end up with splits that are too small to be meaningful, or don’t have all the categories.\nRemember – anything above 25% is better than random!\n\n\n\n\n\n\nTry It: Evaluate the model\n\n\n\nUse cross-validation with the cross_val_score to evaluate your model. Start out with the 'balanced_accuracy' scoring method, and 4 cross-validation groups.\n\n\n\n# Evaluate the model with cross-validation\n\n\n\nSee our solution!\ncross_val_score(\n    tree_classifier, \n    denver_ndvi_gdf[['mean']],\n    denver_ndvi_gdf.grade_codes, \n    cv=4,\n    scoring='balanced_accuracy',\n)\n\n\n\n\n\n\n\n\nLooking for an Extra Challenge?: Fit and evaluate an alternative model\n\n\n\nTry out some other models and/or hyperparameters (e.g. changing the max_depth). What do you notice?\n\n\n\n# Try another model\n\n\n\n\n\n\n\nReflect and Respond\n\n\n\nPractice writing about your model. In a few sentences, explain your methods, including some advantages and disadvantages of the choice. Then, report back on your results. Does your model indicate that vegetation health in a neighborhood is related to its redlining grade?\n\n\n\nYOUR MODEL DESCRIPTION AND EVALUATION HERE"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the ESIIL Learning Portal!",
    "section": "",
    "text": "Welcome to the ESIIL Learning Portal!\nExplore textbooks:\n\nIntroduction to Earth Data Science\nESIIL Data Short Course\nESIIL STARS Textbook\n\nExplore collaborative workshops:\n\nMacrosystems Ecology for All (MEFA)\nChicago Symposium"
  },
  {
    "objectID": "notebooks/10-redlining/redlining-41-zonal-stats.html",
    "href": "notebooks/10-redlining/redlining-41-zonal-stats.html",
    "title": "\n                STEP 6: Calculate zonal statistics\n            ",
    "section": "",
    "text": "In order to evaluate the connection between vegetation health and redlining, we need to summarize NDVI across the same geographic areas as we have redlining information.\n\n\n\n\n\n\nTry It: Import packages\n\n\n\nSome packages are included that will help you calculate statistics for areas imported below. Add packages for:\n\nInteractive plotting of tabular and vector data\nWorking with categorical data in DataFrames\n\n\n\n\n# Interactive plots with pandas\n# Ordered categorical data\nimport regionmask # Convert shapefile to mask\nfrom xrspatial import zonal_stats # Calculate zonal statistics\n\n\n\nSee our solution!\nimport hvplot.pandas # interactive plots with pandas\nimport pandas as pd # Ordered categorical data\nimport regionmask # Convert shapefile to mask\nfrom xrspatial import zonal_stats # Calculate zonal statistics\n\n\n\n\n\n\n\n\nTry It: Convert vector to raster\n\n\n\nYou can convert your vector data to a raster mask using the regionmask package. You will need to give regionmask the geographic coordinates of the grid you are using for this to work:\n\nReplace gdf with your redlining GeoDataFrame.\nAdd code to put your GeoDataFrame in the same CRS as your raster data.\nReplace x_coord and y_coord with the x and y coordinates from your raster data.\n\n\n\n\ndenver_redlining_mask = regionmask.mask_geopandas(\n    gdf,\n    x_coord, y_coord,\n    # The regions do not overlap\n    overlap=False,\n    # We're not using geographic coordinates\n    wrap_lon=False\n)\n\n\n\nSee our solution!\ndenver_redlining_mask = regionmask.mask_geopandas(\n    denver_redlining_gdf.to_crs(denver_ndvi_da.rio.crs),\n    denver_ndvi_da.x, denver_ndvi_da.y,\n    # The regions do not overlap\n    overlap=False,\n    # We're not using geographic coordinates\n    wrap_lon=False\n)\n\n\n\n\n\n\n\n\nTry It: Calculate zonal statistics\n\n\n\nCalculate zonal status using the zonal_stats() function. To figure out which arguments it needs, use either the help() function in Python, or search the internet.\n\n\n\n# Calculate NDVI stats for each redlining zone\n\n\n\nSee our solution!\n# Calculate NDVI stats for each redlining zone\ndenver_ndvi_stats = zonal_stats(\n    denver_redlining_mask, \n    denver_ndvi_da\n)\n\n\n\n\n\n\n\n\nTry It: Plot regional statistics\n\n\n\nPlot the regional statistics:\n\nMerge the NDVI values into the redlining GeoDataFrame.\nUse the code template below to convert the grade column (str or object type) to an ordered pd.Categorical type. This will let you use ordered color maps with the grade data!\nDrop all NA grade values.\nPlot the NDVI and the redlining grade next to each other in linked subplots.\n\n\n\n\n# Merge the NDVI stats with redlining geometry into one `GeoDataFrame`\n\n# Change grade to ordered Categorical for plotting\ngdf.grade = pd.Categorical(\n    gdf.grade,\n    ordered=True,\n    categories=['A', 'B', 'C', 'D']\n)\n\n# Drop rows with NA grades\ndenver_ndvi_gdf = denver_ndvi_gdf.dropna()\n\n# Plot NDVI and redlining grade in linked subplots\n\n\n\nSee our solution!\n# Merge NDVI stats with redlining geometry\ndenver_ndvi_gdf = (\n    denver_redlining_gdf\n    .merge(\n        denver_ndvi_stats,\n        left_index=True, right_on='zone'\n    )\n)\n\n# Change grade to ordered Categorical for plotting\ndenver_ndvi_gdf.grade = pd.Categorical(\n    denver_ndvi_gdf.grade,\n    ordered=True,\n    categories=['A', 'B', 'C', 'D']\n)\n\n# Drop rows with NA grads\ndenver_ndvi_gdf = denver_ndvi_gdf.dropna(subset=['grade'])\n\n(\n    denver_ndvi_gdf.hvplot(\n        c='mean', cmap='Greens',\n        geo=True, tiles='CartoLight',\n    )\n    +\n    denver_ndvi_gdf.hvplot(\n        c='grade', cmap='Reds',\n        geo=True, tiles='CartoLight'\n    )\n)"
  }
]