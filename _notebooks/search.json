[
  {
    "objectID": "notebooks/05-vegetation/vegetation.html",
    "href": "notebooks/05-vegetation/vegetation.html",
    "title": "The Cameron Peak Fire, Colorado, USA",
    "section": "",
    "text": "The Cameron Peak Fire was the largest fire in Colorado history, with 326 square miles burned."
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation.html#observing-vegetation-health-from-space",
    "href": "notebooks/05-vegetation/vegetation.html#observing-vegetation-health-from-space",
    "title": "The Cameron Peak Fire, Colorado, USA",
    "section": "Observing vegetation health from space",
    "text": "Observing vegetation health from space\nWe will look at the destruction and recovery of vegetation using NDVI (Normalized Difference Vegetation Index). How does it work? First, we need to learn about spectral reflectance signatures.\nEvery object reflects some wavelengths of light more or less than others. We can see this with our eyes, since, for example, plants reflect a lot of green in the summer, and then as that green diminishes in the fall they look more yellow or orange. The image below shows spectral signatures for water, soil, and vegetation:\n &gt; Image source: SEOS Project\nHealthy vegetation reflects a lot of Near-InfraRed (NIR) radiation. Dead ve Less healthy vegetation reflects a similar amounts of the visible light spectra, but less NIR radiation. We don’t see a huge drop in Green radiation until the plant is very stressed or dead. That means that NIR allows us to get ahead of what we can see with our eyes.\n &gt; Image source: Spectral signature literature review by px39n\nDifferent species of plants reflect different spectral signatures, but the pattern of the signatures are similar. NDVI compares the amount of NIR reflectance to the amount of Red reflectance, thus accounting for many of the species differences and isolating the health of the plant. The formula for calculating NDVI is:\n\\[NDVI = \\frac{(NIR - Red)}{(NIR + Red)}\\]\nRead more about NDVI and other vegetation indices: * earthdatascience.org * USGS\n\n\n\n\n\n\n Import necessary libraries\n\n\n\nIn the cell below, making sure to keep the packages in order, add packages for:\n\nWorking with DataFrames\nWorking with GeoDataFrames\n\n What are we using the rest of these packages for? See if you can figure it out as you complete the notebook.\n\n\n\nimport getpass\nimport json\nimport os\nimport pathlib\nfrom glob import glob\n\nimport earthpy.appeears as eaapp\nimport hvplot.pandas\nimport hvplot.xarray\nimport rioxarray as rxr\nimport xarray as xr\n\n\n\nSee our solution!\nimport getpass\nimport json\nimport os\nimport pathlib\nfrom glob import glob\n\nimport earthpy.appeears as eaapp\nimport geopandas as gpd\nimport hvplot.pandas\nimport hvplot.xarray\nimport pandas as pd\nimport rioxarray as rxr\nimport xarray as xr\n\n\nWe have one more setup task. We’re not going to be able to load all our data directly from the web to Python this time. That means we need to set up a place for it.\n\n\n\n\n\n\n GOTCHA ALERT\n\n\n\nA lot of times in Python we say “directory” to mean a “folder” on your computer. The two words mean the same thing in this context.\n\n\n\n\n\n\n\n\n Your task\n\n\n\nIn the cell below, replace ‘my-data-folder’ with a descriptive directory name.\n\n\n\ndata_dir = os.path.join(pathlib.Path.home(), 'my-data-folder')\n# Make the data directory\nos.makedirs(data_dir, exist_ok=True)\n\n\n\nSee our solution!\ndata_dir = os.path.join(\n    pathlib.Path.home(), 'earth-analytics', 'data', 'cameron-peak')\n# Make the data directory\nos.makedirs(data_dir, exist_ok=True)"
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation.html#study-area-cameron-peak-fire-boundary",
    "href": "notebooks/05-vegetation/vegetation.html#study-area-cameron-peak-fire-boundary",
    "title": "The Cameron Peak Fire, Colorado, USA",
    "section": "Study Area: Cameron Peak Fire Boundary",
    "text": "Study Area: Cameron Peak Fire Boundary\n\nEarth Data Science data formats\nIn Earth Data Science, we get data in three main formats:\n\n\n\n\n\n\n\n\n\nData type\nDescriptions\nCommon file formats\nPython type\n\n\n\n\nTime Series\nThe same data points (e.g. streamflow) collected multiple times over time\nTabular formats (e.g. .csv, or .xlsx)\npandas DataFrame\n\n\nVector\nPoints, lines, and areas (with coordinates)\nShapefile (often an archive like a .zip file because a Shapefile is actually a collection of at least 3 files)\ngeopandas GeoDataFrame\n\n\nRaster\nEvenly spaced spatial grid (with coordinates)\nGeoTIFF (.tif), NetCDF (.nc), HDF (.hdf)\nrioxarray DataArray\n\n\n\n\n\n\n\n\n\n Read more\n\n\n\nCheck out the sections about about vector data and raster data in the textbook.\n\n\n\n\n\n\n\n\n What do you think?\n\n\n\nFor this coding challenge, we are interested in the boundary of the Cameron Peak Fire. In the cell below, answer the following question: What data type do you think the reservation boundaries will be?\n\n\n\n\n\n\n\n\n Your task:\n\n\n\n\nSearch the National Interagency Fire Center Wildfire Boundary catalog for and incident name “Cameron Peak”\nCopy the API results to your clipboard.\nLoad the data into Python using the geopandas library, e.g.: python      gpd.read_file(url)\nCall your data at the end of the cell for testing.\n\n\n\n\n# Download the Cameron Peak fire boundary\n\n\n\nSee our solution!\nurl = (\n    \"https://services3.arcgis.com/T4QMspbfLg3qTGWY/arcgis/rest/services\"\n    \"/WFIGS_Interagency_Perimeters/FeatureServer/0/query\"\n    \"?where=poly_IncidentName%20%3D%20'CAMERON%20PEAK'\"\n    \"&outFields=*&outSR=4326&f=json\")\n\ngdf = gpd.read_file(url)\ngdf\n\n\n\nans_gdf = _\ngdf_pts = 0\n\nif isinstance(ans_gdf, gpd.GeoDataFrame):\n    print('\\u2705 Great work! You downloaded and opened a GeoDataFrame')\n    gdf_pts +=2\nelse:\n    print('\\u274C Hmm, your answer is not a GeoDataFrame')\n\nprint('\\u27A1 You earned {} of 2 points for downloading data'.format(gdf_pts))\n\n\n\nSite Map\nWe always want to create a site map when working with geospatial data. This helps us see that we’re looking at the right location, and learn something about the context of the analysis.\n\n\n\n\n\n\n Your task\n\n\n\n\nPlot your Cameron Peak Fire shapefile on an interactive map\nMake sure to add a title\nAdd ESRI World Imagery as the basemap/background using the tiles=... parameter\n\n\n\n\n# Plot the Cameron Peak Fire boundary\n\n\ngdf.hvplot(\n    title='Cameron Peak Fire, 2020',\n    tiles='EsriImagery')"
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation.html#exploring-the-appeears-api-for-nasa-earthdata-access",
    "href": "notebooks/05-vegetation/vegetation.html#exploring-the-appeears-api-for-nasa-earthdata-access",
    "title": "The Cameron Peak Fire, Colorado, USA",
    "section": "Exploring the AppEEARS API for NASA Earthdata access",
    "text": "Exploring the AppEEARS API for NASA Earthdata access\nOver the next four cells, you will download MODIS NDVI data for the study period. MODIS is a multispectral instrument that measures Red and NIR data (and so can be used for NDVI). There are two MODIS sensors on two different platforms: satellites Terra and Aqua.\n\n\n\n\n\n\n Read more\n\n\n\nLearn more about MODIS datasets and the science they support\n\n\nSince we’re asking for a special download that only covers our study area, we can’t just find a link to the data - we have to negotiate with the data server. We’re doing this using the APPEEARS API (Application Programming Interface). The API makes it possible for you to request data using code. You can use code from the earthpy library to handle the API request.\n\n\n\n\n\n\n Your task\n\n\n\nOften when we want to do something more complex in coding we find an example and modify it. This download code is already almost a working example. Your task will be:\n\nEnter your NASA Earthdata username and password when prompted\nReplace the start and end dates in the task parameters. Download data from July, when greenery is at its peak in the Northern Hemisphere.\nReplace the year range. You should get 3 years before and after the fire so you can see the change!\nReplace gdf with the name of your site geodataframe.\n\n What would the product and layer name be if you were trying to download Landsat Surface Temperature Analysis Ready Data (ARD) instead of MODIS NDVI?\n\n\n\n# Initialize AppeearsDownloader for MODIS NDVI data\nndvi_downloader = eaapp.AppeearsDownloader(\n    download_key='cp-ndvi',\n    ea_dir=data_dir,\n    product='MOD13Q1.061',\n    layer='_250m_16_days_NDVI',\n    start_date=\"01-01\",\n    end_date=\"01-31\",\n    recurring=True,\n    year_range=[2021, 2021],\n    polygon=gdf\n)\n\n\n# Initialize AppeearsDownloader for MODIS NDVI data\nndvi_downloader = eaapp.AppeearsDownloader(\n    download_key='cp-ndvi',\n    ea_dir=data_dir,\n    product='MOD13Q1.061',\n    layer='_250m_16_days_NDVI',\n    start_date=\"07-01\",\n    end_date=\"07-31\",\n    recurring=True,\n    year_range=[2018, 2023],\n    polygon=gdf\n)\nndvi_downloader.download_files(cache=True)"
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation.html#putting-it-together-working-with-multi-file-raster-datasets-in-python",
    "href": "notebooks/05-vegetation/vegetation.html#putting-it-together-working-with-multi-file-raster-datasets-in-python",
    "title": "The Cameron Peak Fire, Colorado, USA",
    "section": "Putting it together: Working with multi-file raster datasets in Python",
    "text": "Putting it together: Working with multi-file raster datasets in Python\nNow you need to load all the downloaded files into Python. Let’s start by getting all the file names. You will also need to extract the date from the filename. Check out the lesson on getting information from filenames in the textbook.\n\n\n\n\n\n\n GOTCHA ALERT\n\n\n\nglob doesn’t necessarily find files in the order you would expect. Make sure to sort your file names like it says in the textbook.\n\n\n\n# Get a list of NDVI tif file paths\n\n\n\nSee our solution!\n# Get list of NDVI tif file paths\nndvi_paths = sorted(glob(os.path.join(data_dir, 'cp-ndvi', '*', '*NDVI*.tif')))\nlen(ndvi_paths)\n\n\n\nRepeating tasks in Python\nNow you should have a few dozen files! For each file, you need to:\n\nLoad the file in using the rioxarray library\nGet the date from the file name\nAdd the date as a dimension coordinate\nGive your data variable a name\nDivide by the scale factor of 10000\n\nYou don’t want to write out the code for each file! That’s a recipe for copy pasta. Luckily, Python has tools for doing similar tasks repeatedly. In this case, you’ll use one called a for loop.\nThere’s some code below that uses a for loop in what is called an accumulation pattern to process each file. That means that you will save the results of your processing to a list each time you process the files, and then merge all the arrays in the list.\n\n\n\n\n\n\n Your task\n\n\n\n\nLook at the file names. How many characters from the end is the date?\nReplace any required variable names with your chosen variable names\nChange the scale_factor variable to be the correct scale factor for this NDVI dataset (HINT: NDVI should range between 0 and 1)\nUsing indices or regular\n\n\n\n\nscale_factor = 1\ndoy_start = -1\ndoy_end = -1\n\n\n\nSee our solution!\nscale_factor = 10000\ndoy_start = -19\ndoy_end = -12\n\n\n\nndvi_das = []\nfor ndvi_path in ndvi_paths:\n    # Get date from file name\n    doy = ndvi_path[doy_start:doy_end]\n    date = pd.to_datetime(doy, format='%Y%j')\n\n    # Open dataset\n    da = rxr.open_rasterio(ndvi_path, masked=True).squeeze()\n\n    # Add date dimension and clean up metadata\n    da = da.assign_coords({'date': date})\n    da = da.expand_dims({'date': 1})\n    da.name = 'NDVI'\n\n    # Multiple by scale factor\n    da = da / scale_factor\n\n    # Prepare for concatenation\n    ndvi_das.append(da)\n\nlen(ndvi_das)\n\nNext, stack your arrays by date into a time series using the xr.combine_by_coords() function. You will have to tell it which dimension you want to stack your data in.\n\n\nSee our solution!\nndvi_da = xr.combine_by_coords(ndvi_das, coords=['date'])\nndvi_da\n\n\n\n\n\n\n\n\n Plot the change in NDVI spatially\n\n\n\nComplete the following: * Select data from 2021 to 2023 (3 years after the fire) * Take the temporal mean (over the date, not spatially) * Get the NDVI variable (should be a DataArray, not a Dataset) * Repeat for the data from 2018 to 2020 (3 years before the fire) * Subtract the 2018-2020 time period from the 2021-2023 time period * Plot the result using a diverging color map like cmap=plt.cm.PiYG\n\nThere are different types of color maps for different types of data. In this case, we want decreases to be a different color from increases, so we should use a diverging color map. Check out available colormaps in the matplotlib documentation.\n\n For an extra challenge, add the fire boundary to the plot\n\n\n\n# Compute the difference in NDVI before and after the fire\n\n# Plot the difference\n(\n    ndvi_diff.hvplot(x='', y='', cmap='', geo=True)\n    *\n    gdf.hvplot(geo=True, fill_color=None, line_color='black')\n)\n\n\n\nSee our solution!\nndvi_diff = (\n    ndvi_da\n        .sel(date=slice('2021', '2023'))\n        .mean('date')\n        .NDVI \n   - ndvi_da\n        .sel(date=slice('2018', '2020'))\n        .mean('date')\n        .NDVI\n)\n(\n    ndvi_diff.hvplot(x='x', y='y', cmap='PiYG', geo=True)\n    *\n    gdf.hvplot(geo=True, fill_color=None, line_color='black')\n)"
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation.html#your-turn-repeat-this-workflow-in-a-different-time-and-place.",
    "href": "notebooks/05-vegetation/vegetation.html#your-turn-repeat-this-workflow-in-a-different-time-and-place.",
    "title": "The Cameron Peak Fire, Colorado, USA",
    "section": "Your turn! Repeat this workflow in a different time and place.",
    "text": "Your turn! Repeat this workflow in a different time and place.\nIt’s not just fires that affect NDVI! You could look at:\n\nRecovery after a national disaster, like a wildfire or hurricane\nDrought\nDeforestation\nIrrigation\nBeaver reintroduction"
  },
  {
    "objectID": "notebooks/02-flood/flood.html",
    "href": "notebooks/02-flood/flood.html",
    "title": "In March of 2019 there were floods in South Dakota, USA",
    "section": "",
    "text": "In March 2019, large parts of South Dakota were flooded for weeks. What happened to cause this flooding? What impacts did the flooding have? Before we look at data about the flooding, we need to check out what other sources are saying about it.\n📖 Here are some resources from different sources to get you started: * The National Weather Service * South Dakota Public Radio * The Intercept\n💬 If you or someone you know have experience with this site, or were there during the floods, we also invite you to write about that."
  },
  {
    "objectID": "notebooks/02-flood/flood.html#the-cheyenne-river-near-wasta-sd-was-one-of-the-locations-affected-by-the-flooding",
    "href": "notebooks/02-flood/flood.html#the-cheyenne-river-near-wasta-sd-was-one-of-the-locations-affected-by-the-flooding",
    "title": "In March of 2019 there were floods in South Dakota, USA",
    "section": "The Cheyenne River near Wasta, SD was one of the locations affected by the flooding",
    "text": "The Cheyenne River near Wasta, SD was one of the locations affected by the flooding\nTo start, you’ll be focusing on the Cheyenne River, which flows into Lake Oahu. Then, you’ll pick your own site that was affected by a flood.\n\nSite Description\n✎ In the cell below, describe the Cheyenne River area in a few sentences. You can include: * Information about the climatology of the area, or typical precipitation and temperature at different months of the year * The runoff ratio (average annual runoff divided by average annual precipitation) * Which wildlife and ecosystems exist in the area * What communities and infrastructure are in the area\n\n\nInteractive Site Map\n\nGet set up to use Python\nUse the cell below to add necessary package imports to this notebook. It’s best to import everything in your very first code cell because it helps folks who are reading your code to figure out where everything comes from (mostly right now this is you in the future). It’s very frustrating to try to figure out what packages need to be installed to get some code to run.\n📖 Our friend the PEP-8 style guide has some things to say about imports. In particular - standard library packages should be listed at the top. These are packages that you don’t need to install because they come with Python. You can check if a package is part of the standard library by searching the Python Standard Library documentation page.\n💻 Your task: * Uncomment all the import lines below. HINT: Use the CMD-/ shortcut to uncomment many lines at once. * Add the library for working with DataFrames in Python to the imports * Separate the standard library package(s) at the top * Run and test your import cell to make sure everything will work\n\n# import folium\n# from io import BytesIO\n# import matplotlib.pyplot as plt\n# import matplotlib.dates as dates\n# import requests\n# import subprocess\n\n# BEGIN SOLUTION\nimport subprocess\nfrom io import BytesIO\n\nimport folium\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as dates\nimport pandas as pd\nimport requests\n# END SOLUTION\n\n\n# Test package imports - DO NOT MODIFY THIS CELL!\nimport_answer_points = 3\n\n# Check that pandas has been imported properly\ntry:\n    na_val = pd.NA\n    print(\"\\u2705 Score! Pandas has been imported as a pd!\")\n    import_answer_points += 2\nexcept NameError:\n    print(\n        \"\\u274C Pandas has not been imported as a pd, please make \"\n        \"sure to import it properly.\"\n    )\n\n# Subtract one point for any PEP-8 errors\ntmp_path = \"tmp.py\"\nwith open(tmp_path, \"w\") as tmp_file:\n    tmp_file.write(In[-2])\nignore_flake8 = 'W292,F401,E302'\nflake8_out = subprocess.run(\n    ['flake8', \n     '--ignore', ignore_flake8, \n     '--import-order-style', 'edited',\n     '--count', \n     tmp_path],\n    stdout=subprocess.PIPE,\n).stdout.decode(\"ascii\")\nprint(flake8_out)\nimport_answer_points -= int(flake8_out.splitlines()[-1])\n\nprint(\n    \"\\n \\u27A1 You received {} out of 5 points.\".format(import_answer_points)\n)\n\nimport_answer_points\n\n\n\n\nSite Map: The Cheyenne River near Wasta\nThe code below will create an interactive map of the area using the folium library. But something is wrong - no one defined the latitude and longitude as variables.\n💻 Your task: * Find the location of the Cheyenne River near Wasta USGS stream gauge using the National Water Information System. This is not the easiest thing to find if you aren’t used to NWIS, so you can use the following instructions to get started: * Go to the National Water Information System Mapper * Type in Wasta in the Find a Place box * Click on the Cheyenne River near Wasta site. It should open a new window. * Click on Site page at the top * Scroll to the bottom and open the Location metadata section. * Define latitude and longitude variables to match the variable names used in the code. * Change the current label, “Thingy” to be descriptive of the site. * Run and test your cell to make sure everything works.\n🌶 EXTRA CHALLENGE: Customize your folium plot using the folium documentation. For example, you could: * Change the base map images * Change the initial zoom\n\n# BEGIN SOLUTION\nsg_lat = 44.08109849 \nsg_lon = -102.4012746\n# END SOLUTION\n\n# Initialize map and tweak settings\nm = folium.Map(\n    # Location to display\n    location=(sg_lat, sg_lon),\n    # Turns off annoying zooming while trying to scroll to the next cell\n    scrollWheelZoom=False)\n\n# Put a marker at the stream gauge location\nfolium.Marker([sg_lat, sg_lon], popup=\"Thingy\").add_to(m)\n\n# Display the map\nm"
  },
  {
    "objectID": "notebooks/02-flood/flood.html#one-way-to-express-how-big-a-flood-is-by-estimating-how-often-larger-floods-occur.",
    "href": "notebooks/02-flood/flood.html#one-way-to-express-how-big-a-flood-is-by-estimating-how-often-larger-floods-occur.",
    "title": "In March of 2019 there were floods in South Dakota, USA",
    "section": "One way to express how big a flood is by estimating how often larger floods occur.",
    "text": "One way to express how big a flood is by estimating how often larger floods occur.\nFor example, you might have heard news media talking about a “100-year flood”.\nIn this notebook, you will write Python code to download and work with a time series of streamflow data during the flooding on the Cheyenne River.\n\nA time series of data is taken at the same location but collected regularly or semi-regularly over time.\n\nYou will then consider how the values compared to previous years before the flood event by computing the flood’s return period.\n\nA return period is an estimate of how often you might expect to see a flood of at least a particular size. This does NOT mean an extreme flood “has” to occur within the return period, or that it couldn’t occur more than once.\n\n📖 Here are some resources from your text book you can review to learn more: * Introduction to time-series data * Flood return period and probability\n✎ In the cell below, explain what data you will need to complete this analysis, including: 1. What type or types of data do you need? 2. How many years of data do you think you need to compute the return period of an extreme event like the 2019 Cheyenne River floods?\nYOUR ANSWER HERE\n\nUS streamflow data are available from the National Water Information Service (NWIS)\n💻 Practice downloading the data you need using the NWIS website. You will not use your downloaded data in the analysis, but you must follow these steps to get the correct urls. In the cell below, use the following instructions to get urls for downloading the USGS data:\n\nGo back to the Cheyenne River near Wasta station page.\nThis time, click Data instead of Site Page\nSelect Daily Data from the list of datasets.\nSelect the entire available date range, and set your results to be as Tab-separated, and press Go.\nCopy the url that populates in your browser window and paste it below. You don’t need to save the data - we will do that using Python.\n\n=== BEGIN MARK SCHEME ===\nUSGS url: https://waterdata.usgs.gov/nwis/dv?cb_00060=on&format=rdb&site_no=06423500&legacy=&referred_module=sw&period=&begin_date=1914-10-01&end_date=2023-04-29\n=== END MARK SCHEME ===\n✎ USGS streamflow URL: url here\n\nExploring the NWIS API\nOne way to access data is through an Application Programming Interface, or API. The URL you’ve just found is an example of a simple, public API. All the parameters of your data search are visible in the URL. For example, to get data starting in 1950, we could change begin_date=1914-10-01 to begin_date=1950-01-01)\n✎ In the cell below - what parameter would you change in the USGS url if you wanted to switch locations?\n=== BEGIN MARK SCHEME ===\nsite_no\n=== END MARK SCHEME ====\n\n\nData description and citation\n✎ In the cell below, describe your data. Include the following information: 1. A 1-2 sentence description of the data 2. Data citation 3. What are the units? 4. What is the time interval for each data point? 5. Is there a “no data” value, or a value used to indicate when the sensor was broken or didn’t detect anything? (These are also known as NA, N/A, NaN, nan, or nodata values)\n📖 The NWIS data format page might be helpful.\n\n\nDownload the data\nIn the cell below complete the following task:\n\nReplace the empty string '' in the code below with the USGS NWIS URL you found, saving it in the nwis_url variable.\nDownload the data using the provided code.\nSave the result (or HTTP Response) to a descriptive variable, and call the variable at the end of the cell.\n\n\nnwis_url = ''\n# BEGIN SOLUTION\nnwis_url = (\n    'https://waterdata.usgs.gov/nwis/dv'\n    '?cb_00060=on'\n    '&format=rdb'\n    '&site_no=06423500'\n    '&period=&'\n    'begin_date=1914-10-01'\n    '&end_date=2023-04-29'\n)\nresponse = requests.get(nwis_url)\n# END SOLUTION\n\n# Download data using a GET HTTP Request\nrequests.get(nwis_url)\n\n\nans_req = _\nreq_pts = 0\n\nif ans_req.ok:\n    print('\\u2705 Great work! Your download succeeded')\n    req_pts +=2\nelse:\n    print('\\u274C Hmm, looks like your url is not correct')\n\nprint('\\u27A1 You earned {} of 2 points for downloading data'.format(req_pts))\n\n\n\nYou will need to take a look at the raw downloaded data to figure out what import parameters to use with the pandas read_csv() function\n💻 In the cell below, replace response with the name of the response variable that you defined above.\nThe code below prints the first 10 lines of your download and numbers them. Does this look like streamflow data to you?\n\nfor i, line in enumerate(response.content.splitlines()[:10]):\n    print(i, line)\n\nIn the NWIS documentation, they say that you can ignore lines that start with a hash sign (#) because they are commented. When we use pandas to import the data, we’ll be able to tell it what character indicates a comment, but we’re not there yet. The code below again prints the first 35 lines of the response content, this time skipping all commented lines.\n💻 In the cell below, replace response with the name of the response variable that you defined above. Then run the code.\n\n# Take a look at the data. What got downloaded?\nfor i, line in enumerate(response.content.splitlines()[:35]):\n    if not line.startswith(b'#'):\n        print(i, line)\n\n✎ What do you notice about the data now? In the following cell, write down your thoughts on: * What separator or delimiter does the data use to separate columns? * What should the data types of each column be? * Which column contains the streamflow data? * Do you need to skip any rows that don’t contain data? * Which column do you think makes sense as the index (unique identifier) for each row? * Is there anything else strange?\nThe answers to the questions above will help you figure out what parameters to use with the pd.read_csv() function.\n\n\nNow we’re ready to import the data with pandas.\nNotice that when you print your downloaded data, each line has a b in front of it. The b stands for “bytes”. In order for pandas to be able to read the data, we need to decode it so each line is a regular string. In the cell below, we do this using the io.BytesIO function, which tricks pandas into thinking it is reading a binary file.\n💻 Your task: * Replace response with the name of your HTTP Response variable * Uncomment the code below, one line at a time. * Using the observations you made above, add the necessary values to get pandas to correctly import the data. * Make sure to include units in your column names where applicable! What units are these streamflow measurements?\n\npd.read_csv(\n    BytesIO(response.content),\n    comment='#',\n    #delimiter='', \n    #skiprows=[],\n    #names=[],\n    #index_col='',\n    #parse_dates=True,\n)\n\n# BEGIN SOLUTION #\ndataframe = pd.read_csv(\n    BytesIO(response.content), \n    sep='\\t', \n    comment='#',\n    skiprows=[29, 30],\n    names=['agency', 'site', 'datetime', 'streamflow_cfs', 'code'],\n    index_col='datetime',\n    parse_dates=True)\ndataframe\n# END SOLUTION\n\n\nans_df = _\ndf_points = 0\n\nif len(ans_df) &gt;= 39658:\n    print(\"\\u2705 Looks like your DataFrame has enough rows!\")\n    df_points += 2\nelse:\n    print(\"\\u274C Oops, your DataFrame doesnt have enough rows\")\n\nif len(ans_df.columns) == 4:\n    print(\"\\u2705 Looks like your DataFrame has enough columns!\")\n    df_points += 2\nelif len(ans_df.columns) == 5:\n    print(\"\\u274C Hmm, looks like you didn't set an index column\")\nelse:\n    print(\"\\u274C Oops, your DataFrame doesn't have the right number of \"\n          \"columns\")\n    \nprint(\"\\u27A1 You earned {} of 4 points\".format(df_points))\n\nLet’s check your data. A useful method for looking at the datatypes in your pd.DataFrame is the pd.DataFrame.info() method.\n\nIn Python, you will see both methods and functions. This is an important and tricky distinction we’ll be talking about a lot. For right now – functions have all of their arguments/parameters inside the parentheses, as in pd.read_csv(args). For methods, the first argument is always some kind of Python object like a pd.DataFrame. Take a look at the next cell for an example of using the pd.DataFrame.info() method.\n\n💻 Replace dataframe with the name of your DataFrame variable\n\ndataframe.info()\n\nOops, we have one more problem! Take a look at the data types of your DataFrame columns…\n✎ In the cell below, write down what data type you would expect the streamflow column to be. The main options are: Integer, Float, Datetime, or Object.\n📖 Check out this example showing the most common data types for pandas columns\n\nA float is a non-integer number. You can identify them because they have decimal points in Python, unlike integers. We do not call them decimals for a reason - a decimal.Decimal is different, and more precise than, a float in Python. If you are ever working with really, really small numbers, you may need to use decimals, but for most applications floats are fine.\n\npandas was able to apply the correct data type to some columns, but not to the streamflow column. One reason this happens is because there are some values in the DataFrame that cannot be read in or parsed as the same data type as everything else. Often, these are no data values. Unfortunately, the documentation does not list any no data values.\nThe code below runs through the values in the streamflow column one by one. It tries to convert each value to a float, but if it fails it prints the result and then stops.\n\nQ is a common variable name for streamflow in hydrology\n\n💻 Replace dataframe below with your DataFrame name, and streamflow_cfs with your streamflow column name.\n\nfor q in dataframe.streamflow_cfs:\n    try: \n        float(q)\n    except:\n        print(q)\n        break\n\nLooks like some of the streamflow data is a string instead of a number. This lets us know that no data could be taken that day because the Cheyenne River was frozen! We can let Python know that there isn’t any data there using the na_values='...' parameter. Substitute the value you found for the ...\n💻 Re-import your data below, this time indicating an NA value. Call your new DataFrame at the end for testing.\n\n# BEGIN SOLUTION #\ndataframe = pd.read_csv(\n    BytesIO(response.content), \n    sep='\\t', \n    comment='#',\n    skiprows=[29, 30],\n    names=['agency', 'site', 'datetime', 'streamflow_cfs', 'code'],\n    index_col='datetime',\n    parse_dates=True,\n    na_values='Ice')\nprint(round(dataframe.iloc[:,2].mean(), 0))\nprint(dataframe.iloc[:,2].dtype)\ndataframe\n# END SOLUTION\n\n\nans_q = _\nq_points = 0\n\nif isinstance(ans_q, pd.DataFrame):\n    print(\"\\u2705 Great, you created a pandas dataframe above\")\n    q_points += 1\nelse:\n    print(\"\\u274C Oops - the cell above should have a DataFrame output.\")\n\nif type(ans_q.index) == pd.DatetimeIndex:\n    print(\"\\u2705 Your DataFrame has the date as the index, \"\n          \"good job!\")\n    q_points += 1\nelse:\n    print(\"\\u274C Your DataFrame does not have the date \"\n          \"as the index.\")\n    \nimport numpy as np\nif ans_q.iloc[:,2].dtype == np.float64:\n    print(\"\\u2705 Your streamflow column is floats!\")\n    q_points += 2\nelse:\n    print(\"\\u274C Your streamflow column still isn't floats.\")\n\nif round(ans_q.iloc[:,2].mean(), 0)==385:\n    print(\"\\u2705 Your streamflow DataFrame has the expected values \"\n          \"in it, good job!\")\n    q_points += 2\nelse:\n    print(\"\\u274C Your streamflow DataFrame does not have the \"\n          \"expected values in it.\")\n\nprint(\"\\u27A1 You received {} out of 6 points for opening the \"\n      \"streamflow data.\".format(\n    q_points))\nq_points\n\n\n\n\nCan we see the flood in the streamflow data?\nIn the cell below, subset the stream discharge data to the same timeframe that you are interested in: February - April, 2019. Save the result to a variable and call it at the end of the cell for testing.\nYou can find some examples of subsetting time series data in the textbook.\n\n# BEGIN SOLUTION\ndataframe_subset = dataframe['2019-02':'2019-04']\nprint(round(dataframe_subset.iloc[:,2].mean(), 0))\ndataframe_subset\n# END SOLUTION\n\n\nans_subset = _\nsubset_points = 0\n\n# Answer should be a DataFrame\nif isinstance(ans_subset, pd.DataFrame):\n    print(\"\\u2705 Great, you created a pandas dataframe above\")\n    subset_points += 1\nelse:\n    print(\"\\u274C Oops - the cell above should have a DataFrame output.\")\n\n# Answer should have a Datetime index\nif type(ans_subset.index) == pd.DatetimeIndex:\n    print(\"\\u2705 Your DataFrame has the date as the index, \"\n          \"good job!\")\n    subset_points += 1\nelse:\n    print(\"\\u274C Your DataFrame does not have the date \"\n          \"as the index.\")\n\n# Answer should include 89 days of data\nif len(ans_subset)==89:\n    print(\"\\u2705 Your DataFrame has the right number of days\")\n    subset_points += 2\nelif len(ans_subset) &gt; 89:\n    print(\"\\u274C Your subset has too many days.\")\nelse:\n    print(\"\\u274C Your subset has too few days.\")\n\n# The mean of the streamflow column should be 1951\nif round(ans_subset.iloc[:,2].mean(), 0)==1951:\n    print(\"\\u2705 Your streamflow DataFrame has the expected values \"\n          \"in it, good job!\")\n    subset_points += 1\nelse:\n    print(\"\\u274C Your streamflow DataFrame does not have the \"\n          \"expected values in it.\")\n\nprint(\"\\u27A1 You received {} out of 5 points for subsetting the \"\n      \"streamflow data.\".format(\n    subset_points))\nsubset_points\n\n💻 Now, in the cell below, plot your subsetted data. Don’t forget to label your plot!\n=== BEGIN MARK SCHEME ===\n\n(2 pts) Subsetted data plotted with dates on the x-axis\n(3 pts) Appropriate axis labels\n(2 pt) Appropriate title or caption\n\n=== END MARK SCHEME ===\n\n# BEGIN SOLUTION\n(dataframe_subset\n .streamflow_cfs\n .plot(\n     xlabel='', \n     ylabel='Streamflow (cfs)',\n     title='Streamflow on the Cheyenne River during a flood'))\nplt.show()\n# END SOLUTION\n\nYou should be able to see the flood in your data going up above 12000 cfs at its peak. But how unusual is that really?\nLet’s start by plotting ALL the data. Then we’ll use a return period statistic to quantify how unusual it was.\n💻 In the cell below, plot the entire time series of streamflow data, without any parameters.\n\n# BEGIN SOLUTION\n\ndataframe.streamflow_cfs.plot()\n\n# END SOLUTION\n\nThis plot looks a little fuzzy because it is trying to fit too many data points in a small area. One way to improve this is by resampling the data to annual maxima. That way we still get the same peak streamflows, but the computer will be able to plot all the values without overlapping.\n\nResampling means changing the time interval between time series observations - in this case from daily to annual.\n\n📖 Read about different ways to resample time series data in your textbook\n📖 You can use a list of offset aliases to look up how to specify the final dates. This list is pretty hard to find - you might want to bookmark it.\n💻 In the cell below, select the streamflow column, and then resample it to get an annual maximum.\n\nWatch out for this gotcha - the test below is looking for a pandas DataFrame, but when we select a single column we get a pandas Series (a DataFrame is a collection of Series.) To get a DataFrame with a single column, use the syntax below with two square brackets:\n\ndataframe[['column_name']]\n\n# BEGIN SOLUTION\n\ndataframe_annual = dataframe[['streamflow_cfs']].resample('AS').max()\nprint(round(int(dataframe_annual.mean()), 0))\ndataframe_annual\n\n# END SOLUTION\n\n\nans_resample = _\nresample_points = 0\n\n# Answer should be a DataFrame\nif isinstance(ans_resample, pd.DataFrame):\n    print(\"\\u2705 Great, you created a pandas DataFrame above\")\n    resample_points += 1\nelse:\n    print(\"\\u274C Oops - the cell above should have a DataFrame output.\")\n\n# Answer should have a Datetime index\nif type(ans_resample.index) == pd.DatetimeIndex:\n    print(\"\\u2705 Your DataFrame has the date as the index, \"\n          \"good job!\")\n    resample_points += 1\nelse:\n    print(\"\\u274C Your DataFrame does not have the date \"\n          \"as the index.\")\n\n# Answer should include 89 days of data\nif len(ans_resample)&gt;=110:\n    print(\"\\u2705 Your DataFrame has the right number of years\")\n    resample_points += 2\nelse:\n    print(\"\\u274C Oops - did you resample your DataFrame to annual?\")\n\n# The mean of the streamflow Series should be 7888\nif round(int(ans_resample.mean()), 0)==7888:\n    print(\"\\u2705 Your annual max streamflow DataFrame has the expected \"\n          \"values in it, good job!\")\n    resample_points += 1\nelse:\n    print(\"\\u274C Your annual max streamflow DataFrame does not have the \"\n          \"expected values in it.\")\n\nprint(\"\\u27A1 You received {} out of 5 points for subsetting the \"\n      \"streamflow data.\".format(\n    resample_points))\nresample_points\n\n💻 Plot your resampled data.\n\n# BEGIN SOLUTION\n\ndataframe_annual.plot(\n    figsize=(14, 4),\n    xlabel='Year',\n    ylabel='Daily Streamflow (cfs)',\n    title='Annual Maximum Daily Streamflow Values on the Cheyenne River')\nplt.show()\n# END SOLUTION\n\nIn the cell below, write a headline and 2-3 sentence description of your plot. What do you estimate the return period was for the flood in 2019?\n🌶 In the cell below, calculate the exceedence probability and return period for each year of the annual data, and add them as columns to your DataFrame.\n\nHINT: pandas columns have a rank method, which you can use. BUT – you will need to use the ascending=False parameter, since higher rank should be lower exceedence probability\n\n\n# BEGIN SOLUTION\n\ndataframe_annual['exceed_prob'] = (\n    dataframe_annual.rank(ascending=False).streamflow_cfs / len(dataframe_annual))\ndataframe_annual['return_period'] = 1 / dataframe_annual.exceed_prob\n\nprint(round(dataframe_annual.mean().product(), 0))\ndataframe_annual\n\n# END SOLUTION\n\n\nans_return = _\nreturn_points = 0\n\n# Answer should be a DataFrame\nif isinstance(ans_return, pd.DataFrame):\n    print(\"\\u2705 Great, you created a pandas dataframe above\")\n    return_points += 1\nelse:\n    print(\"\\u274C Oops - the cell above should have a DataFrame output.\")\n\n# Answer should have a Datetime index\nif type(ans_return.index) == pd.DatetimeIndex:\n    print(\"\\u2705 Your DataFrame has the date as the index, \"\n          \"good job!\")\n    return_points += 1\nelse:\n    print(\"\\u274C Your DataFrame does not have the date \"\n          \"as the index.\")\n\n# Answer should include 110 years of data\nif len(ans_return)==110:\n    print(\"\\u2705 Your DataFrame has the right number of days\")\n    return_points += 2\nelif len(ans_return) &gt; 110:\n    print(\"\\u274C Your DataFrame has too many years.\")\nelse:\n    print(\"\\u274C Your DataFrame has too few years.\")\n\n# The value \"hash\" should be 20549.0\nif round(ans_return.mean().product(), 0)==20549.0:\n    print(\"\\u2705 Your streamflow DataFrame has the expected values \"\n          \"in it, good job!\")\n    return_points += 1\nelse:\n    print(\"\\u274C Your streamflow DataFrame does not have the \"\n          \"expected values in it.\")\n\nprint(\"\\u27A1 You received {} out of 5 extra credit points for calculating the \"\n      \"return period.\".format(return_points))\nreturn_points"
  },
  {
    "objectID": "notebooks/02-flood/flood.html#pep-8-and-does-the-notebook-run",
    "href": "notebooks/02-flood/flood.html#pep-8-and-does-the-notebook-run",
    "title": "In March of 2019 there were floods in South Dakota, USA",
    "section": "Pep 8, and Does the Notebook Run?",
    "text": "Pep 8, and Does the Notebook Run?\nIn this cell, we will give you points for the following\n\nPEP 8 is followed throughout the notebook (3 points)\nThe notebook runs from top to bottom without any editing (it is reproducible) (3 points)"
  },
  {
    "objectID": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html",
    "href": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html",
    "title": "Climate change is impacting the way people live around the world",
    "section": "",
    "text": "::: {.cell .markdown}\nHigher highs, lower lows, storms, and smoke – we’re all feeling the effects of climate change. In this workflow, you will take a look at trends in temperature over time in Rapid City, SD."
  },
  {
    "objectID": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#get-started-with-open-reproducible-science",
    "href": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#get-started-with-open-reproducible-science",
    "title": "Climate change is impacting the way people live around the world",
    "section": "Get started with open reproducible science!",
    "text": "Get started with open reproducible science!\nOpen reproducible science makes scientific methods, data and outcomes available to everyone. That means that everyone who wants should be able to find, read, understand, and run your workflows for themselves.\n\n\nImage from https://www.earthdata.nasa.gov/esds/open-science/oss-for-eso-workshops\n\nFew if any science projects are 100% open and reproducible (yet!). However, members of the open science community have developed open source tools and practices that can help you move toward that goal. You will learn about many of those tools in the Intro to Earth Data Science textbook. Don’t worry about learning all the tools at once – we’ve picked a few for you to get started with.\n\n\n Further reading\nRead our textbook chapter about open reproducible science.\n\n\n What does open reproducible science mean to you?\n Create a new Markdown cell below this one using the + Markdown button in the upper left.\n In the new cell, answer the following questions using a numbered list in Markdown:\n\nIn 1-2 sentences, define open reproducible science.\nIn 1-2 sentences, choose one of the open source tools that you have learned about (i.e. Shell, Git/GitHub, Jupyter Notebook, Python) and explain how it supports open reproducible science."
  },
  {
    "objectID": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#human-readable-and-machine-readable",
    "href": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#human-readable-and-machine-readable",
    "title": "Climate change is impacting the way people live around the world",
    "section": " Human-readable and Machine-readable",
    "text": "Human-readable and Machine-readable\n Create a new Markdown cell below this one using the ESC + b keyboard shortcut.\n In the new cell, answer the following question in a Markdown quote: In 1-2 sentences, does this Jupyter Notebook file have a machine-readable name? Explain your answer."
  },
  {
    "objectID": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#what-the-fork-who-wrote-this",
    "href": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#what-the-fork-who-wrote-this",
    "title": "Climate change is impacting the way people live around the world",
    "section": "What the fork?! Who wrote this?",
    "text": "What the fork?! Who wrote this?\nBelow is a scientific Python workflow. But something’s wrong – The code won’t run! Your task is to follow the instructions below to clean and debug the Python code below so that it runs.\n\nTip\nDon’t worry if you can’t solve every bug right away. We’ll get there! The most important thing is to identify problems with the code and write high-quality GitHub Issues.\n\nAt the end, you’ll repeat the workflow for a location and measurement of your choosing.\nAlright! Let’s clean up this code. First things first…\n\n Machine-readable file names\nRename this notebook (if necessary) with an expressive and machine-readable file name"
  },
  {
    "objectID": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#python-packages-let-you-use-code-written-by-experts-around-the-world",
    "href": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#python-packages-let-you-use-code-written-by-experts-around-the-world",
    "title": "Climate change is impacting the way people live around the world",
    "section": "Python packages let you use code written by experts around the world",
    "text": "Python packages let you use code written by experts around the world\nBecause Python is open source, lots of different people and organizations can contribute (including you!). Many contributions are in the form of packages which do not come with a standard Python download.\n\n Read more\nPackages need to be installed and imported.\n\nIn the cell below, someone was trying to import the pandas package, which helps us to work with tabular data such as comma-separated value or csv files.\n\n Your task\n\nCorrect the typo below to properly import the pandas package under its alias pd.\nRun the cell to import pandas\n\nNOTE: Run your code in the right environment** to avoid import errors**\nWe’ve created a coding environment for you to use that already has all the software and libraries you will need! When you try to run some code, you may be prompted to select a kernel. The kernel refers to the version of Python you are using. You should use the base kernel, which should be the default option.\n\n\n# Import pandas\nimport pandsa as pd\n\nOnce you have run the cell above and imported pandas, run the cell below. It is a test cell that will tell you if you completed the task successfully. If a test cell isn’t working the way you expect, check that you ran your code immediately before running the test.\n\n# DO NOT MODIFY THIS TEST CELL\npoints = 0\ntry:\n    pd.DataFrame()\n    points += 5\n    print('\\u2705 Great work! You correctly imported the pandas library.')\nexcept:\n    print('\\u274C Oops - pandas was not imported correctly.')\nprint('You earned {} of 5 points for importing pandas'.format(points))"
  },
  {
    "objectID": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#there-are-more-earth-observation-data-online-than-any-one-person-could-ever-look-at",
    "href": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#there-are-more-earth-observation-data-online-than-any-one-person-could-ever-look-at",
    "title": "Climate change is impacting the way people live around the world",
    "section": "There are more Earth Observation data online than any one person could ever look at",
    "text": "There are more Earth Observation data online than any one person could ever look at\nNASA’s Earth Observing System Data and Information System (EOSDIS) alone manages over 9PB of data. 1 PB is roughly 100 times the entire Library of Congress (a good approximation of all the books available in the US). It’s all available to you once you learn how to download what you want.\nHere we’re using the NOAA National Centers for Environmental Information (NCEI) Access Data Service application progamming interface (API) to request data from their web servers. We will be using data collected as part of the Global Historical Climatology Network daily (GHCNd) from their Climate Data Online library program at NOAA.\nFor this example we’re requesting daily summary data in Rapid City, CO (station ID USC00396947).\n\n Your task:\n\nResearch the Global Historical Climatology Network - Daily data source.\nIn the cell below, write a 2-3 sentence description of the data source. You should describe:\n\nwho takes the data\nwhere the data were taken\nwhat the maximum temperature units are\nhow the data are collected\n\nInclude a citation of the data (HINT: See the ‘Data Citation’ tab on the GHCNd overview page).\n\n\nYOUR DATA DESCRIPTION AND CITATION HERE 🛎️"
  },
  {
    "objectID": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#you-can-access-ncei-ghcnd-data-from-the-internet-using-its-api",
    "href": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#you-can-access-ncei-ghcnd-data-from-the-internet-using-its-api",
    "title": "Climate change is impacting the way people live around the world",
    "section": "You can access NCEI GHCNd Data from the internet using its API 🖥️ 📡 🖥️",
    "text": "You can access NCEI GHCNd Data from the internet using its API 🖥️ 📡 🖥️\nThe cell below contains the URL for the data you will use in this part of the notebook. We created this URL by generating what is called an API endpoint using the NCEI API documentation.\n\nNote\nAn application programming interface (API) is a way for two or more computer programs or components to communicate with each other. It is a type of software interface, offering a service to other pieces of software (Wikipedia).\n\nHowever, we still have a problem - we can’t get the URL back later on because it isn’t saved in a variable. In other words, we need to give the url a name so that we can request in from Python later (sadly, Python has no ‘hey what was that thingy I typed yesterday?’ function).\n\n Read more\nCheck out the textbook section on variables\n\n\n Your task\n\nPick an expressive variable name for the URL. HINT: click on the Variables button up top to see all your variables. Your new url variable will not be there until you define it and run the code\nReformat the URL so that it adheres to the 79-character PEP-8 line limit.You should see two vertical lines in each cell - don’t let your code go past the second line\nAt the end of the cell where you define your url variable, call your variable (type out its name) so it can be tested.\n\n\n\nstuff23 = ('https://www.ncei.noaa.gov/access/services/da'\n'ta/v1?dataset=daily-summaries&dataTypes=TOBS,PRCP&stations=USC00396947&startDate=1949-10-01&endDate=2024-02-18&includeStationName=true&includeStation'\n'Location=1&units=standard')\nstuff23\n\n\n# DO NOT MODIFY THIS TEST CELL\nresp_url = _\npoints = 0\n\nif type(resp_url)==str:\n    points += 3\n    print('\\u2705 Great work! You correctly called your url variable.')\nelse:\n    print('\\u274C Oops - your url variable was not called correctly.')\n\nif len(resp_url)==218:\n    points += 3\n    print('\\u2705 Great work! Your url is the correct length.')\nelse:\n    print('\\u274C Oops - your url variable is not the correct length.')\n\nprint('You earned {} of 6 points for defining a url variable'.format(points))"
  },
  {
    "objectID": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#download-and-get-started-working-with-ncei-data",
    "href": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#download-and-get-started-working-with-ncei-data",
    "title": "Climate change is impacting the way people live around the world",
    "section": "Download and get started working with NCEI data",
    "text": "Download and get started working with NCEI data\nThe pandas library you imported can download data from the internet directly into a type of Python object called a DataFrame. In the code cell below, you can see an attempt to do just this. But there are some problems…\n\n You’re ready to fix some code!\nYour task is to:\n\nLeave a space between the # and text in the comment and try making the comment more informative\nMake any changes needed to get this code to run. HINT: The my_url variable doesn’t exist - you need to replace it with the variable name you chose.\nModify the .read_csv() statement to include the following parameters:\n\nindex_col='DATE' – this sets the DATE column as the index. Needed for subsetting and resampling later on\nparse_dates=True – this lets python know that you are working with time-series data, and values in the indexed column are date time objects\nna_values=['NaN'] – this lets python know how to handle missing values\n\nClean up the code by using expressive variable names, expressive column names, PEP-8 compliant code, and descriptive comments\n\n\nMake sure to call your DataFrame by typing it’s name as the last line of your code cell Then, you will be able to run the test cell below and find out if your answer is correct.\n\nrapid_df = pd.read_csv(\n  rapid_url,\n  index_col='something')\nrapid_df\n\n\n# DO NOT MODIFY THIS TEST CELL\ntmax_df_resp = _\npoints = 0\n\nif isinstance(tmax_df_resp, pd.DataFrame):\n    points += 1\n    print('\\u2705 Great work! You called a DataFrame.')\nelse:\n    print('\\u274C Oops - make sure to call your DataFrame for testing.')\n\nprint('You earned {} of 2 points for downloading data'.format(points))\n\n\nHINT: Check out the type() function below - you can use it to check that your data is now in DataFrame type object\n\n\n# Check that the data was imported into a pandas DataFrame\ntype(rapid_df)\n\n\n Clean up your DataFrame\nUse double brackets to only select the columns you want in your DataFrame\n\nMake sure to call your DataFrame by typing it’s name as the last line of your code cell Then, you will be able to run the test cell below and find out if your answer is correct.\n\nrapid_df = rapid_df[['some_col', 'another_col']]\nrapid_df\n\n\n# DO NOT MODIFY THIS TEST CELL\ntmax_df_resp = _\npoints = 0\n\nsummary = [round(val, 2) for val in tmax_df_resp.mean().values]\nif summary == [0.05, 54.53]:\n    points += 4\n    print('\\u2705 Great work! You correctly downloaded data.')\nelse:\n    print('\\u274C Oops - your data are not correct.')\nprint('You earned {} of 5 points for downloading data'.format(points))"
  },
  {
    "objectID": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#plot-the-precpitation-column-prcp-vs-time-to-explore-the-data",
    "href": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#plot-the-precpitation-column-prcp-vs-time-to-explore-the-data",
    "title": "Climate change is impacting the way people live around the world",
    "section": "Plot the precpitation column (PRCP) vs time to explore the data",
    "text": "Plot the precpitation column (PRCP) vs time to explore the data\nPlotting in Python is easy, but not quite this easy:\n\nrapid_df.plot()\n\n\n****Label and describe your plots****\n\n\n\nSource: https://xkcd.com/833\n\n\nMake sure each plot has:\n\nA title that explains where and when the data are from\nx- and y- axis labels with units where appropriate\nA legend where appropriate\n\n\nYou’ll always need to add some instructions on labels and how you want your plot to look.\n\n Your task:\n\nChange dataframe to your DataFrame name.\nChange y= to the name of your observed temperature column name.\nUse the title, ylabel, and xlabel parameters to add key text to your plot.\nAdjust the size of your figure using figsize=(x,y) where x is figure width and y is figure height\n\n\nHINT: labels have to be a type in Python called a string. You can make a string by putting quotes around your label, just like the column names in the sample code (eg y='TOBS').\n\n\n\n# Plot the data using .plot\nrapid_df.plot(\n    y='the_precipitation_column',\n    title='Title Goes Here',\n    xlabel='Horizontal Axis Label Goes Here',\n    ylabel='Vertical Axis Label Goes Here')\n\n\n Want an EXTRA CHALLENGE?\nThere are many other things you can do to customize your plot. Take a look at the pandas plotting galleries and the documentation of plot to see if there’s other changes you want to make to your plot. Some possibilities include:\n\nRemove the legend since there’s only one data series\nIncrease the figure size\nIncrease the font size\nChange the colors\nUse a bar graph instead (usually we use lines for time series, but since this is annual it could go either way)\nAdd a trend line\n\nNot sure how to do any of these? Try searching the internet, or asking an AI!\n\n\n\n Convert units\nModify the code below to add a column that includes temperature in Celsius. The code below was written by your colleague. Can you fix this so that it correctly calculates temperature in Celsius and adds a new column?\n\n\n# Convert to celcius\ndataframe.loc[:, 'TCel'] = dataframe['temperature_col_name'] - 32 * 5 / 9\ndataframe\n\n\n# DO NOT MODIFY THIS TEST CELL\ntmax_df_resp = _\npoints = 0\n\nif isinstance(tmax_df_resp, pd.DataFrame):\n    points += 1\n    print('\\u2705 Great work! You called a DataFrame.')\nelse:\n    print('\\u274C Oops - make sure to call your DataFrame for testing.')\n\nsummary = [round(val, 2) for val in tmax_df_resp.mean().values]\nif summary == [0.05, 54.53, 12.52]:\n    points += 4\n    print('\\u2705 Great work! You correctly converted to Celcius.')\nelse:\n    print('\\u274C Oops - your data are not correct.')\nprint('You earned {} of 5 points for converting to Celcius'.format(points))\n\n\n Want an EXTRA CHALLENGE?\n\nAs you did above, rewrite the code to be more expressive\nUsing the code below as a framework, write and apply a function that converts to Celcius. &gt; Functions let you reuse code you have already written\nYou should also rewrite this function and parameter names to be more expressive.\n\n\n\ndef a_function(a_parameter):\n    \"\"\"Convert temperature to Celcius\"\"\"\n    return a_parameter # Put your equation in here\n\ndataframe['celcius_column'] = dataframe['fahrenheit_column'].apply(convert)"
  },
  {
    "objectID": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#subsetting-and-resampling",
    "href": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#subsetting-and-resampling",
    "title": "Climate change is impacting the way people live around the world",
    "section": "Subsetting and Resampling",
    "text": "Subsetting and Resampling\nOften when working with time-series data you may want to focus on a shorter window of time, or look at weekly, monthly, or annual summaries to help make the analysis more manageable.\n\n Read more\nRead more about subsetting and resampling time-series data in our Learning Portal.\n\nFor this demonstration, we will look at the last 40 years worth of data and resample to explore a summary from each year that data were recorded.\n\n Your task\n\nReplace start-year and end-year with 1983 and 2023\nReplace dataframe with the name of your data\nReplace new_dataframe with something more expressive\nCall your new variable\nRun the cell\n\n\n\n# Subset the data\nnew_dataframe = dataframe.loc['start-year':'end-year']\nnew_dataframe\n\n\n# DO NOT MODIFY THIS TEST CELL\ndf_resp = _\npoints = 0\n\nif isinstance(df_resp, pd.DataFrame):\n    points += 1\n    print('\\u2705 Great work! You called a DataFrame.')\nelse:\n    print('\\u274C Oops - make sure to call your DataFrame for testing.')\n\nsummary = [round(val, 2) for val in df_resp.mean().values]\nif summary == [0.06, 55.67, 13.15]:\n    points += 5\n    print('\\u2705 Great work! You correctly converted to Celcius.')\nelse:\n    print('\\u274C Oops - your data are not correct.')\nprint('You earned {} of 5 points for subsetting'.format(points))"
  },
  {
    "objectID": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#now-we-are-ready-to-calculate-annual-statistics",
    "href": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#now-we-are-ready-to-calculate-annual-statistics",
    "title": "Climate change is impacting the way people live around the world",
    "section": "Now we are ready to calculate annual statistics",
    "text": "Now we are ready to calculate annual statistics\nHere you will resample the 1983-2023 data to look the annual mean values.\n\n Resample your data\n\nReplace new_dataframe with the variable you created in the cell above where you subset the data\nReplace 'TIME' with a 'W', 'M', or 'Y' depending on whether you’re doing a weekly, monthly, or yearly summary\nReplace STAT with a sum, min, max, or mean depending on what kind of statistic you’re interested in calculating.\nReplace resampled_data with a more expressive variable name\nCall your new variable\nRun the cell\n\n\n\n# Resample the data to look at yearly mean values\nresampled_data = new_dataframe.resample('TIME').STAT()\nresampled_data\n\n\n# DO NOT MODIFY THIS TEST CELL\ndf_resp = _\npoints = 0\n\nif isinstance(df_resp, pd.DataFrame):\n    points += 1\n    print('\\u2705 Great work! You called a DataFrame.')\nelse:\n    print('\\u274C Oops - make sure to call your DataFrame for testing.')\n\nsummary = [round(val, 2) for val in df_resp.mean().values]\nif summary == [0.06, 55.37, 12.99]:\n    points += 5\n    print('\\u2705 Great work! You correctly converted to Celcius.')\nelse:\n    print('\\u274C Oops - your data are not correct.')\nprint('You earned {} of 5 points for resampling'.format(points))\n\n\n Plot your resampled data\n\n\n# Plot mean annual temperature values\n\n\n\n Describe your plot\nWe like to use an approach called “Assertion-Evidence” for presenting scientific results. There’s a lot of video tutorials and example talks available on the Assertion-Evidence web page. The main thing you need to do now is to practice writing a message or headline rather than descriptions or topic sentences for the plot you just made (what they refer to as “visual evidence”).\nFor example, it would be tempting to write something like “A plot of maximum annual temperature in Rapid City, Colorado over time (1983-2023)”. However, this doesn’t give the reader anything to look at, or explain why we made this particular plot (we know, you made this one because we told you to)\nSome alternatives for different plots of Rapid City temperature that are more of a starting point for a presentation or conversation are:\n\nRapid City, SD experienced cooler than average temperatures in 1995\nTemperatures in Rapid City, SD appear to be on the rise over the past 40 years\nMaximum annual temperatures in Rapid City, CO are becoming more variable over the previous 40 years\n\nWe could back up some of these claims with further analysis included later on, but we want to make sure that our audience has some guidance on what to look for in the plot."
  },
  {
    "objectID": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#your-rapid-city-plot-headline-here",
    "href": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#your-rapid-city-plot-headline-here",
    "title": "Climate change is impacting the way people live around the world",
    "section": "YOUR Rapid City PLOT HEADLINE HERE 📰 🗞️ 📻",
    "text": "YOUR Rapid City PLOT HEADLINE HERE 📰 🗞️ 📻\nDescribe your plot in this cell in 2-3 sentences\n\n\n\nWriting bear\n\n\n\nImage credit: https://www.craiyon.com/image/OAbZtyelSoS7FdGko6hvQg"
  },
  {
    "objectID": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#your-turn-pick-a-new-location-andor-measurement-to-plot",
    "href": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#your-turn-pick-a-new-location-andor-measurement-to-plot",
    "title": "Climate change is impacting the way people live around the world",
    "section": "Your turn: pick a new location and/or measurement to plot 🌏 📈",
    "text": "Your turn: pick a new location and/or measurement to plot 🌏 📈\nBelow (or in a new notebook!), recreate the workflow you just did in a place that interests you OR with a different measurement. See the instructions above to adapt the URL that we created for Rapid City, CO using the NCEI API. You will need to make your own new Markdown and Code cells below this one, or create a new notebook."
  },
  {
    "objectID": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#congratulations-youre-almost-done-with-this-coding-challenge-now-make-sure-that-your-code-is-reproducible",
    "href": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#congratulations-youre-almost-done-with-this-coding-challenge-now-make-sure-that-your-code-is-reproducible",
    "title": "Climate change is impacting the way people live around the world",
    "section": "Congratulations, you’re almost done with this coding challenge 🤩 – now make sure that your code is reproducible",
    "text": "Congratulations, you’re almost done with this coding challenge 🤩 – now make sure that your code is reproducible\n\n\nImage source: https://dfwurbanwildlife.com/2018/03/25/chris-jacksons-dfw-urban-wildlife/snow-geese-galore/\n\n\n Your task\n\nIf you didn’t already, go back to the code you modified about and write more descriptive comments so the next person to use this code knows what it does.\n\n\n\nMake sure to Restart and Run all up at the top of your notebook. This will clear all your variables and make sure that your code runs in the correct order. It will also export your work in Markdown format, which you can put on your website.\n\n\nAlways run your code start to finish before submitting!\nBefore you commit your work, make sure it runs reproducibly by clicking:\n\nRestart (this button won’t appear until you’ve run some code), then\nRun All"
  },
  {
    "objectID": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#bonus-create-a-shareable-markdown-of-your-work",
    "href": "notebooks/01b-get-started-api/Get Started with Open Reproducible Science!.out.html#bonus-create-a-shareable-markdown-of-your-work",
    "title": "Climate change is impacting the way people live around the world",
    "section": "BONUS: Create a shareable Markdown of your work",
    "text": "BONUS: Create a shareable Markdown of your work\nBelow is some code that you can run that will save a Markdown file of your work that is easily shareable and can be uploaded to GitHub Pages. You can use it as a starting point for writing your portfolio post!\n\n%%capture\n%%bash\njupyter nbconvert *.ipynb --to markdown"
  },
  {
    "objectID": "notebooks/00-header-nb.html",
    "href": "notebooks/00-header-nb.html",
    "title": "It’s another STARS 2023 Earth Data Science Workflow!",
    "section": "",
    "text": "This notebook contains your next earth data science coding challenge! Before we get started, make sure to read or review the guidelines below. These will help make sure that your code is readable and reproducible.\n\n\n\n\nImage source: https://alaskausfws.medium.com/whats-big-and-brown-and-loves-salmon-e1803579ee36\n\nThese are the most common issues that will keep you from getting started and delay your code review:\n\nWhen you try to run some code, you may be prompted to select a kernel.\n\nThe kernel refers to the version of Python you are using\nYou should use the base kernel, which should be the default option.\nYou can also use the Select Kernel menu in the upper right to select the base kernel\n\nBefore you commit your work, make sure it runs reproducibly by clicking:\n\nRestart (this button won’t appear until you’ve run some code), then\nRun All\n\n\n\n\n\n\n\nFormat all cells prior to submitting (right click on your code).\nUse expressive names for variables so you or the reader knows what they are.\nUse comments to explain your code – e.g. \n# This is a comment, it starts with a hash sign\n\n\n\n\n\n\n\nSource: https://xkcd.com/833\n\n\nMake sure each plot has: * A title that explains where and when the data are from * x- and y- axis labels with units where appropriate * A legend where appropriate\n\n\n\nWe use the following icons to let you know when you need to change something to complete the challenge: * 💻 means you need to write or edit some code.\n\n📖 indicates recommended reading\n✎ marks written responses to questions\n🌶 is an optional extra challenge"
  },
  {
    "objectID": "notebooks/00-header-nb.html#dont-get-caught-by-these-jupyter-notebook-gotchas",
    "href": "notebooks/00-header-nb.html#dont-get-caught-by-these-jupyter-notebook-gotchas",
    "title": "It’s another STARS 2023 Earth Data Science Workflow!",
    "section": "",
    "text": "Image source: https://alaskausfws.medium.com/whats-big-and-brown-and-loves-salmon-e1803579ee36\n\nThese are the most common issues that will keep you from getting started and delay your code review:\n\nWhen you try to run some code, you may be prompted to select a kernel.\n\nThe kernel refers to the version of Python you are using\nYou should use the base kernel, which should be the default option.\nYou can also use the Select Kernel menu in the upper right to select the base kernel\n\nBefore you commit your work, make sure it runs reproducibly by clicking:\n\nRestart (this button won’t appear until you’ve run some code), then\nRun All"
  },
  {
    "objectID": "notebooks/00-header-nb.html#check-your-code-to-make-sure-its-clean-and-easy-to-read",
    "href": "notebooks/00-header-nb.html#check-your-code-to-make-sure-its-clean-and-easy-to-read",
    "title": "It’s another STARS 2023 Earth Data Science Workflow!",
    "section": "",
    "text": "Format all cells prior to submitting (right click on your code).\nUse expressive names for variables so you or the reader knows what they are.\nUse comments to explain your code – e.g. \n# This is a comment, it starts with a hash sign"
  },
  {
    "objectID": "notebooks/00-header-nb.html#label-and-describe-your-plots",
    "href": "notebooks/00-header-nb.html#label-and-describe-your-plots",
    "title": "It’s another STARS 2023 Earth Data Science Workflow!",
    "section": "",
    "text": "Source: https://xkcd.com/833\n\n\nMake sure each plot has: * A title that explains where and when the data are from * x- and y- axis labels with units where appropriate * A legend where appropriate"
  },
  {
    "objectID": "notebooks/00-header-nb.html#icons-how-to-use-this-notebook",
    "href": "notebooks/00-header-nb.html#icons-how-to-use-this-notebook",
    "title": "It’s another STARS 2023 Earth Data Science Workflow!",
    "section": "",
    "text": "We use the following icons to let you know when you need to change something to complete the challenge: * 💻 means you need to write or edit some code.\n\n📖 indicates recommended reading\n✎ marks written responses to questions\n🌶 is an optional extra challenge"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the ESIIL Learning Portal!",
    "section": "",
    "text": "Welcome to the ESIIL Learning Portal!\nExplore textbooks:\n\nIntroduction to Earth Data Science\nESIIL Data Short Course\nESIIL STARS Textbook"
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/05-map.html",
    "href": "pages/03-git-github/03-github-portfolio/05-map.html",
    "title": "Add a map to your website",
    "section": "",
    "text": "Check out our video demo for adding a map to your portfolio:\n\n \n\nDEMO: Add a map to your portfolio by ESIIL\n\n\n\n\n\nVector data are composed of discrete geometric locations (x and y values, or latitude and longitude) that define the “shape” of the spatial object. The organization of the vertices determines the type of vector that you are working with. There are three fundamental types of vector data:\nPoints: Each individual point is defined by a single x, y coordinate. Examples of point data include: sampling locations, the location of individual trees or the location of plots.\nLines: Lines are composed of many (at least 2) vertices, or points, that are connected. For instance, a road or a stream may be represented by a line. This line is composed of a series of segments, each bend in the road or stream represents a vertex that has defined x, y location.\nPolygons: A polygon consists of 3 or more vertices that are connected and closed. Thus, the outlines of plot boundaries, lakes, oceans, and states or countries are often represented by polygons.\n\n\n\nThere are three types of vector data – point, line, and polygon\n\n\n\n\n\n\n\n\nTip\n\n\n\nRead more about working with spatial data using Python in our Intro to Earth Data Science, here.\n\n\n\n\n\nYou’ll need to start by importing some libraries to have access to all the code you need.\nTo run code in Codespaces, click on the play button in the upper left corner of the code you want to run. You may be asked to “Select a kernel”. If you press Return or click on base up at the top, that should select the default kernel.\n\n# Work with vector data\nimport geopandas as gpd\n\n# Save maps and plots to files\nimport holoviews as hv\n# Create interactive maps and plots\nimport hvplot.pandas\n\n# Search for locations by name - this might take a moment\nfrom osmnx import features as osm\n\n\n\n\nYou can use the osmnx package to download and search for spatial vector data in your area, or anywhere around the world.\nIn this case, we’re looking for the location of the United Tribes Technical College campus in North Dakota. The address in here, 'United Tribes Technical College, Bismarck, ND, United States', does not have to be complete or exact, but it should be specific enough to narrow it down.\n\n\n\n\n\n\nTip\n\n\n\nYou can use the Open Street Maps website to fine-tune your address before you copy it into your code.\n\n\nWe are also specifying that we want it to be tagged as a 'college' type of‘amenity’` type. You might have to try a couple different searches with different addresses and/or tags to get the address you want, just like if you are using a map website or app.\n\n\n\n\n\n\nTip\n\n\n\nCheck out the list of all the different amenity types available on Open Street Maps! Different amenity types might be different types of vector data, such as a point location or a building footprint polygon.\n\n\n\n# Search for United Tribes Technical College\nuttc_gdf = osm.features_from_address(\n    'United Tribes Technical College, Bismarck, ND, United States',\n    {'amenity': ['college']})\nuttc_gdf\n\n\nuttc_gdf.plot()\n\nWe have a map of the UTTC Campus!\n\n\n\n\n\n\nWarning\n\n\n\nThe Open Street Maps (OSM) database is not always complete. For example, try searching for UTTC with the {'building': True}, and compare it to the map of the UTTC campus on their website. What do you notice?\n\n\n\n\n\nThere are lots of different ways to create maps and plots in Python. Here, we’re going to use a tool called 'hvplot' and 'geoviews' to create an interactive map, including the online 'EsriImagery' tile source basemap.\n\n# Plot UTTC boundary\nuttc_map = uttc_gdf.hvplot(\n    # Givethe map a descriptive title\n    title=\"United Tribes Technical College, Bismarck, ND\",\n    # Add a basemap\n    geo=True, tiles='EsriImagery',\n    # Change the colors\n    fill_color='white', fill_alpha=0.2,\n    line_color='skyblue', line_width=5,\n    # Change the image size\n    frame_width=400, frame_height=400)\n\n# Save the map as a file to put on the web\nhv.save(uttc_map, 'uttc.html')\n\n# Display the map\nuttc_map\n\n\n\n\nIf you are doing this activity on GitHub Codespaces, you will need to download the map you created:\n\nOpen the Folders tab on the left hand side\nRight-click on uttc.html (or whatever you named your file)\nSelect Download...\n\nThis should download your map.\n::: {.content-hidden when-profile=“nb”}\n\n\n\nYou are now ready to upload your map to your portfolio repository and place it in your webpage. Because it is HTML and not an image, you will need to use the following HTML to get it on your page:\n&lt;embed type=\"text/html\" src=\"uttc.html\" width=\"600\" height=\"600\"&gt;\n\n\n\n\n\n\n\nImportant\n\n\n\nMake sure to make the width and height of your embed element larger than the frame_width and frame_height of your plot, or it will get cut off!\n\n\n:::"
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/05-map.html#get-started-with-map-making-using-open-sources-tools",
    "href": "pages/03-git-github/03-github-portfolio/05-map.html#get-started-with-map-making-using-open-sources-tools",
    "title": "Add a map to your website",
    "section": "",
    "text": "Check out our video demo for adding a map to your portfolio:\n\n \n\nDEMO: Add a map to your portfolio by ESIIL\n\n\n\n\n\nVector data are composed of discrete geometric locations (x and y values, or latitude and longitude) that define the “shape” of the spatial object. The organization of the vertices determines the type of vector that you are working with. There are three fundamental types of vector data:\nPoints: Each individual point is defined by a single x, y coordinate. Examples of point data include: sampling locations, the location of individual trees or the location of plots.\nLines: Lines are composed of many (at least 2) vertices, or points, that are connected. For instance, a road or a stream may be represented by a line. This line is composed of a series of segments, each bend in the road or stream represents a vertex that has defined x, y location.\nPolygons: A polygon consists of 3 or more vertices that are connected and closed. Thus, the outlines of plot boundaries, lakes, oceans, and states or countries are often represented by polygons.\n\n\n\nThere are three types of vector data – point, line, and polygon\n\n\n\n\n\n\n\n\nTip\n\n\n\nRead more about working with spatial data using Python in our Intro to Earth Data Science, here.\n\n\n\n\n\nYou’ll need to start by importing some libraries to have access to all the code you need.\nTo run code in Codespaces, click on the play button in the upper left corner of the code you want to run. You may be asked to “Select a kernel”. If you press Return or click on base up at the top, that should select the default kernel.\n\n# Work with vector data\nimport geopandas as gpd\n\n# Save maps and plots to files\nimport holoviews as hv\n# Create interactive maps and plots\nimport hvplot.pandas\n\n# Search for locations by name - this might take a moment\nfrom osmnx import features as osm\n\n\n\n\nYou can use the osmnx package to download and search for spatial vector data in your area, or anywhere around the world.\nIn this case, we’re looking for the location of the United Tribes Technical College campus in North Dakota. The address in here, 'United Tribes Technical College, Bismarck, ND, United States', does not have to be complete or exact, but it should be specific enough to narrow it down.\n\n\n\n\n\n\nTip\n\n\n\nYou can use the Open Street Maps website to fine-tune your address before you copy it into your code.\n\n\nWe are also specifying that we want it to be tagged as a 'college' type of‘amenity’` type. You might have to try a couple different searches with different addresses and/or tags to get the address you want, just like if you are using a map website or app.\n\n\n\n\n\n\nTip\n\n\n\nCheck out the list of all the different amenity types available on Open Street Maps! Different amenity types might be different types of vector data, such as a point location or a building footprint polygon.\n\n\n\n# Search for United Tribes Technical College\nuttc_gdf = osm.features_from_address(\n    'United Tribes Technical College, Bismarck, ND, United States',\n    {'amenity': ['college']})\nuttc_gdf\n\n\nuttc_gdf.plot()\n\nWe have a map of the UTTC Campus!\n\n\n\n\n\n\nWarning\n\n\n\nThe Open Street Maps (OSM) database is not always complete. For example, try searching for UTTC with the {'building': True}, and compare it to the map of the UTTC campus on their website. What do you notice?\n\n\n\n\n\nThere are lots of different ways to create maps and plots in Python. Here, we’re going to use a tool called 'hvplot' and 'geoviews' to create an interactive map, including the online 'EsriImagery' tile source basemap.\n\n# Plot UTTC boundary\nuttc_map = uttc_gdf.hvplot(\n    # Givethe map a descriptive title\n    title=\"United Tribes Technical College, Bismarck, ND\",\n    # Add a basemap\n    geo=True, tiles='EsriImagery',\n    # Change the colors\n    fill_color='white', fill_alpha=0.2,\n    line_color='skyblue', line_width=5,\n    # Change the image size\n    frame_width=400, frame_height=400)\n\n# Save the map as a file to put on the web\nhv.save(uttc_map, 'uttc.html')\n\n# Display the map\nuttc_map\n\n\n\n\nIf you are doing this activity on GitHub Codespaces, you will need to download the map you created:\n\nOpen the Folders tab on the left hand side\nRight-click on uttc.html (or whatever you named your file)\nSelect Download...\n\nThis should download your map.\n::: {.content-hidden when-profile=“nb”}\n\n\n\nYou are now ready to upload your map to your portfolio repository and place it in your webpage. Because it is HTML and not an image, you will need to use the following HTML to get it on your page:\n&lt;embed type=\"text/html\" src=\"uttc.html\" width=\"600\" height=\"600\"&gt;\n\n\n\n\n\n\n\nImportant\n\n\n\nMake sure to make the width and height of your embed element larger than the frame_width and frame_height of your plot, or it will get cut off!\n\n\n:::"
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html",
    "href": "notebooks/01a-get-started/get-started.html",
    "title": "Get started with open reproducible science!",
    "section": "",
    "text": "(Earth Science Data Systems, 2021)\nOpen reproducible science makes scientific methods, data and outcomes available to everyone. That means that everyone who wants should be able to find, read, understand, and run your workflows for themselves.\nFew if any science projects are 100% open and reproducible (yet!). However, members of the open science community have developed open source tools and practices that can help you move toward that goal. You will learn about many of those tools in the Intro to Earth Data Science textbook. Don’t worry about learning all the tools at once – we’ve picked a few for you to get started with.\n\n\n\n\n\n\n Further reading\n\n\n\nRead our textbook chapter about open reproducible science.\n\n\n\n\n\n\n\n\n What does open reproducible science mean to you?\n\n\n\n Create a new Markdown cell below this one using the + Code button in the upper left.\n In the new cell, answer the following questions using a numbered list in Markdown:\n\nIn 1-2 sentences, define open reproducible science.\nIn 1-2 sentences, choose one of the open source tools that you have learned about (i.e. Shell, Git/GitHub, Jupyter Notebook, Python) and explain how it supports open reproducible science.\n\n\n\n\n\n\n\n\n\n Human-readable and Machine-readable\n\n\n\n Create a new Markdown cell below this one using the ESC + b` keyboard shortcut.\n In the new cell, answer the following question in a Markdown quote:\n\nIn 1-2 sentences, does this Jupyter Notebook file have a machine-readable name? Explain your answer."
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html#open-reproducible-science",
    "href": "notebooks/01a-get-started/get-started.html#open-reproducible-science",
    "title": "Get started with open reproducible science!",
    "section": "",
    "text": "(Earth Science Data Systems, 2021)\nOpen reproducible science makes scientific methods, data and outcomes available to everyone. That means that everyone who wants should be able to find, read, understand, and run your workflows for themselves.\nFew if any science projects are 100% open and reproducible (yet!). However, members of the open science community have developed open source tools and practices that can help you move toward that goal. You will learn about many of those tools in the Intro to Earth Data Science textbook. Don’t worry about learning all the tools at once – we’ve picked a few for you to get started with.\n\n\n\n\n\n\n Further reading\n\n\n\nRead our textbook chapter about open reproducible science.\n\n\n\n\n\n\n\n\n What does open reproducible science mean to you?\n\n\n\n Create a new Markdown cell below this one using the + Code button in the upper left.\n In the new cell, answer the following questions using a numbered list in Markdown:\n\nIn 1-2 sentences, define open reproducible science.\nIn 1-2 sentences, choose one of the open source tools that you have learned about (i.e. Shell, Git/GitHub, Jupyter Notebook, Python) and explain how it supports open reproducible science.\n\n\n\n\n\n\n\n\n\n Human-readable and Machine-readable\n\n\n\n Create a new Markdown cell below this one using the ESC + b` keyboard shortcut.\n In the new cell, answer the following question in a Markdown quote:\n\nIn 1-2 sentences, does this Jupyter Notebook file have a machine-readable name? Explain your answer."
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html#readable-well-documented-scientific-workflows-are-easier-to-reproduce",
    "href": "notebooks/01a-get-started/get-started.html#readable-well-documented-scientific-workflows-are-easier-to-reproduce",
    "title": "Get started with open reproducible science!",
    "section": "Readable, well-documented scientific workflows are easier to reproduce",
    "text": "Readable, well-documented scientific workflows are easier to reproduce\nAs the comic below suggests, code that is hard to read is also hard to get working. We refer to code that is easy to read as clean code.\n\n\n\nAnd because if you just leave it there, it’s going to start contaminating things downstream even if no one touches it directly. (from https://xkcd.com/2138/)\n\n\n In the prompt below, list 3 things you can do to write clean code, and then list 3 more advantages of doing so. * Double click on the cell to edit * You can use examples from the textbook, or come up with your own. * Use Markdown to format your list.\n=== BEGIN MARK SCHEME ===\n(1 pt each) demonstrating understanding of clean code practices and advantages\n(2 pt) Correct spelling and grammar\n=== END MARK SCHEME ===\nI can write clean code by: * YOUR ANSWER HERE\nAdvantages of clean code include: * YOUR ANSWER HERE"
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html#what-the-fork-who-wrote-this",
    "href": "notebooks/01a-get-started/get-started.html#what-the-fork-who-wrote-this",
    "title": "Get started with open reproducible science!",
    "section": "What the fork?! Who wrote this?",
    "text": "What the fork?! Who wrote this?\nBelow is a scientific Python workflow. But something’s wrong – The code won’t run! Your task is to follow the instructions below to clean and debug the Python code below so that it runs. &gt; Don’t worry if you can’t solve every bug right away. We’ll get there! The most important thing is to identify problems with the code and write high-quality GitHub Issues\nAt the end, you’ll repeat the workflow for a location and measurement of your choosing.\n\nAlright! Let’s clean up this code. First things first…\n Rename this notebook if necessary with an expressive and machine-readable file name"
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html#python-packages-let-you-use-code-written-by-experts-around-the-world",
    "href": "notebooks/01a-get-started/get-started.html#python-packages-let-you-use-code-written-by-experts-around-the-world",
    "title": "Get started with open reproducible science!",
    "section": "Python packages let you use code written by experts around the world",
    "text": "Python packages let you use code written by experts around the world\nBecause Python is open source, lots of different people and organizations can contribute (including you!). Many contributions are in the form of packages which do not come with a standard Python download. Read more in your textbook: *  Packages need to be installed and imported.\n\n In the cell below, someone was trying to import the pandas package, which helps us to work with tabular data such as comma-separated value or csv files.\n\n Your task – uncomment the code in the cell below by removeing the # symbol on the left of line 2, and correct the typo to properly import the pandas package under its alias pd.\n\n#can't get this to work :(\n#import pands as pd\n\n# BEGIN SOLUTION\nimport pandas as pd\n# END SOLUTION\n\nOnce you have run the cell above and imported pandas, run the cell below. It is a test cell that will tell you if you completed the task successfully. If a test cell isn’t working the way you expect, check that you ran your code immediately before running the test.\n\n# DO NOT MODIFY THIS TEST CELL\npoints = 0\ntry:\n    pd.DataFrame()\n    points += 5\n    print('\\u2705 Great work! You correctly imported the pandas library.')\nexcept:\n    print('\\u274C Oops - pandas was not imported correctly.')\nprint('You earned {} of 5 points for importing pandas'.format(points))"
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html#there-are-more-earth-observation-data-online-than-any-one-person-could-ever-look-at",
    "href": "notebooks/01a-get-started/get-started.html#there-are-more-earth-observation-data-online-than-any-one-person-could-ever-look-at",
    "title": "Get started with open reproducible science!",
    "section": "There are more Earth Observation data online than any one person could ever look at",
    "text": "There are more Earth Observation data online than any one person could ever look at\nNASA’s Earth Observing System Data and Information System (EOSDIS) alone manages over 9PB of data. 1 PB is roughly 100 times the entire Library of Congress (a good approximation of all the books available in the US). It’s all available to you once you learn how to download what you want.\nThe following workflow looks at maximum daily average temperatures over time in Rapid City, South Dakota. This notebook uses data from the National Centers for Environmental Information (NCEI). Check out the NCEI Climate at a Glance website where you can search for more data like this. &gt; Wait a second - what is maximum daily average temperature? NCEI first takes the daily average temperature. Then, they take the annual maximum. You’ll notice these temperatures are a bit lower than we would expect from maxima - that’s because nighttime temperatures get incorporated into the daily average.\n Your task: 1. Research the Climate at a Glance data source. 2. In the cell below, write a 2-3 sentence description of the data source. You should describe: - who takes the data - where the data were taken - what the maximum temperature units are - how the data are collected. 3. Include a citation of the data (HINT: NCEI has a section for ‘Citing this page’, but you will have to select a particular dataset such as City &gt; Time Series).\n===BEGIN MARK SCHEME ===\n“Climate at a Glance was developed at the request of NOAA Headquarters for near real-time analysis of monthly temperature and precipitation data across the contiguous U.S. and intended for the study of climate variability…Because these data are primarily intended for the study of climate variability, observations have been adjusted to account for the artificial effects introduced into the climate record by factors such as instrument changes, station relocation, observer practice changes and urbanization.”\nNOAA National Centers for Environmental information, Climate at a Glance: City Time Series, published February 2023, retrieved on February 26, 2023 from https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/city/time-series\n=== END MARK SCHEME ===\nYOUR DATA DESCRIPTION AND CITATION HERE"
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html#you-can-access-ncei-climate-at-a-glance-data-from-the-internet-using-its-url",
    "href": "notebooks/01a-get-started/get-started.html#you-can-access-ncei-climate-at-a-glance-data-from-the-internet-using-its-url",
    "title": "Get started with open reproducible science!",
    "section": "You can access NCEI Climate At a Glance Data from the internet using its URL",
    "text": "You can access NCEI Climate At a Glance Data from the internet using its URL\nThe cell below contains the URL for the data you will use in this part of the notebook. We got that URL by right-clicking on the blue CSV download button. You don’t have to do that just yet – this URL is correct! However, we still have a problem - we can’t get the URL back later on because it isn’t saved in a variable. In other words, we need to give the url a name so that we can request in from Python later (sadly, Python has no ‘hey what was that thingy I typed earlier?’ function)\n Check out the textbook section on variables\n Your task: 1. Pick an expressive variable name for the URL &gt; HINT: click on the Variables button up top to see all your variables. Your new url variable will not be there until you define it and run the code 2. Reformat the URL so that it adheres to the 79-character PEP-8 line limit &gt; HINT: You should see two vertical lines in each cell - don’t let your code go past the second line 3. At the end of the cell where you define your url variable, call your variable (type out its name) so it can be tested.\n\n'https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/city/time-series/USW00024090/tmax/ann/2/1949-2023.csv'\n\n# BEGIN SOLUTION\nmy_url = (\n    \"https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/\"\n    \"city/time-series/USW00024090/tmax/ann/2/1949-2023.csv\")\nprint(len(my_url))\nmy_url\n# END SOLUTION\n\n\n# DO NOT MODIFY THIS TEST CELL\nresp_url = _\npoints = 0\n\nif type(resp_url)==str:\n    points += 3\n    print('\\u2705 Great work! You correctly called your url variable.')\nelse:\n    print('\\u274C Oops - your url variable was not called correctly.')\n\nif len(resp_url)==117:\n    points += 3\n    print('\\u2705 Great work! Your url is the correct length.')\nelse:\n    print('\\u274C Oops - your url variable is not the correct length.')\n\nprint('You earned {} of 6 points for defining a url variable'.format(points))"
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html#download-and-get-started-working-with-ncei-data",
    "href": "notebooks/01a-get-started/get-started.html#download-and-get-started-working-with-ncei-data",
    "title": "Get started with open reproducible science!",
    "section": "Download and get started working with NCEI data",
    "text": "Download and get started working with NCEI data\nThe pandas library you imported can download data from the internet directly into a type of Python object called a DataFrame. In the code cell below, you can see an attempt to do just this. But there are some problems…\n What do you notice about the code below? Answer the following questions in this cell:\n\nWhat do you think the parameters of the pd.read_csv() function are supposed to do? &gt; HINT: Check out the pandas read_csv() documentation for more info. You can also try changing the values and running the cell to see what happens! * my_url: YOUR ANSWER HERE * header: YOUR ANSWER HERE * names: YOUR ANSWER HERE\nAre the data importing correctly? Why or why not?\nYOUR ANSWER HERE\nWhat are two things you could do to make this code more expressive? 1. YOUR ANSWER HERE 2. YOUR ANSWER HERE\n\n You’re ready to fix some code! Your task is to: 1. Make any changes needed to get this code to run. Here’s some hints: &gt; HINT: The my_url variable doesn’t exist - you need to replace it with the variable name you chose. 2. Modify the value of the header parameter so that only numeric data values are included in each column. 3. Clean up the code by using expressive variable names, expressive column names, PEP-8 compliant code, and descriptive comments\nMake sure to call your DataFrame by typing it’s name as the last line of your code cell Then, you will be able to run the test cell below and find out if your answer is correct.\n=== BEGIN MARK SCHEME ===\n(2 pts) Expressive column names\n\n(2 pts) Expressive variable names\n\n(1 pt) PEP-8\n\n(1 pt) Descriptive comment\n=== END MARK SCHEME ===\n\n#download\ndataframe = pd.read_csv(my_url, header=2, names=['col_1', 'col_2'])\ndataframe\n\n# BEGIN SOLUTION\n# Fix the dataframe variable\ndataframe = pd.read_csv(my_url, header=3, names=['col_1', 'col_2'])\n\n# Read Annual Maximum Temperature Data in to a DataFrame\nsd_tmax_df = pd.read_csv(\n    my_url, \n    header=3,\n    names=['year', 'temperature_f'])\nprint([round(val, 2) for val in sd_tmax_df.mean().values])\nsd_tmax_df\n# END SOLUTION\n\n\n# DO NOT MODIFY THIS TEST CELL\ntmax_df_resp = _\npoints = 0\n\nif isinstance(tmax_df_resp, pd.DataFrame):\n    points += 1\n    print('\\u2705 Great work! You called a DataFrame.')\nelse:\n    print('\\u274C Oops - make sure to call your DataFrame for testing.')\n    \nsummary = [round(val, 2) for val in tmax_df_resp.mean().values]\nif summary == [198562.0, 58.89]:\n    points += 4\n    print('\\u2705 Great work! You correctly downloaded data.')\nelse:\n    print('\\u274C Oops - your data are not correct.')\nprint('You earned {} of 5 points for downloading data'.format(points))\n\n\nHINT: Check out the type() function below - you can use it to check that your data is now in DataFrame type object\n\n\n# Check that the data was imported into a pandas DataFrame\ntype(dataframe)"
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html#cleaning-up-your-dataframe",
    "href": "notebooks/01a-get-started/get-started.html#cleaning-up-your-dataframe",
    "title": "Get started with open reproducible science!",
    "section": "Cleaning up your DataFrame",
    "text": "Cleaning up your DataFrame\nTake a look at your data. Do you want to use it as is, or does it need to be modified? The original author of this code thought it needed some modification, but didn’t document their work very well.\n Playing with code: your task\n\nReplace dataframe with the name of your dataframe whenever it appears.\nRun the code below.\n\n\n# ncei has wacky years\ndataframe.iloc[:,0] = dataframe.iloc[:,0] // 100\ndataframe\n# BEGIN SOLUTION\nsd_tmax_df.year = sd_tmax_df.year // 100\nprint([round(val, 2) for val in sd_tmax_df.mean().values])\nsd_tmax_df\n# END SOLUTION\n\n\n# DO NOT MODIFY THIS TEST CELL\ntmax_df_resp = _\npoints = 0\n\nif isinstance(tmax_df_resp, pd.DataFrame):\n    points += 1\n    print('\\u2705 Great work! You called a DataFrame.')\nelse:\n    print('\\u274C Oops - make sure to call your DataFrame for testing.')\n    \nsummary = [round(val, 2) for val in tmax_df_resp.mean().values]\nif summary == [1985.5, 58.89]:\n    points += 4\n    print('\\u2705 Great work! You correctly cleaned up years.')\nelse:\n    print('\\u274C Oops - your data are not correct.')\nprint('You earned {} of 5 points for cleaning up years'.format(points))\n\n Want an EXTRA CHALLENGE? Modify the code to be more expressive.\nRewrite the code below to select columns by name instead of by index. You might find the pandas User Guide section on slicing and dicing to be useful. However - don’t worry if you can’t figure this out yet! We’re going to talk a lot about how to use pandas DataFrames.\n What just happened? 1. +, -, * and / are known as operators in Python, and are used for arithmetic (add, subtract, multiply, and divide, respectively). What do you think the // operator does?\nYOUR ANSWER HERE\n\niloc is an attribute of DataFrames, meaning that it is available for all DataFrames by attaching .iloc to the end its name. What do you think the iloc is or does?\n\nYOUR ANSWER HERE\n\nHINT: It’s ok if you can’t figure out these questions yet. You can also list an experiment you tried. For example, you could run 4 // 2 and 4 // 3 in a new code cell and record the answers to help you figure out what // does. Or, you could change the 0 in .iloc[:,0] to 1 to see what happens. Play around with the code! It doesn’t cost you anything to try things when you’re coding."
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html#uh-oh-we-have-a-variable-problem",
    "href": "notebooks/01a-get-started/get-started.html#uh-oh-we-have-a-variable-problem",
    "title": "Get started with open reproducible science!",
    "section": "Uh-oh, we have a variable problem",
    "text": "Uh-oh, we have a variable problem\n Try running the cell above a second time. And a third time. What do you notice?\nYOUR ANSWER HERE\nYou don’t have to do anything about this now - you can go Run all to reset. In the future, there are two approaches we recommend to address this sort of problem: 1. Do not modify a DataFrame after it has been created - perform any changes you need in the same cell where you create the DataFrame using pd.read_csv(). 2. Save a copy of the DataFrame using the .copy() method of DataFrames and modify the copy (in the same cell)"
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html#data-conversion-to-celcius",
    "href": "notebooks/01a-get-started/get-started.html#data-conversion-to-celcius",
    "title": "Get started with open reproducible science!",
    "section": "Data Conversion to Celcius",
    "text": "Data Conversion to Celcius\nThe cell below converts the data to Celcius, using Python mathematical operators. Again, it’s not well documented and doesn’t follow PEP-8 guidelines. This has caused the author to miss an important error!\n Your task: 1. Replace the variable name dataframe with the name of your DataFrame. 2. Fix the error\n What is the mistake in the equation below? You might want to try writing out the formula for converting Fahrenheit to Celcius.\nYOUR ANSWER HERE\n\n#convert to celcius\ndataframe.iloc[:,1] = dataframe.iloc[:,1] - 32 * 5 / 9\ndataframe\n# BEGIN SOLUTION\nsd_tmax_df['temperature_c'] = (sd_tmax_df['temperature_f'] - 32) * 5/9\nprint([round(val, 2) for val in sd_tmax_df.mean().values])\nsd_tmax_df\n# END SOLUTION\n\n\n# DO NOT MODIFY THIS TEST CELL\ntmax_df_resp = _\npoints = 0\n\nif isinstance(tmax_df_resp, pd.DataFrame):\n    points += 1\n    print('\\u2705 Great work! You called a DataFrame.')\nelse:\n    print('\\u274C Oops - make sure to call your DataFrame for testing.')\n    \nsummary = [round(val, 2) for val in tmax_df_resp.mean().values]\nif summary == [1985.5, 59.04, 15.02]:\n    points += 4\n    print('\\u2705 Great work! You correctly converted to Celcius.')\nelse:\n    print('\\u274C Oops - your data are not correct.')\nprint('You earned {} of 5 points for converting to Celcius'.format(points))\n\n Want an EXTRA CHALLENGE? 1. As you did above, rewrite the code to be more expressive 2. Using the code below as a framework, write and apply a function that converts to Celcius. &gt; Functions let you reuse code you have already written\n\nYou should also rewrite this function name to be more expressive.\ndef convert(temperature):\n    \"\"\"Convert temperature to Celcius\"\"\"\n    return temperature # Put your equation in here\n\ndataframe['temp_c'] = dataframe['temp_f'].apply(convert)"
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html#plot-the-maximum-annual-temperature-in-rapid-city-sd-usa",
    "href": "notebooks/01a-get-started/get-started.html#plot-the-maximum-annual-temperature-in-rapid-city-sd-usa",
    "title": "Get started with open reproducible science!",
    "section": "Plot the maximum annual temperature in Rapid City, SD, USA",
    "text": "Plot the maximum annual temperature in Rapid City, SD, USA\nPlotting in Python is easy, but not quite this easy! You’ll always need to add some instructions on labels and how you want your plot to look.\n\nChange dataframe to your DataFrame name.\nChange 'col_1' and 'col_2' to your column names\nUse the title, ylabel, and xlabel parameters to add key text to your plot.\n\n\nHINT: labels have to be a type in Python called a string. You can make a string by putting quotes around your label, just like the column names in the sample code.\n\n\ndataframe.plot(x='col_1', y='col_2')\n\n# BEGIN SOLUTION\nsd_tmax_df.plot(\n    y='temperature_c',\n    x='year',\n    legend=False,\n    title='Maximum Annual Temperature in Rapid City, SD, USA',\n    ylabel='Temperature ($^\\circ$F)',\n    xlabel='Year',\n    figsize=(8, 6))\n# END SOLUTION\n\nTHIS ISN’T THE END! Don’t forget to complete the next task where you will describe your plot\n\n\nImage source: https://www.nps.gov/pais/learn/nature/hatchlingreleases.htm\n\n Want an EXTRA CHALLENGE?\nThere are many other things you can do to customize your plot. Take a look at the pandas plotting galleries and the documentation of plot to see if there’s other changes you want to make to your plot. Some possibilities include: * Remove the legend since there’s only one data series * Increase the figure size * Increase the font size * Change the colors * Use a bar graph instead (usually we use lines for time series, but since this is annual it could go either way) * Add a trend line"
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html#describe-your-plot-in-the-markdown-cell-below",
    "href": "notebooks/01a-get-started/get-started.html#describe-your-plot-in-the-markdown-cell-below",
    "title": "Get started with open reproducible science!",
    "section": "Describe your plot in the Markdown cell below",
    "text": "Describe your plot in the Markdown cell below\nWe like to use an approach called “Assertion-Evidence” for presenting scientific results. There’s a lot of video tutorials and example talks available on the Assertion-Evidence web page. The main thing you need to do now is to practice writing a message or headline rather than descriptions or topic sentences for the plot you just made (what they refer to as “visual evidence”).\nFor example, it would be tempting to write something like “A plot of maximum annual temperature in Rapid City, South Dakota over time (1947-2023)”. However, this doesn’t give the reader anything to look at, or explain why we made this particular plot (we know, you made this one because we told you to)\nSome alternatives that are more of a starting point for a presentation or conversation are: * Rapid City, SD, USA experienced extreme heat in 2013 * Extreme temperatures in Rapid City, SD appear to be on the rise over the past 70 years * Maximum annual temperatures in Rapid City, SD are becoming more variable over the previous 70 years\nWe could back up some of these claims with further analysis included later on, but we want to make sure that our audience has some guidance on what to look for in the plot.\n=== BEGIN MARK SCHEME ===\n(2 pts) Headline\n\n(3 pts) Description\n=== END MARK SCHEME ==="
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html#your-rapid-city-plot-headline-here",
    "href": "notebooks/01a-get-started/get-started.html#your-rapid-city-plot-headline-here",
    "title": "Get started with open reproducible science!",
    "section": "YOUR RAPID CITY PLOT HEADLINE HERE",
    "text": "YOUR RAPID CITY PLOT HEADLINE HERE\nDescribe your plot in this cell in 2-3 sentences\nTHIS ISN’T THE END EITHER! Don’t forget to reproduce your analysis in a new location!\n\n\nImage source: https://www.independent.co.uk/climate-change/news/by-the-left-quick-march-the-emperor-penguins-migration-1212420.html"
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html#your-turn-pick-a-new-location-andor-measurement-to-plot",
    "href": "notebooks/01a-get-started/get-started.html#your-turn-pick-a-new-location-andor-measurement-to-plot",
    "title": "Get started with open reproducible science!",
    "section": "Your turn: pick a new location and/or measurement to plot",
    "text": "Your turn: pick a new location and/or measurement to plot\nBelow, recreate the workflow you just did in a place that interests you OR with a different measurement. See the instructions above fore how to get your URL. You will need to make your own new Markdown and Code cells below this one."
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html#congratulations-you-finished-this-coding-challenge-now-make-sure-that-your-code-is-reproducible",
    "href": "notebooks/01a-get-started/get-started.html#congratulations-you-finished-this-coding-challenge-now-make-sure-that-your-code-is-reproducible",
    "title": "Get started with open reproducible science!",
    "section": "Congratulations, you finished this coding challenge – now make sure that your code is reproducible",
    "text": "Congratulations, you finished this coding challenge – now make sure that your code is reproducible\n\nIf you didn’t already, go back to the code you modified about and write more descriptive comments so the next person to use this code knows what it does.\nMake sure to Restart and Run all up at the top of your notebook. This will clear all your variables and make sure that your code runs in the correct order. It will also export your work in Markdown format, which you can put on your website.\n\n\n\nImage source: https://dfwurbanwildlife.com/2018/03/25/chris-jacksons-dfw-urban-wildlife/snow-geese-galore/\n\n\n!jupyter nbconvert --to markdown *.ipynb --TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}'"
  },
  {
    "objectID": "notebooks/01a-get-started/get-started.html#bibliography",
    "href": "notebooks/01a-get-started/get-started.html#bibliography",
    "title": "Get started with open reproducible science!",
    "section": "Bibliography",
    "text": "Bibliography\nEarth Science Data Systems, N. (2021, October 5). Open Source Science for the Earth System Observatory Mission Data Processing Study Workshops | Earthdata [Tutorial]. Earth Science Data Systems, NASA. https://www.earthdata.nasa.gov/technology/open-science/oss-for-eso-workshops\nU.S. Geological Survey. (n.d.). Landsat 8 [Remote-sensing image]. National Aeronautics and Space Administration, U.S. Government, U.S. Geological Survey. Retrieved April 4, 2024, from http://eros.usgs.gov"
  },
  {
    "objectID": "notebooks/01b-get-started-api/get-started-api.html",
    "href": "notebooks/01b-get-started-api/get-started-api.html",
    "title": "Climate change is impacting the way people live around the world",
    "section": "",
    "text": "Higher highs, lower lows, storms, and smoke – we’re all feeling the effects of climate change. In this workflow, you will take a look at trends in temperature over time in Rapid City, SD.\nWhat does open reproducible science mean to you?\n\n\n\n Create a new Markdown cell below this one using the + Markdown button in the upper left.\n In the new cell, answer the following questions using a numbered list in Markdown:\n\nIn 1-2 sentences, define open reproducible science.\nIn 1-2 sentences, choose one of the open source tools that you have learned about (i.e. Shell, Git/GitHub, Jupyter Notebook, Python) and explain how it supports open reproducible science."
  },
  {
    "objectID": "notebooks/01b-get-started-api/get-started-api.html#get-started-with-open-reproducible-science",
    "href": "notebooks/01b-get-started-api/get-started-api.html#get-started-with-open-reproducible-science",
    "title": "Climate change is impacting the way people live around the world",
    "section": "Get started with open reproducible science!",
    "text": "Get started with open reproducible science!\nOpen reproducible science makes scientific methods, data and outcomes available to everyone. That means that everyone who wants should be able to find, read, understand, and run your workflows for themselves.\n\n\nImage from https://www.earthdata.nasa.gov/esds/open-science/oss-for-eso-workshops\n\nFew if any science projects are 100% open and reproducible (yet!). However, members of the open science community have developed open source tools and practices that can help you move toward that goal. You will learn about many of those tools in the Intro to Earth Data Science textbook. Don’t worry about learning all the tools at once – we’ve picked a few for you to get started with."
  },
  {
    "objectID": "notebooks/01b-get-started-api/get-started-api.html#human-readable-and-machine-readable",
    "href": "notebooks/01b-get-started-api/get-started-api.html#human-readable-and-machine-readable",
    "title": "Climate change is impacting the way people live around the world",
    "section": " Human-readable and Machine-readable",
    "text": "Human-readable and Machine-readable\n Create a new Markdown cell below this one using the ESC + b keyboard shortcut.\n In the new cell, answer the following question in a Markdown quote: In 1-2 sentences, does this Jupyter Notebook file have a machine-readable name? Explain your answer."
  },
  {
    "objectID": "notebooks/01b-get-started-api/get-started-api.html#what-the-fork-who-wrote-this",
    "href": "notebooks/01b-get-started-api/get-started-api.html#what-the-fork-who-wrote-this",
    "title": "Climate change is impacting the way people live around the world",
    "section": "What the fork?! Who wrote this?",
    "text": "What the fork?! Who wrote this?\nBelow is a scientific Python workflow. But something’s wrong – The code won’t run! Your task is to follow the instructions below to clean and debug the Python code below so that it runs.\n\n\n\n\n\n\nTip\n\n\n\nDon’t worry if you can’t solve every bug right away. We’ll get there! The most important thing is to identify problems with the code and write high-quality GitHub Issues.\n\n\nAt the end, you’ll repeat the workflow for a location and measurement of your choosing.\nAlright! Let’s clean up this code. First things first…\n\n\n\n\n\n\n Machine-readable file names\n\n\n\nRename this notebook (if necessary) with an expressive and machine-readable file name"
  },
  {
    "objectID": "notebooks/01b-get-started-api/get-started-api.html#python-packages-let-you-use-code-written-by-experts-around-the-world",
    "href": "notebooks/01b-get-started-api/get-started-api.html#python-packages-let-you-use-code-written-by-experts-around-the-world",
    "title": "Climate change is impacting the way people live around the world",
    "section": "Python packages let you use code written by experts around the world",
    "text": "Python packages let you use code written by experts around the world\nBecause Python is open source, lots of different people and organizations can contribute (including you!). Many contributions are in the form of packages which do not come with a standard Python download.\n\n\n\n\n\n\n Read more\n\n\n\nPackages need to be installed and imported.\n\n\nIn the cell below, someone was trying to import the pandas package, which helps us to work with tabular data such as comma-separated value or csv files.\n\n\n\n\n\n\n\n Your task\n\n\n\n\nCorrect the typo below to properly import the pandas package under its alias pd.\nRun the cell to import pandas\n\n\n\n\n\n# Import pandas\nimport pandsa as pd\n\n\n\nSee our solution!\n# Import pandas\nimport pandas as pd\n\n\nOnce you have run the cell above and imported pandas, run the cell below. It is a test cell that will tell you if you completed the task successfully. If a test cell isn’t working the way you expect, check that you ran your code immediately before running the test.\n\n# DO NOT MODIFY THIS TEST CELL\npoints = 0\ntry:\n    pd.DataFrame()\n    points += 5\n    print('\\u2705 Great work! You correctly imported the pandas library.')\nexcept:\n    print('\\u274C Oops - pandas was not imported correctly.')\nprint('You earned {} of 5 points for importing pandas'.format(points))"
  },
  {
    "objectID": "notebooks/01b-get-started-api/get-started-api.html#there-are-more-earth-observation-data-online-than-any-one-person-could-ever-look-at",
    "href": "notebooks/01b-get-started-api/get-started-api.html#there-are-more-earth-observation-data-online-than-any-one-person-could-ever-look-at",
    "title": "Climate change is impacting the way people live around the world",
    "section": "There are more Earth Observation data online than any one person could ever look at",
    "text": "There are more Earth Observation data online than any one person could ever look at\nNASA’s Earth Observing System Data and Information System (EOSDIS) alone manages over 9PB of data. 1 PB is roughly 100 times the entire Library of Congress (a good approximation of all the books available in the US). It’s all available to you once you learn how to download what you want.\nHere we’re using the NOAA National Centers for Environmental Information (NCEI) Access Data Service application progamming interface (API) to request data from their web servers. We will be using data collected as part of the Global Historical Climatology Network daily (GHCNd) from their Climate Data Online library program at NOAA.\nFor this example we’re requesting daily summary data in Rapid City, CO (station ID USC00396947).\n\n\n\n\n\n\n\n Your task:\n\n\n\n\nResearch the Global Historical Climatology Network - Daily data source.\nIn the cell below, write a 2-3 sentence description of the data source. You should describe:\n\nwho takes the data\nwhere the data were taken\nwhat the maximum temperature units are\nhow the data are collected\n\nInclude a citation of the data (HINT: See the ‘Data Citation’ tab on the GHCNd overview page).\n\n\n\n\n\nYOUR DATA DESCRIPTION AND CITATION HERE 🛎️"
  },
  {
    "objectID": "notebooks/01b-get-started-api/get-started-api.html#you-can-access-ncei-ghcnd-data-from-the-internet-using-its-api",
    "href": "notebooks/01b-get-started-api/get-started-api.html#you-can-access-ncei-ghcnd-data-from-the-internet-using-its-api",
    "title": "Climate change is impacting the way people live around the world",
    "section": "You can access NCEI GHCNd Data from the internet using its API 🖥️ 📡 🖥️",
    "text": "You can access NCEI GHCNd Data from the internet using its API 🖥️ 📡 🖥️\nThe cell below contains the URL for the data you will use in this part of the notebook. We created this URL by generating what is called an API endpoint using the NCEI API documentation.\n\n\n\n\n\n\nNote\n\n\n\nAn application programming interface (API) is a way for two or more computer programs or components to communicate with each other. It is a type of software interface, offering a service to other pieces of software (Wikipedia).\n\n\nHowever, we still have a problem - we can’t get the URL back later on because it isn’t saved in a variable. In other words, we need to give the url a name so that we can request in from Python later (sadly, Python has no ‘hey what was that thingy I typed yesterday?’ function).\n\n\n\n\n\n\n Read more\n\n\n\nCheck out the textbook section on variables\n\n\n\n\n\n\n\n\n Your task\n\n\n\n\nPick an expressive variable name for the URL. HINT: click on the Variables button up top to see all your variables. Your new url variable will not be there until you define it and run the code\nReformat the URL so that it adheres to the 79-character PEP-8 line limit.You should see two vertical lines in each cell - don’t let your code go past the second line\nAt the end of the cell where you define your url variable, call your variable (type out its name) so it can be tested.\n\n\n\n\nstuff23 = ('https://www.ncei.noaa.gov/access/services/da'\n'ta/v1?dataset=daily-summaries&dataTypes=TOBS,PRCP&stations=USC00396947&startDate=1949-10-01&endDate=2024-02-18&includeStationName=true&includeStation'\n'Location=1&units=standard')\nstuff23\n\n\n\nSee our solution!\nrapid_url = (\n    'https://www.ncei.noaa.gov/access/services/data/v1'\n    '?dataset=daily-summaries'\n    '&dataTypes=TOBS,PRCP'\n    '&stations=USC00396947'\n    '&startDate=1949-10-01'\n    '&endDate=2024-02-18'\n    '&includeStationName=true'\n    '&includeStationLocation=1'\n    '&units=standard')\nrapid_url\n\n\n\n# DO NOT MODIFY THIS TEST CELL\nresp_url = _\npoints = 0\n\nif type(resp_url)==str:\n    points += 3\n    print('\\u2705 Great work! You correctly called your url variable.')\nelse:\n    print('\\u274C Oops - your url variable was not called correctly.')\n\nif len(resp_url)==218:\n    points += 3\n    print('\\u2705 Great work! Your url is the correct length.')\nelse:\n    print('\\u274C Oops - your url variable is not the correct length.')\n\nprint('You earned {} of 6 points for defining a url variable'.format(points))"
  },
  {
    "objectID": "notebooks/01b-get-started-api/get-started-api.html#download-and-get-started-working-with-ncei-data",
    "href": "notebooks/01b-get-started-api/get-started-api.html#download-and-get-started-working-with-ncei-data",
    "title": "Climate change is impacting the way people live around the world",
    "section": "Download and get started working with NCEI data",
    "text": "Download and get started working with NCEI data\nThe pandas library you imported can download data from the internet directly into a type of Python object called a DataFrame. In the code cell below, you can see an attempt to do just this. But there are some problems…\n\n\n\n\n\n\n You’re ready to fix some code!\n\n\n\nYour task is to:\n\nLeave a space between the # and text in the comment and try making the comment more informative\nMake any changes needed to get this code to run. HINT: The my_url variable doesn’t exist - you need to replace it with the variable name you chose.\nModify the .read_csv() statement to include the following parameters:\n\nindex_col='DATE' – this sets the DATE column as the index. Needed for subsetting and resampling later on\nparse_dates=True – this lets python know that you are working with time-series data, and values in the indexed column are date time objects\nna_values=['NaN'] – this lets python know how to handle missing values\n\nClean up the code by using expressive variable names, expressive column names, PEP-8 compliant code, and descriptive comments\n\n\n\n\nMake sure to call your DataFrame by typing it’s name as the last line of your code cell Then, you will be able to run the test cell below and find out if your answer is correct.\n\n\nrapid_df = pd.read_csv(\n  rapid_url,\n  index_col='something')\nrapid_df\n\n\n\nSee our solution!\n# Download the Rapid City climate data\nrapid_df = pd.read_csv(\n  rapid_url,\n  index_col='DATE',\n  parse_dates=True,\n  na_values=['NaN'])\nrapid_df\n\n\n\n# DO NOT MODIFY THIS TEST CELL\ntmax_df_resp = _\npoints = 0\n\nif isinstance(tmax_df_resp, pd.DataFrame):\n    points += 1\n    print('\\u2705 Great work! You called a DataFrame.')\nelse:\n    print('\\u274C Oops - make sure to call your DataFrame for testing.')\n\nprint('You earned {} of 2 points for downloading data'.format(points))\n\n\nHINT: Check out the type() function below - you can use it to check that your data is now in DataFrame type object\n\n\n# Check that the data was imported into a pandas DataFrame\ntype(rapid_df)\n\n\n\n\n\n\n\n Clean up your DataFrame\n\n\n\nUse double brackets to only select the columns you want in your DataFrame\n\n\n\nMake sure to call your DataFrame by typing it’s name as the last line of your code cell Then, you will be able to run the test cell below and find out if your answer is correct.\n\n\nrapid_df = rapid_df[['some_col', 'another_col']]\nrapid_df\n\n\n\nSee our solution!\n# Clean up the DataFrame\nrapid_df = rapid_df[['PRCP', 'TOBS']]\nrapid_df\n\n\n\n# DO NOT MODIFY THIS TEST CELL\ntmax_df_resp = _\npoints = 0\n\nsummary = [round(val, 2) for val in tmax_df_resp.mean().values]\nif summary == [0.05, 54.53]:\n    points += 4\n    print('\\u2705 Great work! You correctly downloaded data.')\nelse:\n    print('\\u274C Oops - your data are not correct.')\nprint('You earned {} of 5 points for downloading data'.format(points))"
  },
  {
    "objectID": "notebooks/01b-get-started-api/get-started-api.html#plot-the-precpitation-column-prcp-vs-time-to-explore-the-data",
    "href": "notebooks/01b-get-started-api/get-started-api.html#plot-the-precpitation-column-prcp-vs-time-to-explore-the-data",
    "title": "Climate change is impacting the way people live around the world",
    "section": "Plot the precpitation column (PRCP) vs time to explore the data",
    "text": "Plot the precpitation column (PRCP) vs time to explore the data\nPlotting in Python is easy, but not quite this easy:\n\nrapid_df.plot()\n\n\n\n\n\n\n\nLabel and describe your plots\n\n\n\n\n\n\nSource: https://xkcd.com/833\n\n\nMake sure each plot has:\n\nA title that explains where and when the data are from\nx- and y- axis labels with units where appropriate\nA legend where appropriate\n\n\n\nYou’ll always need to add some instructions on labels and how you want your plot to look.\n\n\n\n\n\n\n Your task:\n\n\n\n\nChange dataframe to your DataFrame name.\nChange y= to the name of your observed temperature column name.\nUse the title, ylabel, and xlabel parameters to add key text to your plot.\nAdjust the size of your figure using figsize=(x,y) where x is figure width and y is figure height\n\n\nHINT: labels have to be a type in Python called a string. You can make a string by putting quotes around your label, just like the column names in the sample code (eg y='TOBS').\n\n\n\n\n# Plot the data using .plot\nrapid_df.plot(\n    y='the_precipitation_column',\n    title='Title Goes Here',\n    xlabel='Horizontal Axis Label Goes Here',\n    ylabel='Vertical Axis Label Goes Here')\n\n\n\nSee our solution!\n# Plot the data using .plot\nrapid_df.plot(\n    y='PRCP',\n    title='Daily Precipitation in Rapid City, SD',\n    xlabel='Date',\n    ylabel='Precipitation (mm)')\n\n\n\n\n\n\n\n\n Want an EXTRA CHALLENGE?\n\n\n\nThere are many other things you can do to customize your plot. Take a look at the pandas plotting galleries and the documentation of plot to see if there’s other changes you want to make to your plot. Some possibilities include:\n\nRemove the legend since there’s only one data series\nIncrease the figure size\nIncrease the font size\nChange the colors\nUse a bar graph instead (usually we use lines for time series, but since this is annual it could go either way)\nAdd a trend line\n\nNot sure how to do any of these? Try searching the internet, or asking an AI!\n\n\n\n\n\n\n\n\n\n Convert units\n\n\n\nModify the code below to add a column that includes temperature in Celsius. The code below was written by your colleague. Can you fix this so that it correctly calculates temperature in Celsius and adds a new column?\n\n\n\n# Convert to celcius\ndataframe.loc[:, 'TCel'] = dataframe['temperature_col_name'] - 32 * 5 / 9\ndataframe\n\n\n\nSee our solution!\n# Convert to celcius\nrapid_df.loc[:, 'TCel'] = (rapid_df['TOBS'] - 32) * 5 / 9\nrapid_df\n\n\n\n# DO NOT MODIFY THIS TEST CELL\ntmax_df_resp = _\npoints = 0\n\nif isinstance(tmax_df_resp, pd.DataFrame):\n    points += 1\n    print('\\u2705 Great work! You called a DataFrame.')\nelse:\n    print('\\u274C Oops - make sure to call your DataFrame for testing.')\n\nsummary = [round(val, 2) for val in tmax_df_resp.mean().values]\nif summary == [0.05, 54.53, 12.52]:\n    points += 4\n    print('\\u2705 Great work! You correctly converted to Celcius.')\nelse:\n    print('\\u274C Oops - your data are not correct.')\nprint('You earned {} of 5 points for converting to Celcius'.format(points))\n\n\n\n\n\n\n\n Want an EXTRA CHALLENGE?\n\n\n\n\nAs you did above, rewrite the code to be more expressive\nUsing the code below as a framework, write and apply a function that converts to Celcius. &gt; Functions let you reuse code you have already written\nYou should also rewrite this function and parameter names to be more expressive.\n\n\n\n\ndef a_function(a_parameter):\n    \"\"\"Convert temperature to Celcius\"\"\"\n    return a_parameter # Put your equation in here\n\ndataframe['celcius_column'] = dataframe['fahrenheit_column'].apply(convert)\n\n\n\nSee our solution!\ndef convert_to_celcius(fahrenheit):\n    \"\"\"Convert temperature to Celcius\"\"\"\n    return (fahrenheit - 32) * 5 / 9\n\nrapid_df['TCel'] = rapid_df['TOBS'].apply(convert_to_celcius)"
  },
  {
    "objectID": "notebooks/01b-get-started-api/get-started-api.html#subsetting-and-resampling",
    "href": "notebooks/01b-get-started-api/get-started-api.html#subsetting-and-resampling",
    "title": "Climate change is impacting the way people live around the world",
    "section": "Subsetting and Resampling",
    "text": "Subsetting and Resampling\nOften when working with time-series data you may want to focus on a shorter window of time, or look at weekly, monthly, or annual summaries to help make the analysis more manageable.\n\n\n\n\n\n\n Read more\n\n\n\nRead more about subsetting and resampling time-series data in our Learning Portal.\n\n\nFor this demonstration, we will look at the last 40 years worth of data and resample to explore a summary from each year that data were recorded.\n\n\n\n\n\n\n Your task\n\n\n\n\nReplace start-year and end-year with 1983 and 2023\nReplace dataframe with the name of your data\nReplace new_dataframe with something more expressive\nCall your new variable\nRun the cell\n\n\n\n\n# Subset the data\nnew_dataframe = dataframe.loc['start-year':'end-year']\nnew_dataframe\n\n\n\nSee our solution!\n# Subset the data\nrapid_1983_2023_df = rapid_df.loc['1983':'2023']\nrapid_1983_2023_df\n\n\n\n# DO NOT MODIFY THIS TEST CELL\ndf_resp = _\npoints = 0\n\nif isinstance(df_resp, pd.DataFrame):\n    points += 1\n    print('\\u2705 Great work! You called a DataFrame.')\nelse:\n    print('\\u274C Oops - make sure to call your DataFrame for testing.')\n\nsummary = [round(val, 2) for val in df_resp.mean().values]\nif summary == [0.06, 55.67, 13.15]:\n    points += 5\n    print('\\u2705 Great work! You correctly converted to Celcius.')\nelse:\n    print('\\u274C Oops - your data are not correct.')\nprint('You earned {} of 5 points for subsetting'.format(points))"
  },
  {
    "objectID": "notebooks/01b-get-started-api/get-started-api.html#now-we-are-ready-to-calculate-annual-statistics",
    "href": "notebooks/01b-get-started-api/get-started-api.html#now-we-are-ready-to-calculate-annual-statistics",
    "title": "Climate change is impacting the way people live around the world",
    "section": "Now we are ready to calculate annual statistics",
    "text": "Now we are ready to calculate annual statistics\nHere you will resample the 1983-2023 data to look the annual mean values.\n\n\n\n\n\n\n Resample your data\n\n\n\n\nReplace new_dataframe with the variable you created in the cell above where you subset the data\nReplace 'TIME' with a 'W', 'M', or 'Y' depending on whether you’re doing a weekly, monthly, or yearly summary\nReplace STAT with a sum, min, max, or mean depending on what kind of statistic you’re interested in calculating.\nReplace resampled_data with a more expressive variable name\nCall your new variable\nRun the cell\n\n\n\n\n# Resample the data to look at yearly mean values\nresampled_data = new_dataframe.resample('TIME').STAT()\nresampled_data\n\n\n\nSee our solution!\n# Resample the data to look at yearly mean values\nrapid_ann_mean_df = rapid_1983_2023_df.resample('YS').mean()\nrapid_ann_mean_df\n\n\n\n# DO NOT MODIFY THIS TEST CELL\ndf_resp = _\npoints = 0\n\nif isinstance(df_resp, pd.DataFrame):\n    points += 1\n    print('\\u2705 Great work! You called a DataFrame.')\nelse:\n    print('\\u274C Oops - make sure to call your DataFrame for testing.')\n\nsummary = [round(val, 2) for val in df_resp.mean().values]\nif summary == [0.06, 55.37, 12.99]:\n    points += 5\n    print('\\u2705 Great work! You correctly converted to Celcius.')\nelse:\n    print('\\u274C Oops - your data are not correct.')\nprint('You earned {} of 5 points for resampling'.format(points))\n\n\n\n\n\n\n\n Plot your resampled data\n\n\n\n\n\n\n\n# Plot mean annual temperature values\n\n\n\nSee our solution!\n# Plot the data using .plot\nrapid_ann_mean_df.plot(\n    y='TOBS',\n    title='Daily Precipitation in Rapid City, SD',\n    xlabel='Date',\n    ylabel='Precipitation (mm)')\n\n\n\n\n\n\n\n\n\n Describe your plot\n\n\n\nWe like to use an approach called “Assertion-Evidence” for presenting scientific results. There’s a lot of video tutorials and example talks available on the Assertion-Evidence web page. The main thing you need to do now is to practice writing a message or headline rather than descriptions or topic sentences for the plot you just made (what they refer to as “visual evidence”).\nFor example, it would be tempting to write something like “A plot of maximum annual temperature in Rapid City, Colorado over time (1983-2023)”. However, this doesn’t give the reader anything to look at, or explain why we made this particular plot (we know, you made this one because we told you to)\nSome alternatives for different plots of Rapid City temperature that are more of a starting point for a presentation or conversation are:\n\nRapid City, SD experienced cooler than average temperatures in 1995\nTemperatures in Rapid City, SD appear to be on the rise over the past 40 years\nMaximum annual temperatures in Rapid City, CO are becoming more variable over the previous 40 years\n\nWe could back up some of these claims with further analysis included later on, but we want to make sure that our audience has some guidance on what to look for in the plot."
  },
  {
    "objectID": "notebooks/01b-get-started-api/get-started-api.html#your-rapid-city-plot-headline-here",
    "href": "notebooks/01b-get-started-api/get-started-api.html#your-rapid-city-plot-headline-here",
    "title": "Climate change is impacting the way people live around the world",
    "section": "YOUR Rapid City PLOT HEADLINE HERE 📰 🗞️ 📻",
    "text": "YOUR Rapid City PLOT HEADLINE HERE 📰 🗞️ 📻\nDescribe your plot in this cell in 2-3 sentences\n\n\n\nWriting bear\n\n\n\nImage credit: https://www.craiyon.com/image/OAbZtyelSoS7FdGko6hvQg"
  },
  {
    "objectID": "notebooks/01b-get-started-api/get-started-api.html#your-turn-pick-a-new-location-andor-measurement-to-plot",
    "href": "notebooks/01b-get-started-api/get-started-api.html#your-turn-pick-a-new-location-andor-measurement-to-plot",
    "title": "Climate change is impacting the way people live around the world",
    "section": "Your turn: pick a new location and/or measurement to plot 🌏 📈",
    "text": "Your turn: pick a new location and/or measurement to plot 🌏 📈\nBelow (or in a new notebook!), recreate the workflow you just did in a place that interests you OR with a different measurement. See the instructions above to adapt the URL that we created for Rapid City, CO using the NCEI API. You will need to make your own new Markdown and Code cells below this one, or create a new notebook."
  },
  {
    "objectID": "notebooks/01b-get-started-api/get-started-api.html#congratulations-youre-almost-done-with-this-coding-challenge-now-make-sure-that-your-code-is-reproducible",
    "href": "notebooks/01b-get-started-api/get-started-api.html#congratulations-youre-almost-done-with-this-coding-challenge-now-make-sure-that-your-code-is-reproducible",
    "title": "Climate change is impacting the way people live around the world",
    "section": "Congratulations, you’re almost done with this coding challenge 🤩 – now make sure that your code is reproducible",
    "text": "Congratulations, you’re almost done with this coding challenge 🤩 – now make sure that your code is reproducible\n\n\nImage source: https://dfwurbanwildlife.com/2018/03/25/chris-jacksons-dfw-urban-wildlife/snow-geese-galore/\n\n\n\n\n\n\n\n Your task\n\n\n\n\nIf you didn’t already, go back to the code you modified about and write more descriptive comments so the next person to use this code knows what it does."
  },
  {
    "objectID": "notebooks/01b-get-started-api/get-started-api.html#bonus-create-a-shareable-markdown-of-your-work",
    "href": "notebooks/01b-get-started-api/get-started-api.html#bonus-create-a-shareable-markdown-of-your-work",
    "title": "Climate change is impacting the way people live around the world",
    "section": "BONUS: Create a shareable Markdown of your work",
    "text": "BONUS: Create a shareable Markdown of your work\nBelow is some code that you can run that will save a Markdown file of your work that is easily shareable and can be uploaded to GitHub Pages. You can use it as a starting point for writing your portfolio post!\n\n%%capture\n%%bash\njupyter nbconvert *.ipynb --to markdown"
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution.html",
    "href": "notebooks/03-species-distribution/species-distribution.html",
    "title": "Mapping Tasiyagnunpa (Western Meadowlark) migration",
    "section": "",
    "text": "Tasiyagnunpa (or Western Meadowlark, or sturnella neglecta) migrates each year to next on the Great Plains in the United States. Using crowd-sourced observations of these birds, we can see that migration happening throughout the year."
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution.html#set-up-your-reproducible-workflow",
    "href": "notebooks/03-species-distribution/species-distribution.html#set-up-your-reproducible-workflow",
    "title": "Mapping Tasiyagnunpa (Western Meadowlark) migration",
    "section": "Set up your reproducible workflow",
    "text": "Set up your reproducible workflow\n\nImport Python libraries\nWe will be getting data from a source called GBIF (Global Biodiversity Information Facility). We need a package called pygbif to access the data, which is not included in your environment. Install it by running the cell below:\n\n%%bash\npip install pygbif\n\n\n\n\n\n\n\n Your Task: Import packages\n\n\n\nAdd imports for packages that will help you:\n\nWork with tabular data\nWork with geospatial vector data\nMake an interactive plot of tabular and/or vector data\n\n\n\n\nimport calendar\nimport os\nimport pathlib\nimport requests\nimport time\nimport zipfile\nfrom getpass import getpass\n\nimport cartopy.crs as ccrs\nimport panel as pn\nimport pygbif.occurrences as occ\n\n\n\nSee our solution!\nimport calendar\nimport os\nimport pathlib\nimport requests\nimport time\nimport zipfile\nfrom getpass import getpass\nfrom glob import glob\n\nimport cartopy.crs as ccrs\nimport geopandas as gpd\nimport hvplot.pandas\nimport pandas as pd\nimport panel as pn\nimport pygbif.occurrences as occ\nimport pygbif.species as species\n\n\n\n\nCreate a folder for your data\nFor this challenge, you will need to save some data to your computer. We suggest saving to somewhere in your home folder (e.g. /home/username), rather than to your GitHub repository, since data files can easily become too large for GitHub.\n\n\n\n\n\n\nWarning\n\n\n\nThe home directory is different for every user! Your home directory probably won’t exist on someone else’s computer. Make sure to use code like pathlib.Path.home() to compute the home directory on the computer the code is running on. This is key to writing reproducible and interoperable code.\n\n\n\n\n\n\n\n\n Your Task: Create a project folder\n\n\n\nThe code below will help you get started with making a project directory\n\nReplace 'your-project-directory-name-here' and 'your-gbif-data-directory-name-here' with descriptive names\nRun the cell\n(OPTIONAL) Check in the terminal that you created the directory using the command ls ~/earth-analytics/data\n\n\n\n\n# Create data directory in the home folder\ndata_dir = os.path.join(\n    # Home directory\n    pathlib.Path.home(),\n    # Earth analytics data directory\n    'earth-analytics',\n    'data',\n    # Project directory\n    'your-project-directory-name-here',\n)\nos.makedirs(data_dir, exist_ok=True)\n\n# Define the directory name for GBIF data\ngbif_dir = os.path.join(data_dir, 'your-gbif-data-directory-name-here')\n\n\n\nSee our solution!\n# Create data directory in the home folder\ndata_dir = os.path.join(\n    pathlib.Path.home(),\n    'earth-analytics',\n    'data',\n    'species-distribution',\n)\nos.makedirs(data_dir, exist_ok=True)\n\n# Define the directory name for GBIF data\ngbif_dir = os.path.join(data_dir, 'meadowlark_observations')"
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution.html#define-your-study-area-the-ecoregions-of-north-america",
    "href": "notebooks/03-species-distribution/species-distribution.html#define-your-study-area-the-ecoregions-of-north-america",
    "title": "Mapping Tasiyagnunpa (Western Meadowlark) migration",
    "section": "Define your study area – the ecoregions of North America",
    "text": "Define your study area – the ecoregions of North America\nTrack observations of Taciyagnunpa across the different ecoregions of North America! You should be able to see changes in the number of observations in each ecoregion throughout the year.\n\nDownload and save ecoregion boundaries\n\n\n\n\n\n\n Your Task\n\n\n\n\nFind the URL for for the level III ecoregion boundaries. You can get ecoregion boundaries from the Environmental Protection Agency (EPA)..\nReplace your/url/here with the URL you found, making sure to format it so it is easily readable.\nChange all the variable names to descriptive variable names\nRun the cell to download and save the data.\n\n\n\n\n# Set up the ecoregions level III boundary URL\na_url = (\"your/url/here\")\n# Set up a path to save the dataon your machine\na_path = os.path.join(data_dir, 'filename.zip')\n\n# Don't download twice\nif not os.path.exists(a_path):\n    # Download, and don't check the certificate for the EPA\n    a_response = requests.get(a_url, verify=False)\n    # Save the binary data to a file\n    with open(a_path, 'wb') as a_file:\n        a_file.write(a_response.content)\n\n\n\nSee our solution!\n# Set up the ecoregions level III boundary URL\necoregions_url = (\n    \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/cec_na\"\n    \"/NA_CEC_Eco_Level3.zip\")\n# Set up a path to save the dataon your machine\necoregions_path = os.path.join(data_dir, 'NA_CEC_Eco_Level3.zip')\n\n# Don't download twice\nif not os.path.exists(ecoregions_path):\n    # Download\n    ecoregions_response = requests.get(ecoregions_url, verify=False)\n    # Save the data to your file\n    with open(ecoregions_path, 'wb') as ecoregions_file:\n        ecoregions_file.write(ecoregions_response.content)\n\n\n\n\nLoad the ecoregions into Python\n\n\n\n\n\n\n Your task\n\n\n\nDownload and save ecoregion boundaries from the EPA:\n\nReplace a_path with the path your created for your ecoregions file.\n(optional) Consider renaming and selecting columns to make your GeoDataFrame easier to work with.\nMake a quick plot with .plot() to make sure the download worked.\nRun the cell to load the data into Python\n\n\n\n\n# Open up the ecoregions boundaries\ngdf = gpd.read_file(a_path)\n\n# Name the index so it will match the other data later on\ngdf.index.name = 'ecoregion'\n\n# Plot the ecoregions to check download\n\n\n\nSee our solution!\n# Open up the ecoregions boundaries\necoregions_gdf = (\n    gpd.read_file(ecoregions_path)\n    .rename(columns={\n        'NA_L3NAME': 'name',\n        'Shape_Area': 'area'})\n    [['name', 'area', 'geometry']]\n)\n\n# We'll name the index so it will match the other data\necoregions_gdf.index.name = 'ecoregion'\n\n# Plot the ecoregions to check download\necoregions_gdf.plot(edgecolor='black', color='skyblue')\n\n\n\n\nCreate a simplified GeoDataFrame for plotting\nPlotting larger files can be time consuming. The code below will streamline plotting with hvplot by simplifying the geometry, projecting it to a Mercator projection that is compatible with geoviews, and cropping off areas in the Arctic.\n\n\n\n\n\n\n Your task\n\n\n\nDownload and save ecoregion boundaries from the EPA:\n\nMake a copy of your ecoregions GeoDataFrame with the .copy() method, and save it to another variable name. Make sure to do everything else in this cell with your new copy!\nSimplify the ecoregions with .simplify(1000), and save it back to the geometry column.\nChange the Coordinate Reference System (CRS) to Mercator with .to_crs(ccrs.Mercator())\nUse the plotting code in the cell to check that the plotting runs quickly and looks the way you want, making sure to change gdf to YOUR GeoDataFrame name.\n\n\n\n\n# Make a copy of the ecoregions\n\n# Simplify the geometry to speed up processing\n\n# Change the CRS to Mercator for mapping\n\n# Check that the plot runs\ngdf.hvplot(geo=True, crs=ccrs.Mercator())\n\n\n\nSee our solution!\n# Make a copy of the ecoregions\necoregions_plot_gdf = ecoregions_gdf.copy()\n\n# Simplify the geometry to speed up processing\necoregions_plot_gdf.geometry = ecoregions_plot_gdf.simplify(1000)\n\n# Change the CRS to Mercator for mapping\necoregions_plot_gdf = ecoregions_plot_gdf.to_crs(ccrs.Mercator())\n\n# Check that the plot runs\necoregions_plot_gdf.hvplot(geo=True, crs=ccrs.Mercator())"
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution.html#access-locations-and-times-of-tasiyagnunpa-encounters",
    "href": "notebooks/03-species-distribution/species-distribution.html#access-locations-and-times-of-tasiyagnunpa-encounters",
    "title": "Mapping Tasiyagnunpa (Western Meadowlark) migration",
    "section": "Access locations and times of Tasiyagnunpa encounters",
    "text": "Access locations and times of Tasiyagnunpa encounters\nFor this challenge, you will use a database called the Global Biodiversity Information Facility (GBIF). GBIF is compiled from species observation data all over the world, and includes everything from museum specimens to photos taken by citizen scientists in their backyards.\n\n\n\n\n\n\n Your task: Explore GBIF\n\n\n\nBefore your get started, go to the GBIF occurrences search page and explore the data.\n\n\n\n\n\n\n\n\nContribute to open data\n\n\n\nYou can get your own observations added to GBIF using iNaturalist!\n\n\n\nRegister and log in to GBIF\nYou will need a GBIF account to complete this challenge. You can use your GitHub account to authenticate with GBIF. Then, run the following code to save your credentials on your computer.\n\n\n\n\n\n\nTip\n\n\n\nIf you accidentally enter your credentials wrong, you can set reset_credentials=True instead of reset_credentials=False\n\n\n\nreset_credentials = False\n# GBIF needs a username, password, and email\ncredentials = dict(\n    GBIF_USER=(input, 'GBIF username:'),\n    GBIF_PWD=(getpass, 'GBIF password'),\n    GBIF_EMAIL=(input, 'GBIF email'),\n)\nfor env_variable, (prompt_func, prompt_text) in credentials.items():\n    # Delete credential from environment if requested\n    if reset_credentials and (env_variable in os.environ):\n        os.environ.pop(env_variable)\n    # Ask for credential and save to environment\n    if not env_variable in os.environ:\n        os.environ[env_variable] = prompt_func(prompt_text)\n\n\n\nGet the species key\n\n\n\n\n\n\n Your task\n\n\n\n\nReplace the species_name with the name of the species you want to look up\nRun the code to get the species key\n\n\n\n\n# Query species\nspecies_info = species.name_lookup(species_name, rank='SPECIES')\n\n# Get the first result\nfirst_result = species_info['results'][0]\n\n# Get the species key (nubKey)\nspecies_key = first_result['nubKey']\n\n# Check the result\nfirst_result['species'], species_key\n\n\n\nSee our solution!\n# Query species\nspecies_info = species.name_lookup('sturnella neglecta', rank='SPECIES')\n\n# Get the first result\nfirst_result = species_info['results'][0]\n\n# Get the species key (nubKey)\nspecies_key = first_result['nubKey']\n\n# Check the result\nfirst_result['species'], species_key\n\n\n\n\nDownload data from GBIF\n\n\n\n\n\n\n Your task\n\n\n\n\nReplace csv_file_pattern with a string that will match any .csv file when used in the glob function. HINT: the character * represents any number of any values except the file separator (e.g. /)\nAdd parameters to the GBIF download function, occ.download() to limit your query to:\n\nSturnella Neglecta observations\nin north america (NORTH_AMERICA)\nfrom 2023\nwith spatial coordinates.\n\nThen, run the download. This can take a few minutes.\n\n\n\n\n# Only download once\ngbif_pattern = os.path.join(gbif_dir, csv_file_pattern)\nif not glob(gbif_pattern):\n    # Submit query to GBIF\n    gbif_query = occ.download([\n        \"continent = \",\n        \"speciesKey = \",\n        \"year = \",\n        \"hasCoordinate = \",\n    ])\n    if not 'GBIF_DOWNLOAD_KEY' in os.environ:\n        os.environ['GBIF_DOWNLOAD_KEY'] = gbif_query[0]\n\n        # Wait for the download to build\n        wait = occ.download_meta(download_key)['status']\n        while not wait=='SUCCEEDED':\n            wait = occ.download_meta(download_key)['status']\n            time.sleep(5)\n\n    # Download GBIF data\n    download_info = occ.download_get(\n        os.environ['GBIF_DOWNLOAD_KEY'], \n        path=data_dir)\n\n    # Unzip GBIF data\n    with zipfile.ZipFile(download_info['path']) as download_zip:\n        download_zip.extractall(path=gbif_dir)\n\n# Find the extracted .csv file path\ngbif_path = glob(gbif_pattern)[0]\n\n\n\nSee our solution!\n# Only download once\ngbif_pattern = os.path.join(gbif_dir, '*.csv')\nif not glob(gbif_pattern):\n    # Submit query to GBIF\n    gbif_query = occ.download([\n        \"continent = NORTH_AMERICA\",\n        \"speciesKey = 9596413\",\n        \"hasCoordinate = TRUE\",\n        \"year = 2023\",\n    ])\n    download_key = gbif_query[0]\n\n    # Wait for the download to build\n    if not 'GBIF_DOWNLOAD_KEY' in os.environ:\n        os.environ['GBIF_DOWNLOAD_KEY'] = gbif_query[0]\n\n        # Wait for the download to build\n        wait = occ.download_meta(download_key)['status']\n        while not wait=='SUCCEEDED':\n            wait = occ.download_meta(download_key)['status']\n            time.sleep(5)\n\n    # Download GBIF data\n    download_info = occ.download_get(\n        os.environ['GBIF_DOWNLOAD_KEY'], \n        path=data_dir)\n\n    # Unzip GBIF data\n    with zipfile.ZipFile(download_info['path']) as download_zip:\n        download_zip.extractall(path=gbif_dir)\n\n# Find the extracted .csv file path (take the first result)\ngbif_path = glob(gbif_pattern)[0]\n\n\n\n\nLoad the GBIF data into Python\n\n\n\n\n\n\n Your task\n\n\n\n\nLook at the beginning of the file you downloaded using the code below. What do you think the delimiter is?\nRun the following code cell. What happens?\nUncomment and modify the parameters of pd.read_csv() below until your data loads successfully and you have only the columns you want.\n\n\n\nYou can use the following code to look at the beginning of your file:\n\n!head $gbif_path\n\n\n# Load the GBIF data\ngbif_df = pd.read_csv(\n    gbif_path, \n    #delimiter='',\n    #index_col='',\n    #usecols=[]\n)\ngbif_df.head()\n\n\n\nSee our solution!\n# Load the GBIF data\ngbif_df = pd.read_csv(\n    gbif_path, \n    delimiter='\\t',\n    index_col='gbifID',\n    usecols=['gbifID', 'decimalLatitude', 'decimalLongitude', 'month'])\ngbif_df.head()\n\n\n\n\nConvert the GBIF data to a GeoDataFrame\nTo plot the GBIF data, we need to convert it to a GeoDataFrame first.\n\n\n\n\n\n\n Your task\n\n\n\n\nReplace your_dataframe with the name of the DataFrame you just got from GBIF\nReplace longitude_column_name and latitude_column_name with column names from your `DataFrame\nRun the code to get a GeoDataFrame of the GBIF data.\n\n\n\n\ngbif_gdf = (\n    gpd.GeoDataFrame(\n        your_dataframe, \n        geometry=gpd.points_from_xy(\n            your_dataframe.longitude_column_name, \n            your_dataframe.latitude_column_name), \n        crs=\"EPSG:4326\")\n    # Select the desired columns\n    [[]]\n)\ngbif_gdf\n\n\n\nSee our solution!\ngbif_gdf = (\n    gpd.GeoDataFrame(\n        gbif_df, \n        geometry=gpd.points_from_xy(\n            gbif_df.decimalLongitude, \n            gbif_df.decimalLatitude), \n        crs=\"EPSG:4326\")\n    # Select the desired columns\n    [['month', 'geometry']]\n)\ngbif_gdf"
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution.html#count-the-number-of-observations-in-each-ecosystem-during-each-month-of-2023",
    "href": "notebooks/03-species-distribution/species-distribution.html#count-the-number-of-observations-in-each-ecosystem-during-each-month-of-2023",
    "title": "Mapping Tasiyagnunpa (Western Meadowlark) migration",
    "section": "Count the number of observations in each ecosystem, during each month of 2023",
    "text": "Count the number of observations in each ecosystem, during each month of 2023\n\nIdentify the ecoregion for each observation\nYou can combine the ecoregions and the observations spatially using a method called .sjoin(), which stands for spatial join.\n\n\n\n\n\n\n Further reading\n\n\n\nCheck out the geopandas documentation on spatial joins to help you figure this one out. You can also ask your favorite LLM (Large-Language Model, like ChatGPT)\n\n\n\n\n\n\n\n\n Your task\n\n\n\n\nIdentify the correct values for the how= and predicate= parameters of the spatial join.\nSelect only the columns you will need for your plot.\nRun the code.\n\n\n\n\ngbif_ecoregion_gdf = (\n    ecoregions_gdf\n    # Match the CRS of the GBIF data and the ecoregions\n    .to_crs(gbif_gdf.crs)\n    # Find ecoregion for each observation\n    .sjoin(\n        gbif_gdf,\n        how='', \n        predicate='')\n    # Select the required columns\n    \n)\ngbif_ecoregion_gdf\n\n\n\nSee our solution!\ngbif_ecoregion_gdf = (\n    ecoregions_gdf\n    # Match the CRS of the GBIF data and the ecoregions\n    .to_crs(gbif_gdf.crs)\n    # Find ecoregion for each observation\n    .sjoin(\n        gbif_gdf,\n        how='inner', \n        predicate='contains')\n    # Select the required columns\n    [['month', 'name']]\n)\ngbif_ecoregion_gdf\n\n\n\n\nCount the observations in each ecoregion each month\n\n\n\n\n\n\n Your task:\n\n\n\n\nReplace columns_to_group_by with a list of columns. Keep in mind that you will end up with one row for each group – you want to count the observations in each ecoregion by month.\nSelect only month/ecosystem combinations that have more than one occurrence recorded, since a single occurrence could be an error.\nUse the .groupby() and .mean() methods to compute the mean occurrences by ecoregion and by month.\nRun the code – it will normalize the number of occurrences by month and ecoretion.\n\n\n\n\noccurrence_df = (\n    gbif_ecoregion_gdf\n    # For each ecoregion, for each month...\n    .groupby(columns_to_group_by)\n    # ...count the number of occurrences\n    .agg(occurrences=('name', 'count'))\n)\n\n# Get rid of rare observations (possible misidentification?)\noccurrence_df = occurrence_df[...]\n\n# Take the mean by ecoregion\nmean_occurrences_by_ecoregion = (\n    occurrence_df\n    ...\n)\n# Take the mean by month\nmean_occurrences_by_month = (\n    occurrence_df\n    ...\n)\n\n# Normalize the observations by the monthly mean throughout the year\noccurrence_df['norm_occurrences'] = (\n    occurrence_df.occurrences \n    / mean_occurrences_by_ecoregion\n    / mean_occurrences_by_month\n)\noccurrence_df\n\n\n\nSee our solution!\noccurrence_df = (\n    gbif_ecoregion_gdf\n    # For each ecoregion, for each month...\n    .groupby(['ecoregion', 'month'])\n    # ...count the number of occurrences\n    .agg(occurrences=('name', 'count'))\n)\n\n# Get rid of rare observation noise (possible misidentification?)\noccurrence_df = occurrence_df[occurrence_df.occurrences&gt;1]\n\n# Take the mean by ecoregion\nmean_occurrences_by_ecoregion = (\n    occurrence_df\n    .groupby(['ecoregion'])\n    .mean()\n)\n# Take the mean by month\nmean_occurrences_by_month = (\n    occurrence_df\n    .groupby(['month'])\n    .mean()\n)\n\n# Normalize the observations by the monthly mean throughout the year\noccurrence_df['norm_occurrences'] = (\n    occurrence_df\n    / mean_occurrences_by_ecoregion\n    / mean_occurrences_by_month\n)\noccurrence_df"
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution.html#plot-the-tasiyagnunpa-observations-by-month",
    "href": "notebooks/03-species-distribution/species-distribution.html#plot-the-tasiyagnunpa-observations-by-month",
    "title": "Mapping Tasiyagnunpa (Western Meadowlark) migration",
    "section": "Plot the Tasiyagnunpa observations by month",
    "text": "Plot the Tasiyagnunpa observations by month\n\n\n\n\n\n\n Your task\n\n\n\n\nIf applicable, replace any variable names with the names you defined previously.\nReplace column_name_used_for_ecoregion_color and column_name_used_for_slider with the column names you wish to use.\nCustomize your plot with your choice of title, tile source, color map, and size.\n\n\n\n\n# Join the occurrences with the plotting GeoDataFrame\noccurrence_gdf = ecoregions_plot_gdf.join(occurrence_df)\n\n# Get the plot bounds so they don't change with the slider\nxmin, ymin, xmax, ymax = occurrence_gdf.total_bounds\n\n# Plot occurrence by ecoregion and month\nmigration_plot = (\n    occurrence_gdf\n    .hvplot(\n        c=column_name_used_for_shape_color,\n        groupby=column_name_used_for_slider,\n        # Use background tiles\n        geo=True, crs=ccrs.Mercator(), tiles='CartoLight',\n        title=\"Your Title Here\",\n        xlim=(xmin, xmax), ylim=(ymin, ymax),\n        frame_height=600,\n        widget_location='bottom'\n    )\n)\n\n# Save the plot\nmigration_plot.save('migration.html', embed=True)\n\n# Show the plot\nmigration_plot\n\n\n\nSee our solution!\n# Join the occurrences with the plotting GeoDataFrame\noccurrence_gdf = ecoregions_plot_gdf.join(occurrence_df)\n\n# Get the plot bounds so they don't change with the slider\nxmin, ymin, xmax, ymax = occurrence_gdf.total_bounds\n\n# Define the slider widget\nslider = pn.widgets.DiscreteSlider(\n    name='month', \n    options={calendar.month_name[i]: i for i in range(1, 13)}\n)\n\n# Plot occurrence by ecoregion and month\nmigration_plot = (\n    occurrence_gdf\n    .hvplot(\n        c='norm_occurrences',\n        groupby='month',\n        # Use background tiles\n        geo=True, crs=ccrs.Mercator(), tiles='CartoLight',\n        title=\"Tasiyagnunpa migration\",\n        xlim=(xmin, xmax), ylim=(ymin, ymax),\n        frame_height=600,\n        colorbar=False,\n        widgets={'month': slider},\n        widget_location='bottom'\n    )\n)\n\n# Save the plot\nmigration_plot.save('migration.html', embed=True)\n\n# Show the plot\nmigration_plot\n\n\n::: {.content-visible when-format=“html”}  :::\n\n\n\n\n\n\n Want an EXTRA CHALLENGE?\n\n\n\nNotice that the month slider displays numbers instead of the month name. Use pn.widgets.DiscreteSlider() with the options= parameter set to give the months names. You might want to try asking ChatGPT how to do this, or look at the documentation for pn.widgets.DiscreteSlider(). This is pretty tricky!"
  }
]