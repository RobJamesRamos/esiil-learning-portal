[
  {
    "objectID": "notebooks/10-redlining/redlining-92-bulk-download.html",
    "href": "notebooks/10-redlining/redlining-92-bulk-download.html",
    "title": "\n                Bulk Download Data with earthaccess\n            ",
    "section": "",
    "text": "Whew – we’ve processed some reflectance data! But you may have noticed that your image doesn’t include all of Denver, or all the spectral bands you will need – Your search returned 60 different files covering different spectral bands and spatial areas! To work with all of them, we will have to utilize DRY coding techniques – for loops and functions.\nFirst things first – load your stored variables into memory:\n%store -r denver_redlining_gdf data_dir\n# Interoperable file paths\nimport re # Use regular expressions to extract metadata\n\nimport earthaccess # Access NASA data from the cloud\n# Overlay plots\nimport numpy as np # Process bit-wise cloud mask\n# Group and aggregate\n# Work with raster data\nfrom rioxarray.merge import merge_arrays # Merge rasters\nSee our solution!\nimport os # Interoperable file paths\nimport re # Use regular expressions to extract metadata\n\nimport earthaccess # Access NASA data from the cloud\nimport matplotlib.pyplot as plt # Overlay plots\nimport numpy as np # Process bit-wise cloud mask\nimport pandas as pd # Group and aggregate\nimport rioxarray as rxr # Work with raster data\nfrom rioxarray.merge import merge_arrays # Mosaic rasters\n# Search earthaccess\nSee our solution!\nearthaccess.login(strategy=\"interactive\", persist=True)\ndenver_results = earthaccess.search_data(\n    short_name='HLSL30',\n    bounding_box=tuple(denver_redlining_gdf.total_bounds),\n    temporal=(\"2023-07-12\", \"2023-07-12\"),\n    count=1\n)\nea_uris = earthaccess.open(denver_results)"
  },
  {
    "objectID": "notebooks/10-redlining/redlining-92-bulk-download.html#putting-it-all-together",
    "href": "notebooks/10-redlining/redlining-92-bulk-download.html#putting-it-all-together",
    "title": "\n                Bulk Download Data with earthaccess\n            ",
    "section": "Putting it all together",
    "text": "Putting it all together\nTo get a complete image, we will have to:\n\nLoad in all 4 rasters that cover Denver\nProcess them\nMerge, or mosaic, them into one image\n\nAnd…we haven’t even talked about the other bands you might need, or what to do if you want a time-series of images. You can see that if we were to copy and paste all the code above for each raster we need to load, it could get pretty overwhelming, and very error-prone. This is the opposite of what we mean by DRY (Don’t Repeat Yourself) code.\nBefore we continue, we’re going to make some functions to do the tasks you completed up above. We’ve already set up the code for you, identified parameters and returns, and added docstrings to document your function. When writing a function, we recommend the following process:\n\nCopy the code you’re using into the function shell, making sure to indent it so Python knows it is part of the function.\nIdentify any variables that are too specific. For example, if I called a DataArray denver_da, but I could use my function to process data from anywhere…I might change the name to da whereever it appears.\nIdentify the function parameters or arguments and make sure they match your code. Python may let you, but we recommend not using variables defined outside the function inside the function – if you need something in your function make sure to pass it in as a parameter!\nIdentify the function returns and make sure they match your code. What do you want to be able to access at the end? Keep in mind that unless you return them, the variables you create in a function will be stuck there. The return statement at the end of your function will pass the variables you want to keep back out of the function.\nWrite some code to test your function, such as by plotting the results. We recommend going line by line when you’re just getting started. This usually involves commenting code later on, and modifying the returns and test code as you go.\nRestart the kernel and run your function to check that doesn’t have any hidden requirements.\n\n\n\n\n\n\n\nTry It: DRY code with functions\n\n\n\nTake each processing step from above, and create a function to do it. We recommend writing the following 2 functions:\n\nA function to load a raster, crop it, and apply the scale factor\nA function to process the cloud mask\n\nApplying the cloud mask is a single line of code, so we don’t think it needs its own function.\nMake sure to test all your functions using your example from up top!\n\n\n\ndef process_image(uri, bounds_gdf):\n    \"\"\"\n    Load, crop, and scale a raster image from earthaccess\n\n    Parameters\n    ----------\n    uri: file-like or path-like\n      File accessor downloaded or obtained from earthaccess\n    bounds_gdf: gpd.GeoDataFrame\n      Area of interest to crop to\n\n    Returns\n    -------\n    cropped_da: rxr.DataArray\n      Processed raster\n    \"\"\"\n\n    return cropped_da\n\n\n\nSee our solution!\ndef process_image(uri, bounds_gdf):\n    \"\"\"\n    Load, crop, and scale a raster image from earthaccess\n\n    Parameters\n    ----------\n    uri: file-like or path-like\n      File accessor downloaded or obtained from earthaccess\n    bounds_gdf: gpd.GeoDataFrame\n      Area of interest to crop to\n\n    Returns\n    -------\n    cropped_da: rxr.DataArray\n      Processed raster\n    \"\"\"\n    # Open raster connection\n    da = rxr.open_rasterio(uri, mask_and_scale=True).squeeze()\n\n    # Crop raster\n    bounds = bounds_gdf.to_crs(da.rio.crs).total_bounds\n    cropped_da = da.rio.clip_box(*bounds)\n    return cropped_da\n\n\n\ndef process_cloud_mask(cloud_uri, bounds_gdf, bits_to_mask):\n    \"\"\"\n    Load an 8-bit Fmask file and process to a boolean mask\n\n    Parameters\n    ----------\n    uri: file-like or path-like\n      Fmask file accessor downloaded or obtained from earthaccess\n    bounds_gdf: gpd.GeoDataFrame\n      Area of interest to crop to\n    bits_to_mask: list of int\n      The indices of the bits to mask if set\n\n    Returns\n    -------\n    cloud_mask: np.array\n      Cloud mask\n    \"\"\"\n    \n    return cloud_mask\n\n\n\nSee our solution!\ndef process_cloud_mask(cloud_uri, bounds_gdf, bits_to_mask):\n    \"\"\"\n    Load an 8-bit Fmask file and process to a boolean mask\n\n    Parameters\n    ----------\n    uri: file-like or path-like\n      Fmask file accessor downloaded or obtained from earthaccess\n    bounds_gdf: gpd.GeoDataFrame\n      Area of interest to crop to\n    bits_to_mask: list of int\n      The indices of the bits to mask if set\n\n    Returns\n    -------\n    cloud_mask: np.array\n      Cloud mask\n    \"\"\"\n    cloud_da = process_image(cloud_uri, bounds_gdf)\n    cloud_bits = (\n        np.unpackbits(\n            (\n                # Get the cloud mask as an array...\n                cloud_da.values\n                # ... of 8-bit integers\n                .astype('uint8')\n                # With an extra axis to unpack the bits into\n                [:, :, np.newaxis]\n            ), \n            # List the least significant bit first to match the user guide\n            bitorder='little',\n            # Expand the array in a new dimension\n            axis=-1)\n    )\n    cloud_mask = np.sum(\n        # Select bits to mask\n        cloud_bits[:,:,bits_to_mask], \n        # Sum along the bit axis\n        axis=-1\n    )\n    # Check if any of the masked bits are true\n    cloud_mask = cloud_mask == 0\n    return cloud_mask\n\n# green_da = process_image(ea_uris[8], denver_redlining_gdf)\n# bits_to_mask = [\n#     1, # Cloud\n#     2, # Adjacent to cloud\n#     3, # Cloud shadow\n#     5, # Water\n# ]\n# cloud_mask = process_cloud_mask(\n#     ea_uris[14], denver_redlining_gdf, bits_to_mask)\n# green_masked_da = green_da.where(cloud_mask)\n# green_masked_da.plot(cmap='Greens', vmin=0, robust=True)\n\n\nOur next new tool to help you write DRY and correct code is the regular expression. Regular expressions are a little like the patterns we use with glob that contain wildcard characters (*) – but, they are much more powerful. With regular expressions, we can extract different segments from a string (file name, in this case) based on landmarks – even if those segments are not always the same length!\nFirst – run another earthaccess search, this time removing the count=1 argument, or setting it to count=-1. This will include all the results.\n\n# Search earthaccess\n\n# Open earthaccess results\n\n\n\nSee our solution!\nearthaccess.login(strategy=\"interactive\", persist=True)\ndenver_results = earthaccess.search_data(\n    short_name='HLSL30',\n    bounding_box=tuple(denver_redlining_gdf.total_bounds),\n    temporal=(\"2023-07-12\", \"2023-07-12\"),\n)\nea_uris = earthaccess.open(denver_results)\n\n\n\n\n\n\n\n\nTry It: Get metadata with **regular expressions**\n\n\n\nUsing the code below as a starting point, extract metadata from file names and put them into a DataFrame. This strategy will help you later on because you will be able to group rasters by their metadata values, such as tile ID, band ID, and/or date.\n\nBuild your regular expression. ChatGPT is a great tool to get started with your regular expression. You can also check out https://regex101.com/ to test your regular expressions, making sure to select the Python regular expression engine.\nReplace file_name with a string version of the URI. You can access it from the object you got from earthaccess through the .full_name attribute.\nAdd the URIs from earthaccess to the DataFrame you created as a new column.\n\n\n\n\n# Compile a regular expression to search for metadata\nuri_re = re.compile(\n    ...\n)\n\n# Find all the metadata in the file name\nuri_groups = [\n    uri_re.search(file_name_to_search).groupdict()\n    for uri in ea_uris]\n\n# Create a DataFrame with the metadata\nraster_df = pd.DataFrame(uri_groups)\n\n# Add the File-like URI to the DataFrame\n\n# Check the results\nraster_df\n\n\n\nSee our solution!\n# Compile a regular expression to search for metadata\nuri_re = re.compile(\n    r'HLS.L30.(?P&lt;tile_id&gt;[^.]*)'\n    r'.(?P&lt;date&gt;\\d*)'\n    r'T\\d*.v2.0.'\n    r'(?P&lt;band_id&gt;[^.]*)'\n    r'.tif'\n)\n\n# Find all the metadata in the file name\nuri_groups = [\n    uri_re.search(uri.full_name).groupdict()\n    for uri in ea_uris]\n\n# Create a DataFrame with the metadata\nraster_df = pd.DataFrame(uri_groups)\n\n# Add the File-like URI to the DataFrame\nraster_df['uri'] = ea_uris\n\n# Check the results\nraster_df\n\n\nNow you are ready to run your code repeatedly on each raster you want to load. To do this, we’ll use a structure called a for loop, which runs the same code repeatedly with different variable values. The values that change are special variables called looping variables.\nTo set up a for loop, you can use the following process:\n\nCopy the code you’re using into the for loop shell, making sure to indent it so Python knows it is part of the loop.\nIdentify any variables that are too specific. For example, if I called a DataArray green_da, but the loop will be processing data from other bands…I might change the name to da whereever it appears.\nIdentify the looping variable(s) and make sure they match your code.\nEstablish an accumulator – a data structure to store the result. Add what you want to keep from each iteration of the loop to it.\nWrite some code to test your loop, such as by printing out an intermediate value in the loop, or eventually plotting the final results. We recommend going line by line when you’re just getting started. This usually involves commenting code later on, and modifying the accumulator and testing as you go. You can also use the break keywork to stop the loop after a single iteration for testing.\n\n\n\n\n\n\n\nTry It: Process data\n\n\n\nProcess all your bands. We’ve provided the structure of the for loop – you will need to call your functions and work out how to pass them the arguments they need.\nSomething that is tricky about looping through DataFrames is that they tend to wrap values in external structures like Series (which are the data type of columns in a DataFrame). The print out of a Series containing one value and the print out of that value are not identical, but the are very similar. To get a value out from inside a Series, you can add the following code: .values[0]. This will first remove the Series wrapper, leaving an array, and the get the first value in the array.\n\n\n\n# Labels for each band to process\nbands = {\n    'B02': 'red',\n    ...\n}\n# Initialize structure for saving images\ndenver_das = {band_name: [] for band_name in bands.values()}\nfor tile_id, tile_df in raster_df.groupby('tile_id'):\n    # Load the cloud mask\n\n    for band_id, row in tile_df.groupby('band_id'):\n        if band_id in bands:\n            band_name = bands[band_id]\n            # Process band\n\n            # Mask band\n            \n            # Store the resulting DataArray ofr later\n            denver_das[band_name].append(band_masked_da)\n\n# Merge all tiles\ndenver_das = {\n    band_name: merge_arrays(das) \n    for band_name, das \n    in denver_das.items()}\n\ndenver_das['green'].plot(cmap='Greens', robust=True)\n\n\n\nSee our solution!\n# Labels for each band to process\nbands = {\n    'B02': 'red',\n    'B03': 'green',\n    'B04': 'blue',\n    'B05': 'nir'\n}\n# Initialize structure for saving images\ndenver_das = {band_name: [] for band_name in bands.values()}\nfor tile_id, tile_df in raster_df.groupby('tile_id'):\n\n    # Load the cloud mask\n    cloud_mask = process_cloud_mask(\n        tile_df.loc[tile_df.band_id=='Fmask', 'uri'].values[0],\n        denver_redlining_gdf, \n        [1, 2, 3, 5])\n\n    for band_id, row in tile_df.groupby('band_id'):\n        if band_id in bands:\n            band_name = bands[band_id]\n            band_da = process_image(\n                row.uri.values[0], \n                denver_redlining_gdf)\n            band_masked_da = band_da.where(cloud_mask)\n            denver_das[band_name].append(band_masked_da)\n\n# Merge all tiles\ndenver_das = {\n    band_name: merge_arrays(das) \n    for band_name, das \n    in denver_das.items()}\n\ndenver_das['green'].plot(cmap='Greens', robust=True)\n\n\n\n\n\n\n\n\nTry It: Check your data\n\n\n\nMake a plot of one of your merged bands with the denver boundary superimposed. You should now have data for the whole city!\n\n\n\n# Plot a merged raster band\n\n\n\nSee our solution!\ndenver_das['green'].plot(cmap='Greens', vmin=0, robust=True)\ndenver_redlining_gdf.to_crs(denver_das['green'].rio.crs).plot(\n    ax=plt.gca(),\n    edgecolor='black', color='none')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the ESIIL Learning Portal!",
    "section": "",
    "text": "Welcome to the ESIIL Learning Portal!\nExplore textbooks:\n\nIntroduction to Earth Data Science\nESIIL Data Short Course\nESIIL STARS Textbook\n\nExplore collaborative workshops:\n\nMacrosystems Ecology for All (MEFA)\nChicago Symposium"
  },
  {
    "objectID": "notebooks/10-redlining/redlining-91-download-earthaccess.html",
    "href": "notebooks/10-redlining/redlining-91-download-earthaccess.html",
    "title": "\n                Download Data with earthaccess\n            ",
    "section": "",
    "text": "Load your stored variables into memory:\n\n%store -r denver_redlining_gdf data_dir\n\n\n\n\n\n\n\nTry It: Import packages\n\n\n\nAdd imports for packages that help you:\n\nWork with the file system interoperably\nWork with vector data\nCreate interactive plots of vector data\nGroup and aggregate tabular data\n\n\n\n\nimport re # Use regular expressions to extract metadata\n\nimport earthaccess # Access NASA data from the cloud\n# Overlay raster and vector data\nimport numpy as np # Process bit-wise cloud mask\n# Group and aggregate\n# Work with raster data\nfrom rioxarray.merge import merge_arrays # Merge rasters\n\n\n\nSee our solution!\nimport os # Interoperable file paths\nimport pathlib # Find the home folder\nimport re # Use regular expressions to extract metadata\n\nimport earthaccess # Access NASA data from the cloud\nimport matplotlib.pyplot as plt # Overlay raster and vector data\nimport numpy as np # Process bit-wise cloud mask\nimport pandas as pd # Group and aggregate\nimport rioxarray as rxr # Work with raster data\nfrom rioxarray.merge import merge_arrays # Mosaic rasters\n\n\n\n\n\n\n\n\nTry It: Set up `earthaccess` connection\n\n\n\n\nMake an account on the earthdata site. You don’t need to spend a lot of time on the registration form – go ahead answer their questions to the best of your ability. That information is used for internal reporting and analysis, not to decide whether or not to grant you an account!\nRun the code below an enter your credentials to log into earthaccess from Python. You should only need to do this once, as long as persist=True is set.\n\n\n\n\nearthaccess.login(strategy=\"interactive\", persist=True)\n\n\n\nSee our solution!\nearthaccess.login(strategy=\"interactive\", persist=True)"
  },
  {
    "objectID": "notebooks/10-redlining/redlining-91-download-earthaccess.html#step-1-set-up",
    "href": "notebooks/10-redlining/redlining-91-download-earthaccess.html#step-1-set-up",
    "title": "\n                Download Data with earthaccess\n            ",
    "section": "",
    "text": "Load your stored variables into memory:\n\n%store -r denver_redlining_gdf data_dir\n\n\n\n\n\n\n\nTry It: Import packages\n\n\n\nAdd imports for packages that help you:\n\nWork with the file system interoperably\nWork with vector data\nCreate interactive plots of vector data\nGroup and aggregate tabular data\n\n\n\n\nimport re # Use regular expressions to extract metadata\n\nimport earthaccess # Access NASA data from the cloud\n# Overlay raster and vector data\nimport numpy as np # Process bit-wise cloud mask\n# Group and aggregate\n# Work with raster data\nfrom rioxarray.merge import merge_arrays # Merge rasters\n\n\n\nSee our solution!\nimport os # Interoperable file paths\nimport pathlib # Find the home folder\nimport re # Use regular expressions to extract metadata\n\nimport earthaccess # Access NASA data from the cloud\nimport matplotlib.pyplot as plt # Overlay raster and vector data\nimport numpy as np # Process bit-wise cloud mask\nimport pandas as pd # Group and aggregate\nimport rioxarray as rxr # Work with raster data\nfrom rioxarray.merge import merge_arrays # Mosaic rasters\n\n\n\n\n\n\n\n\nTry It: Set up `earthaccess` connection\n\n\n\n\nMake an account on the earthdata site. You don’t need to spend a lot of time on the registration form – go ahead answer their questions to the best of your ability. That information is used for internal reporting and analysis, not to decide whether or not to grant you an account!\nRun the code below an enter your credentials to log into earthaccess from Python. You should only need to do this once, as long as persist=True is set.\n\n\n\n\nearthaccess.login(strategy=\"interactive\", persist=True)\n\n\n\nSee our solution!\nearthaccess.login(strategy=\"interactive\", persist=True)"
  },
  {
    "objectID": "notebooks/10-redlining/redlining-91-download-earthaccess.html#step-2-search",
    "href": "notebooks/10-redlining/redlining-91-download-earthaccess.html#step-2-search",
    "title": "\n                Download Data with earthaccess\n            ",
    "section": "STEP 2: Search",
    "text": "STEP 2: Search\n\n\n\n\n\n\nTry It: Search for HLS data\n\n\n\nIt can be useful to use NASA’s online resources to find the data you want to download before accessing it over API. To do that:\n\nGo to the NASA Worldview Site and search for Denver, CO.\nAdd the HLSL30 product as a base map. You can do this by clicking Add Layer and then searching for the short name HLSL30. When looking for other datasets, the short name can usually be found by on the data homepage, accessible from the doi. Practice opening the data page for HLSL30 to find where the shortcode is.\nBy dragging the date indicator on the bottom, search the month of July, 2023 for a day that has data available and little to no cloud cover over Denver. You may need to wait a second or so for the data to load for any given date.\n\nNow, using the code below:\n\nPut the short name of the dataset into the earthaccess.search_data() function.\nReplace gdf with the name of your denver redlining GeoDataFrame.\nPut the date you found into the temporal parameter, replacing YYYY-MM-DD with the appropriate year (Y), month (M), and day (D) digits.\nRun the code and make sure there are results! If not, double check your date.\n\n\n\n\ndenver_results = earthaccess.search_data(\n    short_name=\"\",\n    bounding_box=tuple(gdf.total_bounds),\n    temporal=(\"YYYY-MM-DD\"),\n    count=1\n)\ndenver_results\n\n\n\nSee our solution!\ndenver_results = earthaccess.search_data(\n    short_name='HLSL30',\n    bounding_box=tuple(denver_redlining_gdf.total_bounds),\n    temporal=(\"2023-07-12\", \"2023-07-12\"),\n    count=1\n)\ndenver_results"
  },
  {
    "objectID": "notebooks/10-redlining/redlining-91-download-earthaccess.html#step-3-open-data-connections",
    "href": "notebooks/10-redlining/redlining-91-download-earthaccess.html#step-3-open-data-connections",
    "title": "\n                Download Data with earthaccess\n            ",
    "section": "STEP 3: Open data connections",
    "text": "STEP 3: Open data connections\n\n\n\n\n\n\nTry It: Access HLS data\n\n\n\n\n\n\n\n\nSee our solution!\nea_uris = earthaccess.open(denver_results)\nea_uris"
  },
  {
    "objectID": "notebooks/10-redlining/redlining-91-download-earthaccess.html#step-4-process-data",
    "href": "notebooks/10-redlining/redlining-91-download-earthaccess.html#step-4-process-data",
    "title": "\n                Download Data with earthaccess\n            ",
    "section": "STEP 4: Process data",
    "text": "STEP 4: Process data\n\nConnect with Python\nRight now, you have file connections with your data in the cloud, but you haven’t actually downloaded anything. earthaccess uses a lazy virtual file connection (vsi), meaning that it doesn’t download anything until you ask for specific numbers, like when you plot. In addition, many operations can be performed on the server side, such as selecting data by index and summary operations.\n\n\n\n\n\n\nTry It: Practice importing multispectral data\n\n\n\nTo load in the green band of data to work with them:\n\nFind a green layer: Check out the HLSL30 User Guide. Which layer is the green layer? What is its index in the list of files you got from earthaccess? Remember that Python starts counting at 0!\nOpen the layer: Open one of green layers you downloaded using the rxr.open_rasterio() function. You can do this by index if you like – this code is for you to explore using the data. Don’t forget to .squeeze()! But, hold off on masking and scaling so you can see what it looks like to have unmasked nodata values in your DataArray.\nMask nodata values and apply the scale factor: Now, plot your Data Array. Notice that many of the values are somewhere around -10000. We can guess, because it’s common in datasets like this, that the value -9999 is used here as a “no data” value to mark where no measurement was taken. To encode the no data values correctly, add the parameter mask_and_scale=True to the rxr.open_rasterio() function. Now, the image should be white where there’s no data, the values should range between 0 and 1.\nTry out your file connection to see how long different operations take. You can try taking the .mean() of your data (fast – this happens on the server), and plotting your data (slow – you have to download the whole tile to plot this way). Next, add the study boundary onto the image. Notice that you have quite a bit more data than you need!\n\n\n\n\n# Import one tile of green data\n\n\n\nSee our solution!\ngreen_da = rxr.open_rasterio(\n    ea_uris[8], mask_and_scale=True).squeeze()\ngreen_da.plot(cmap='Greens', vmin=0, robust=True)\ndenver_redlining_gdf.to_crs(green_da.rio.crs).plot(ax=plt.gca())\n\n\n\n\nCrop\nPlotting a whole HLSL30 tile probably took about 30 seconds to download and plot the data, but we know that some operations can be performed without downloading. One of this is cropping the raster image by index using the .rio.clip_box() method. Note that .rio.clip() clips the data to a boundary instead of a rectangle, and will trigger a full download.\nWe almost always want to crop before doing anything else. Cropping usuallyreduces the amount of data you need dramatically, which makes everything faster.\nIn this example, our study area vector data and our raster multispectral reflectance data come in different Coordinate Reference Systems (CRSs). Before using them together, you will need to convert the vector data to be in the same CRS as the raster data.\n\n\n\n\n\n\nWhy convert vector data?\n\n\n\nYou’re being asked to convert vector data to the raster CRS instead of the other way around for two specific reasons. One is that vector data is usually smaller than raster data and converts faster. The other is that converting the CRS of a raster file means that the centers of the grid cells will be in slightly different locations because grids do not transfer over. This requires some kind of interpolation, which can be computationally costly and also have the potential to introduce error.\n\n\n\n\n\n\n\n\nTry It: Get your study bounds\n\n\n\n\nAccess the CRS of the redlining data (.crs) and the reflectance data (.rio.crs) to verify that they are not the same\nConvert the redlining GeoDataFrame to the same CRS as the reflectance data.\nGet the .total_bounds of the redlining data so you can crop to it.\n\n\n\n\n# Get the study bounds\n\n\n\nSee our solution!\ndenver_bounds = (\n    denver_redlining_gdf\n    .to_crs(green_da.rio.crs)\n    .total_bounds\n)\ndenver_bounds\n\n\nNow, you’re ready to crop.\n\n\n\n\n\n\nTry It: Crop raster data\n\n\n\nTo crop your data, use the method .rio.clip_box(*your_bounds_here).\nThe asterisk unpacks the bounds so that they go in as four separate arguments instead of a single tuple.\nYou can also practice cloud masking again here if you like. Note that you will also need to crop your cloud mask!\n\n\n\n# Crop the green reflectance data\n\n\n\nSee our solution!\ngreen_cropped_da = green_da.rio.clip_box(*denver_bounds)\ngreen_cropped_da.plot(cmap='Greens', vmin=0, robust=True)\n\n\nCongratulations – you’ve accessed some data from the cloud!"
  }
]